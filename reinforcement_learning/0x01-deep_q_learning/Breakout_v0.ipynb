{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breakout-v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CGqQIev_qw5",
        "outputId": "aa8a23a6-40bc-4004-9a3c-4b308216af0b"
      },
      "source": [
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars\n",
        "!pip install tensorflow==2.4\n",
        "!pip install keras==2.2.4\n",
        "!pip install Pillow\n",
        "!pip install h5py\n",
        "!pip install keras-rl2\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\" Trains an agent to play Breakout \"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import tensorflow.keras as K\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Permute\n",
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.callbacks import FileLogger\n",
        "from rl.core import Processor\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow.python.util.deprecation as deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    \"\"\" Preprocess images as per deepmind paper \"\"\"\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\" Process observation \"\"\"\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\" Convert batch to float32 \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\" Process reward \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "env = gym.make('Breakout-v0')\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "inputs = Input(shape=((4,) + INPUT_SHAPE))\n",
        "x = Permute((2, 3, 1))(inputs)\n",
        "x = Conv2D(32, 8, strides=4, activation='relu')(x)\n",
        "x = Conv2D(64, 4, strides=2, activation='relu')(x)\n",
        "x = Conv2D(64, 2, strides=3, activation='relu')(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "action = Dense(nb_actions, activation='linear')(x)\n",
        "model = K.Model(inputs=inputs, outputs=action)\n",
        "model.summary()\n",
        "\n",
        "memory = SequentialMemory(limit=1000000, window_length=4)\n",
        "policy = LinearAnnealedPolicy(\n",
        "    EpsGreedyQPolicy(),\n",
        "    attr='eps',\n",
        "    value_max=1.,\n",
        "    value_min=.1,\n",
        "    value_test=0.05,\n",
        "    nb_steps=1000000\n",
        ")\n",
        "dqn = DQNAgent(\n",
        "    model=model,\n",
        "    nb_actions=nb_actions,\n",
        "    memory=memory,\n",
        "    nb_steps_warmup=50000,\n",
        "    target_model_update=10000,\n",
        "    policy=policy,\n",
        "    processor=AtariProcessor(),\n",
        "    gamma=.99,\n",
        "    train_interval=4,\n",
        "    delta_clip=1.\n",
        ")\n",
        "dqn.compile(K.optimizers.Adam(lr=.00025), metrics=['mae'])\n",
        "\n",
        "dqn.fit(\n",
        "    env,\n",
        "    nb_steps=1750000,\n",
        "    visualize=False,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "dqn.save_weights('policy.h5', overwrite=True)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unrar in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "mkdir: cannot create directory ‘rars’: File exists\n",
            "copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "Requirement already satisfied: tensorflow==2.4 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (2.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.17.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.3.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (2.10.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.34.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (0.4.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (3.5.0)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-rl2) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->keras-rl2) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-rl2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->keras-rl2) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow->keras-rl2) (3.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 4, 84, 84)]       0         \n",
            "_________________________________________________________________\n",
            "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 3, 3, 64)          16448     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               295424    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 354,980\n",
            "Trainable params: 354,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 1750000 steps ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     167/1750000: episode: 1, duration: 0.871s, episode steps: 167, steps per second: 192, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.611 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "     473/1750000: episode: 2, duration: 0.973s, episode steps: 306, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "     950/1750000: episode: 3, duration: 1.514s, episode steps: 477, steps per second: 315, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    1123/1750000: episode: 4, duration: 0.566s, episode steps: 173, steps per second: 306, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.671 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    1303/1750000: episode: 5, duration: 0.574s, episode steps: 180, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    1618/1750000: episode: 6, duration: 1.018s, episode steps: 315, steps per second: 309, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    1951/1750000: episode: 7, duration: 1.069s, episode steps: 333, steps per second: 312, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    2219/1750000: episode: 8, duration: 0.851s, episode steps: 268, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    2544/1750000: episode: 9, duration: 1.036s, episode steps: 325, steps per second: 314, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    2718/1750000: episode: 10, duration: 0.563s, episode steps: 174, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    2948/1750000: episode: 11, duration: 0.733s, episode steps: 230, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    3123/1750000: episode: 12, duration: 0.555s, episode steps: 175, steps per second: 315, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    3356/1750000: episode: 13, duration: 0.753s, episode steps: 233, steps per second: 309, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    3585/1750000: episode: 14, duration: 0.732s, episode steps: 229, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    3930/1750000: episode: 15, duration: 1.092s, episode steps: 345, steps per second: 316, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    4097/1750000: episode: 16, duration: 0.533s, episode steps: 167, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    4302/1750000: episode: 17, duration: 0.670s, episode steps: 205, steps per second: 306, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    4617/1750000: episode: 18, duration: 1.004s, episode steps: 315, steps per second: 314, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    4976/1750000: episode: 19, duration: 1.149s, episode steps: 359, steps per second: 313, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    5141/1750000: episode: 20, duration: 0.533s, episode steps: 165, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    5353/1750000: episode: 21, duration: 0.675s, episode steps: 212, steps per second: 314, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    5631/1750000: episode: 22, duration: 0.898s, episode steps: 278, steps per second: 310, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    5810/1750000: episode: 23, duration: 0.574s, episode steps: 179, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    5979/1750000: episode: 24, duration: 0.556s, episode steps: 169, steps per second: 304, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    6284/1750000: episode: 25, duration: 0.973s, episode steps: 305, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    6498/1750000: episode: 26, duration: 0.680s, episode steps: 214, steps per second: 315, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    6783/1750000: episode: 27, duration: 0.904s, episode steps: 285, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    7098/1750000: episode: 28, duration: 1.000s, episode steps: 315, steps per second: 315, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    7343/1750000: episode: 29, duration: 0.783s, episode steps: 245, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    7583/1750000: episode: 30, duration: 0.777s, episode steps: 240, steps per second: 309, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    7757/1750000: episode: 31, duration: 0.564s, episode steps: 174, steps per second: 308, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    7930/1750000: episode: 32, duration: 0.564s, episode steps: 173, steps per second: 307, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    8156/1750000: episode: 33, duration: 0.728s, episode steps: 226, steps per second: 310, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    8608/1750000: episode: 34, duration: 1.434s, episode steps: 452, steps per second: 315, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    8882/1750000: episode: 35, duration: 0.871s, episode steps: 274, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    9183/1750000: episode: 36, duration: 0.967s, episode steps: 301, steps per second: 311, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    9466/1750000: episode: 37, duration: 0.902s, episode steps: 283, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    9803/1750000: episode: 38, duration: 1.077s, episode steps: 337, steps per second: 313, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "    9976/1750000: episode: 39, duration: 0.556s, episode steps: 173, steps per second: 311, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   10283/1750000: episode: 40, duration: 0.980s, episode steps: 307, steps per second: 313, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   10574/1750000: episode: 41, duration: 0.927s, episode steps: 291, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   10783/1750000: episode: 42, duration: 0.678s, episode steps: 209, steps per second: 308, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   11071/1750000: episode: 43, duration: 0.912s, episode steps: 288, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   11245/1750000: episode: 44, duration: 0.558s, episode steps: 174, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   11671/1750000: episode: 45, duration: 1.369s, episode steps: 426, steps per second: 311, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   11956/1750000: episode: 46, duration: 0.908s, episode steps: 285, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   12124/1750000: episode: 47, duration: 0.537s, episode steps: 168, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   12352/1750000: episode: 48, duration: 0.727s, episode steps: 228, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   12579/1750000: episode: 49, duration: 0.723s, episode steps: 227, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   12748/1750000: episode: 50, duration: 0.544s, episode steps: 169, steps per second: 311, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   12927/1750000: episode: 51, duration: 0.576s, episode steps: 179, steps per second: 311, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   13102/1750000: episode: 52, duration: 0.566s, episode steps: 175, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   13436/1750000: episode: 53, duration: 1.061s, episode steps: 334, steps per second: 315, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   13614/1750000: episode: 54, duration: 0.573s, episode steps: 178, steps per second: 311, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   13862/1750000: episode: 55, duration: 0.787s, episode steps: 248, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   14134/1750000: episode: 56, duration: 0.873s, episode steps: 272, steps per second: 311, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   14434/1750000: episode: 57, duration: 0.952s, episode steps: 300, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   14610/1750000: episode: 58, duration: 0.562s, episode steps: 176, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   14887/1750000: episode: 59, duration: 0.885s, episode steps: 277, steps per second: 313, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   15051/1750000: episode: 60, duration: 0.528s, episode steps: 164, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   15357/1750000: episode: 61, duration: 0.967s, episode steps: 306, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   15519/1750000: episode: 62, duration: 0.525s, episode steps: 162, steps per second: 308, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   15698/1750000: episode: 63, duration: 0.573s, episode steps: 179, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   16194/1750000: episode: 64, duration: 1.562s, episode steps: 496, steps per second: 317, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.381 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   16573/1750000: episode: 65, duration: 1.196s, episode steps: 379, steps per second: 317, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   16748/1750000: episode: 66, duration: 0.558s, episode steps: 175, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   16929/1750000: episode: 67, duration: 0.577s, episode steps: 181, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   17135/1750000: episode: 68, duration: 0.660s, episode steps: 206, steps per second: 312, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   17341/1750000: episode: 69, duration: 0.664s, episode steps: 206, steps per second: 310, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   17583/1750000: episode: 70, duration: 0.773s, episode steps: 242, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   17885/1750000: episode: 71, duration: 0.957s, episode steps: 302, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   18050/1750000: episode: 72, duration: 0.534s, episode steps: 165, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   18278/1750000: episode: 73, duration: 0.725s, episode steps: 228, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   18457/1750000: episode: 74, duration: 0.573s, episode steps: 179, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   18700/1750000: episode: 75, duration: 0.778s, episode steps: 243, steps per second: 312, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   19036/1750000: episode: 76, duration: 1.077s, episode steps: 336, steps per second: 312, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   19328/1750000: episode: 77, duration: 0.947s, episode steps: 292, steps per second: 308, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   19650/1750000: episode: 78, duration: 1.032s, episode steps: 322, steps per second: 312, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   19890/1750000: episode: 79, duration: 0.779s, episode steps: 240, steps per second: 308, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   20104/1750000: episode: 80, duration: 0.696s, episode steps: 214, steps per second: 307, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   20374/1750000: episode: 81, duration: 0.882s, episode steps: 270, steps per second: 306, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   20588/1750000: episode: 82, duration: 0.693s, episode steps: 214, steps per second: 309, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   20807/1750000: episode: 83, duration: 0.711s, episode steps: 219, steps per second: 308, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   21038/1750000: episode: 84, duration: 0.750s, episode steps: 231, steps per second: 308, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   21225/1750000: episode: 85, duration: 0.608s, episode steps: 187, steps per second: 307, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   21503/1750000: episode: 86, duration: 0.898s, episode steps: 278, steps per second: 310, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   21795/1750000: episode: 87, duration: 0.955s, episode steps: 292, steps per second: 306, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   21976/1750000: episode: 88, duration: 0.616s, episode steps: 181, steps per second: 294, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   22193/1750000: episode: 89, duration: 0.700s, episode steps: 217, steps per second: 310, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   22427/1750000: episode: 90, duration: 0.747s, episode steps: 234, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   22744/1750000: episode: 91, duration: 1.009s, episode steps: 317, steps per second: 314, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   22977/1750000: episode: 92, duration: 0.747s, episode steps: 233, steps per second: 312, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   23154/1750000: episode: 93, duration: 0.574s, episode steps: 177, steps per second: 308, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   23319/1750000: episode: 94, duration: 0.534s, episode steps: 165, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.442 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   23550/1750000: episode: 95, duration: 0.765s, episode steps: 231, steps per second: 302, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   24040/1750000: episode: 96, duration: 1.543s, episode steps: 490, steps per second: 318, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   24227/1750000: episode: 97, duration: 0.598s, episode steps: 187, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   24473/1750000: episode: 98, duration: 0.775s, episode steps: 246, steps per second: 317, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   24651/1750000: episode: 99, duration: 0.574s, episode steps: 178, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.348 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   24897/1750000: episode: 100, duration: 0.800s, episode steps: 246, steps per second: 307, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   25192/1750000: episode: 101, duration: 0.937s, episode steps: 295, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   25546/1750000: episode: 102, duration: 1.119s, episode steps: 354, steps per second: 316, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   25813/1750000: episode: 103, duration: 0.851s, episode steps: 267, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   25986/1750000: episode: 104, duration: 0.553s, episode steps: 173, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   26155/1750000: episode: 105, duration: 0.538s, episode steps: 169, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   26421/1750000: episode: 106, duration: 0.845s, episode steps: 266, steps per second: 315, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   26628/1750000: episode: 107, duration: 0.667s, episode steps: 207, steps per second: 310, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   26955/1750000: episode: 108, duration: 1.040s, episode steps: 327, steps per second: 314, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   27265/1750000: episode: 109, duration: 0.983s, episode steps: 310, steps per second: 315, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   27542/1750000: episode: 110, duration: 0.881s, episode steps: 277, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   27812/1750000: episode: 111, duration: 0.858s, episode steps: 270, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   28070/1750000: episode: 112, duration: 0.815s, episode steps: 258, steps per second: 317, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   28312/1750000: episode: 113, duration: 0.771s, episode steps: 242, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   28772/1750000: episode: 114, duration: 1.448s, episode steps: 460, steps per second: 318, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   28944/1750000: episode: 115, duration: 0.550s, episode steps: 172, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.384 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   29129/1750000: episode: 116, duration: 0.591s, episode steps: 185, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   29371/1750000: episode: 117, duration: 0.768s, episode steps: 242, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   29671/1750000: episode: 118, duration: 0.957s, episode steps: 300, steps per second: 313, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   29901/1750000: episode: 119, duration: 0.738s, episode steps: 230, steps per second: 312, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   30169/1750000: episode: 120, duration: 0.854s, episode steps: 268, steps per second: 314, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   30466/1750000: episode: 121, duration: 0.941s, episode steps: 297, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   30764/1750000: episode: 122, duration: 0.943s, episode steps: 298, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   30945/1750000: episode: 123, duration: 0.573s, episode steps: 181, steps per second: 316, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   31364/1750000: episode: 124, duration: 1.315s, episode steps: 419, steps per second: 319, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   31675/1750000: episode: 125, duration: 0.976s, episode steps: 311, steps per second: 319, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   31881/1750000: episode: 126, duration: 0.656s, episode steps: 206, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.296 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   32123/1750000: episode: 127, duration: 0.766s, episode steps: 242, steps per second: 316, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   32298/1750000: episode: 128, duration: 0.560s, episode steps: 175, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   32476/1750000: episode: 129, duration: 0.582s, episode steps: 178, steps per second: 306, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   32647/1750000: episode: 130, duration: 0.553s, episode steps: 171, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   32969/1750000: episode: 131, duration: 1.024s, episode steps: 322, steps per second: 315, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   33199/1750000: episode: 132, duration: 0.733s, episode steps: 230, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   33487/1750000: episode: 133, duration: 0.927s, episode steps: 288, steps per second: 311, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   33742/1750000: episode: 134, duration: 0.813s, episode steps: 255, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   33979/1750000: episode: 135, duration: 0.753s, episode steps: 237, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   34535/1750000: episode: 136, duration: 1.759s, episode steps: 556, steps per second: 316, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   34709/1750000: episode: 137, duration: 0.561s, episode steps: 174, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   34944/1750000: episode: 138, duration: 0.751s, episode steps: 235, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   35121/1750000: episode: 139, duration: 0.570s, episode steps: 177, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   35342/1750000: episode: 140, duration: 0.703s, episode steps: 221, steps per second: 314, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   35518/1750000: episode: 141, duration: 0.570s, episode steps: 176, steps per second: 309, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   35686/1750000: episode: 142, duration: 0.536s, episode steps: 168, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   35969/1750000: episode: 143, duration: 0.891s, episode steps: 283, steps per second: 318, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.357 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   36207/1750000: episode: 144, duration: 0.763s, episode steps: 238, steps per second: 312, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   36440/1750000: episode: 145, duration: 0.738s, episode steps: 233, steps per second: 316, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   36719/1750000: episode: 146, duration: 0.883s, episode steps: 279, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   36905/1750000: episode: 147, duration: 0.591s, episode steps: 186, steps per second: 315, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   37216/1750000: episode: 148, duration: 0.987s, episode steps: 311, steps per second: 315, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   37429/1750000: episode: 149, duration: 0.674s, episode steps: 213, steps per second: 316, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   37681/1750000: episode: 150, duration: 0.807s, episode steps: 252, steps per second: 312, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   37849/1750000: episode: 151, duration: 0.536s, episode steps: 168, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   38184/1750000: episode: 152, duration: 1.056s, episode steps: 335, steps per second: 317, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   38425/1750000: episode: 153, duration: 0.767s, episode steps: 241, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.299 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   38594/1750000: episode: 154, duration: 0.542s, episode steps: 169, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   38895/1750000: episode: 155, duration: 0.952s, episode steps: 301, steps per second: 316, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   39079/1750000: episode: 156, duration: 0.585s, episode steps: 184, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   39249/1750000: episode: 157, duration: 0.546s, episode steps: 170, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   39613/1750000: episode: 158, duration: 1.167s, episode steps: 364, steps per second: 312, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   39890/1750000: episode: 159, duration: 0.880s, episode steps: 277, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   40058/1750000: episode: 160, duration: 0.539s, episode steps: 168, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   40277/1750000: episode: 161, duration: 0.699s, episode steps: 219, steps per second: 313, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   40497/1750000: episode: 162, duration: 0.700s, episode steps: 220, steps per second: 314, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   40730/1750000: episode: 163, duration: 0.738s, episode steps: 233, steps per second: 316, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   40938/1750000: episode: 164, duration: 0.660s, episode steps: 208, steps per second: 315, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   41106/1750000: episode: 165, duration: 0.543s, episode steps: 168, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   41439/1750000: episode: 166, duration: 1.058s, episode steps: 333, steps per second: 315, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   41613/1750000: episode: 167, duration: 0.558s, episode steps: 174, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   41881/1750000: episode: 168, duration: 0.851s, episode steps: 268, steps per second: 315, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   42126/1750000: episode: 169, duration: 0.779s, episode steps: 245, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   42350/1750000: episode: 170, duration: 0.723s, episode steps: 224, steps per second: 310, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   42677/1750000: episode: 171, duration: 1.066s, episode steps: 327, steps per second: 307, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   43035/1750000: episode: 172, duration: 1.132s, episode steps: 358, steps per second: 316, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   43398/1750000: episode: 173, duration: 1.146s, episode steps: 363, steps per second: 317, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   43611/1750000: episode: 174, duration: 0.690s, episode steps: 213, steps per second: 309, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   43787/1750000: episode: 175, duration: 0.559s, episode steps: 176, steps per second: 315, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   44082/1750000: episode: 176, duration: 0.927s, episode steps: 295, steps per second: 318, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   44258/1750000: episode: 177, duration: 0.562s, episode steps: 176, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   44533/1750000: episode: 178, duration: 0.868s, episode steps: 275, steps per second: 317, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   44764/1750000: episode: 179, duration: 0.732s, episode steps: 231, steps per second: 316, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   44984/1750000: episode: 180, duration: 0.700s, episode steps: 220, steps per second: 314, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   45217/1750000: episode: 181, duration: 0.741s, episode steps: 233, steps per second: 314, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   45453/1750000: episode: 182, duration: 0.753s, episode steps: 236, steps per second: 313, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   45682/1750000: episode: 183, duration: 0.726s, episode steps: 229, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   45864/1750000: episode: 184, duration: 0.587s, episode steps: 182, steps per second: 310, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.319 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   46099/1750000: episode: 185, duration: 0.744s, episode steps: 235, steps per second: 316, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   46417/1750000: episode: 186, duration: 0.999s, episode steps: 318, steps per second: 318, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   46601/1750000: episode: 187, duration: 0.588s, episode steps: 184, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   46771/1750000: episode: 188, duration: 0.546s, episode steps: 170, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   47005/1750000: episode: 189, duration: 0.742s, episode steps: 234, steps per second: 315, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   47337/1750000: episode: 190, duration: 1.050s, episode steps: 332, steps per second: 316, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   47638/1750000: episode: 191, duration: 0.959s, episode steps: 301, steps per second: 314, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   47974/1750000: episode: 192, duration: 1.064s, episode steps: 336, steps per second: 316, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   48344/1750000: episode: 193, duration: 1.163s, episode steps: 370, steps per second: 318, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   48772/1750000: episode: 194, duration: 1.359s, episode steps: 428, steps per second: 315, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   48977/1750000: episode: 195, duration: 0.648s, episode steps: 205, steps per second: 316, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   49158/1750000: episode: 196, duration: 0.580s, episode steps: 181, steps per second: 312, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   49333/1750000: episode: 197, duration: 0.555s, episode steps: 175, steps per second: 315, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   49693/1750000: episode: 198, duration: 1.138s, episode steps: 360, steps per second: 316, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   49870/1750000: episode: 199, duration: 0.564s, episode steps: 177, steps per second: 314, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "   50096/1750000: episode: 200, duration: 3.522s, episode steps: 226, steps per second:  64, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001523, mae: 0.014972, mean_q: 0.024097, mean_eps: 0.954957\n",
            "   50269/1750000: episode: 201, duration: 3.228s, episode steps: 173, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002493, mae: 0.012258, mean_q: 0.023317, mean_eps: 0.954836\n",
            "   50438/1750000: episode: 202, duration: 3.103s, episode steps: 169, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.763 [0.000, 3.000],  loss: 0.003041, mae: 0.015330, mean_q: 0.028478, mean_eps: 0.954681\n",
            "   50728/1750000: episode: 203, duration: 5.356s, episode steps: 290, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.003706, mae: 0.014555, mean_q: 0.022818, mean_eps: 0.954476\n",
            "   50981/1750000: episode: 204, duration: 4.684s, episode steps: 253, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.003414, mae: 0.014223, mean_q: 0.024631, mean_eps: 0.954231\n",
            "   51159/1750000: episode: 205, duration: 3.234s, episode steps: 178, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000723, mae: 0.009866, mean_q: 0.016138, mean_eps: 0.954037\n",
            "   51469/1750000: episode: 206, duration: 5.752s, episode steps: 310, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.003797, mae: 0.014108, mean_q: 0.021313, mean_eps: 0.953817\n",
            "   51643/1750000: episode: 207, duration: 3.184s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002903, mae: 0.014314, mean_q: 0.017279, mean_eps: 0.953600\n",
            "   51920/1750000: episode: 208, duration: 5.163s, episode steps: 277, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.003377, mae: 0.015424, mean_q: 0.021203, mean_eps: 0.953398\n",
            "   52130/1750000: episode: 209, duration: 3.900s, episode steps: 210, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002662, mae: 0.013668, mean_q: 0.022842, mean_eps: 0.953178\n",
            "   52360/1750000: episode: 210, duration: 4.264s, episode steps: 230, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.002197, mae: 0.011566, mean_q: 0.018463, mean_eps: 0.952980\n",
            "   52559/1750000: episode: 211, duration: 3.662s, episode steps: 199, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002800, mae: 0.012925, mean_q: 0.017496, mean_eps: 0.952788\n",
            "   52793/1750000: episode: 212, duration: 4.313s, episode steps: 234, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001066, mae: 0.010460, mean_q: 0.016387, mean_eps: 0.952592\n",
            "   53069/1750000: episode: 213, duration: 5.073s, episode steps: 276, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002717, mae: 0.013178, mean_q: 0.016670, mean_eps: 0.952361\n",
            "   53471/1750000: episode: 214, duration: 7.306s, episode steps: 402, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002492, mae: 0.011750, mean_q: 0.015314, mean_eps: 0.952057\n",
            "   53784/1750000: episode: 215, duration: 5.761s, episode steps: 313, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001208, mae: 0.010796, mean_q: 0.015918, mean_eps: 0.951737\n",
            "   54058/1750000: episode: 216, duration: 5.055s, episode steps: 274, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001588, mae: 0.011063, mean_q: 0.016485, mean_eps: 0.951472\n",
            "   54287/1750000: episode: 217, duration: 4.167s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002733, mae: 0.012401, mean_q: 0.016148, mean_eps: 0.951245\n",
            "   54466/1750000: episode: 218, duration: 3.285s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002094, mae: 0.011332, mean_q: 0.016571, mean_eps: 0.951062\n",
            "   54767/1750000: episode: 219, duration: 5.451s, episode steps: 301, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.002290, mae: 0.012476, mean_q: 0.017223, mean_eps: 0.950846\n",
            "   55017/1750000: episode: 220, duration: 4.568s, episode steps: 250, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.004935, mae: 0.016655, mean_q: 0.021526, mean_eps: 0.950597\n",
            "   55290/1750000: episode: 221, duration: 4.979s, episode steps: 273, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.002755, mae: 0.014248, mean_q: 0.017521, mean_eps: 0.950361\n",
            "   55471/1750000: episode: 222, duration: 3.324s, episode steps: 181, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.003103, mae: 0.015285, mean_q: 0.019704, mean_eps: 0.950158\n",
            "   55775/1750000: episode: 223, duration: 5.577s, episode steps: 304, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002262, mae: 0.011869, mean_q: 0.017124, mean_eps: 0.949940\n",
            "   55948/1750000: episode: 224, duration: 3.240s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.003258, mae: 0.014854, mean_q: 0.019918, mean_eps: 0.949726\n",
            "   56219/1750000: episode: 225, duration: 4.997s, episode steps: 271, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.002298, mae: 0.012080, mean_q: 0.018540, mean_eps: 0.949526\n",
            "   56613/1750000: episode: 226, duration: 7.226s, episode steps: 394, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001420, mae: 0.011090, mean_q: 0.017252, mean_eps: 0.949226\n",
            "   56781/1750000: episode: 227, duration: 3.085s, episode steps: 168, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002973, mae: 0.013003, mean_q: 0.018396, mean_eps: 0.948972\n",
            "   56959/1750000: episode: 228, duration: 3.206s, episode steps: 178, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002137, mae: 0.011311, mean_q: 0.015167, mean_eps: 0.948817\n",
            "   57130/1750000: episode: 229, duration: 3.123s, episode steps: 171, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001817, mae: 0.011059, mean_q: 0.017231, mean_eps: 0.948660\n",
            "   57302/1750000: episode: 230, duration: 3.139s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.003254, mae: 0.013770, mean_q: 0.017121, mean_eps: 0.948506\n",
            "   57513/1750000: episode: 231, duration: 3.878s, episode steps: 211, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.002067, mae: 0.011821, mean_q: 0.015935, mean_eps: 0.948333\n",
            "   57746/1750000: episode: 232, duration: 4.277s, episode steps: 233, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000818, mae: 0.009483, mean_q: 0.013164, mean_eps: 0.948133\n",
            "   58046/1750000: episode: 233, duration: 5.482s, episode steps: 300, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.001659, mae: 0.010908, mean_q: 0.017794, mean_eps: 0.947894\n",
            "   58223/1750000: episode: 234, duration: 3.233s, episode steps: 177, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.004245, mae: 0.016309, mean_q: 0.019946, mean_eps: 0.947679\n",
            "   58535/1750000: episode: 235, duration: 5.710s, episode steps: 312, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002601, mae: 0.013194, mean_q: 0.016565, mean_eps: 0.947460\n",
            "   58821/1750000: episode: 236, duration: 5.248s, episode steps: 286, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002155, mae: 0.011468, mean_q: 0.017536, mean_eps: 0.947190\n",
            "   59064/1750000: episode: 237, duration: 4.436s, episode steps: 243, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.003110, mae: 0.013826, mean_q: 0.020171, mean_eps: 0.946952\n",
            "   59246/1750000: episode: 238, duration: 3.333s, episode steps: 182, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.004395, mae: 0.017003, mean_q: 0.020453, mean_eps: 0.946761\n",
            "   59417/1750000: episode: 239, duration: 3.112s, episode steps: 171, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.002539, mae: 0.013924, mean_q: 0.021821, mean_eps: 0.946601\n",
            "   59592/1750000: episode: 240, duration: 3.240s, episode steps: 175, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.001464, mae: 0.010153, mean_q: 0.014057, mean_eps: 0.946446\n",
            "   59775/1750000: episode: 241, duration: 3.332s, episode steps: 183, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.003053, mae: 0.014731, mean_q: 0.018428, mean_eps: 0.946286\n",
            "   60227/1750000: episode: 242, duration: 8.202s, episode steps: 452, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002073, mae: 0.013650, mean_q: 0.019650, mean_eps: 0.946000\n",
            "   60464/1750000: episode: 243, duration: 4.389s, episode steps: 237, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.004207, mae: 0.022069, mean_q: 0.028508, mean_eps: 0.945690\n",
            "   60732/1750000: episode: 244, duration: 4.975s, episode steps: 268, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.003237, mae: 0.020586, mean_q: 0.030247, mean_eps: 0.945464\n",
            "   61077/1750000: episode: 245, duration: 6.333s, episode steps: 345, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.003206, mae: 0.020113, mean_q: 0.030769, mean_eps: 0.945186\n",
            "   61319/1750000: episode: 246, duration: 4.406s, episode steps: 242, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.002857, mae: 0.021665, mean_q: 0.029687, mean_eps: 0.944922\n",
            "   61545/1750000: episode: 247, duration: 4.149s, episode steps: 226, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.003742, mae: 0.021590, mean_q: 0.028471, mean_eps: 0.944711\n",
            "   61848/1750000: episode: 248, duration: 5.578s, episode steps: 303, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.003552, mae: 0.022639, mean_q: 0.031599, mean_eps: 0.944474\n",
            "   62113/1750000: episode: 249, duration: 4.928s, episode steps: 265, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002224, mae: 0.018329, mean_q: 0.025942, mean_eps: 0.944218\n",
            "   62447/1750000: episode: 250, duration: 6.019s, episode steps: 334, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.002748, mae: 0.019960, mean_q: 0.027076, mean_eps: 0.943948\n",
            "   62755/1750000: episode: 251, duration: 5.569s, episode steps: 308, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.002442, mae: 0.020428, mean_q: 0.028260, mean_eps: 0.943660\n",
            "   63064/1750000: episode: 252, duration: 5.663s, episode steps: 309, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.002656, mae: 0.021291, mean_q: 0.029088, mean_eps: 0.943383\n",
            "   63228/1750000: episode: 253, duration: 3.018s, episode steps: 164, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001564, mae: 0.020072, mean_q: 0.031893, mean_eps: 0.943170\n",
            "   63594/1750000: episode: 254, duration: 6.673s, episode steps: 366, steps per second:  55, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002664, mae: 0.021368, mean_q: 0.031541, mean_eps: 0.942931\n",
            "   63883/1750000: episode: 255, duration: 5.196s, episode steps: 289, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.001972, mae: 0.020463, mean_q: 0.029978, mean_eps: 0.942636\n",
            "   64112/1750000: episode: 256, duration: 4.197s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.001873, mae: 0.019731, mean_q: 0.029086, mean_eps: 0.942404\n",
            "   64528/1750000: episode: 257, duration: 7.585s, episode steps: 416, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001810, mae: 0.018198, mean_q: 0.025625, mean_eps: 0.942114\n",
            "   64744/1750000: episode: 258, duration: 4.005s, episode steps: 216, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002086, mae: 0.017930, mean_q: 0.024867, mean_eps: 0.941829\n",
            "   64987/1750000: episode: 259, duration: 4.461s, episode steps: 243, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001264, mae: 0.017941, mean_q: 0.026392, mean_eps: 0.941622\n",
            "   65245/1750000: episode: 260, duration: 4.752s, episode steps: 258, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.001463, mae: 0.016951, mean_q: 0.023337, mean_eps: 0.941396\n",
            "   65543/1750000: episode: 261, duration: 5.335s, episode steps: 298, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.001515, mae: 0.018563, mean_q: 0.025131, mean_eps: 0.941145\n",
            "   65726/1750000: episode: 262, duration: 3.330s, episode steps: 183, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002210, mae: 0.019952, mean_q: 0.027136, mean_eps: 0.940929\n",
            "   65894/1750000: episode: 263, duration: 3.059s, episode steps: 168, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.002190, mae: 0.021189, mean_q: 0.028333, mean_eps: 0.940771\n",
            "   66147/1750000: episode: 264, duration: 4.554s, episode steps: 253, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001091, mae: 0.017291, mean_q: 0.023204, mean_eps: 0.940582\n",
            "   66493/1750000: episode: 265, duration: 6.352s, episode steps: 346, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.002198, mae: 0.020311, mean_q: 0.029029, mean_eps: 0.940312\n",
            "   66763/1750000: episode: 266, duration: 4.890s, episode steps: 270, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.001101, mae: 0.016345, mean_q: 0.022485, mean_eps: 0.940035\n",
            "   66931/1750000: episode: 267, duration: 3.042s, episode steps: 168, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: 0.003157, mae: 0.021627, mean_q: 0.028897, mean_eps: 0.939839\n",
            "   67124/1750000: episode: 268, duration: 3.539s, episode steps: 193, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001343, mae: 0.019851, mean_q: 0.027041, mean_eps: 0.939677\n",
            "   67297/1750000: episode: 269, duration: 3.244s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.001044, mae: 0.017120, mean_q: 0.024521, mean_eps: 0.939511\n",
            "   67668/1750000: episode: 270, duration: 6.726s, episode steps: 371, steps per second:  55, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.001328, mae: 0.018434, mean_q: 0.026258, mean_eps: 0.939266\n",
            "   67919/1750000: episode: 271, duration: 4.556s, episode steps: 251, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.000769, mae: 0.017543, mean_q: 0.026483, mean_eps: 0.938987\n",
            "   68150/1750000: episode: 272, duration: 4.189s, episode steps: 231, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.001637, mae: 0.017069, mean_q: 0.025356, mean_eps: 0.938769\n",
            "   68326/1750000: episode: 273, duration: 3.179s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.001591, mae: 0.017643, mean_q: 0.026499, mean_eps: 0.938586\n",
            "   68518/1750000: episode: 274, duration: 3.512s, episode steps: 192, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.510 [0.000, 3.000],  loss: 0.000750, mae: 0.017215, mean_q: 0.026596, mean_eps: 0.938420\n",
            "   68728/1750000: episode: 275, duration: 3.813s, episode steps: 210, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000908, mae: 0.016187, mean_q: 0.023616, mean_eps: 0.938240\n",
            "   68951/1750000: episode: 276, duration: 4.031s, episode steps: 223, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002591, mae: 0.022895, mean_q: 0.032822, mean_eps: 0.938046\n",
            "   69262/1750000: episode: 277, duration: 5.670s, episode steps: 311, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.001663, mae: 0.018749, mean_q: 0.026327, mean_eps: 0.937805\n",
            "   69538/1750000: episode: 278, duration: 5.044s, episode steps: 276, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.001350, mae: 0.018332, mean_q: 0.026143, mean_eps: 0.937540\n",
            "   69884/1750000: episode: 279, duration: 6.292s, episode steps: 346, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.001107, mae: 0.017215, mean_q: 0.024407, mean_eps: 0.937261\n",
            "   70161/1750000: episode: 280, duration: 5.049s, episode steps: 277, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001765, mae: 0.031146, mean_q: 0.041876, mean_eps: 0.936980\n",
            "   70338/1750000: episode: 281, duration: 3.213s, episode steps: 177, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.004101, mae: 0.045881, mean_q: 0.063749, mean_eps: 0.936775\n",
            "   70514/1750000: episode: 282, duration: 3.184s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001927, mae: 0.041137, mean_q: 0.060840, mean_eps: 0.936617\n",
            "   70674/1750000: episode: 283, duration: 2.915s, episode steps: 160, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002437, mae: 0.038299, mean_q: 0.052488, mean_eps: 0.936465\n",
            "   70865/1750000: episode: 284, duration: 3.477s, episode steps: 191, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.660 [0.000, 3.000],  loss: 0.001948, mae: 0.038307, mean_q: 0.050281, mean_eps: 0.936307\n",
            "   71179/1750000: episode: 285, duration: 5.679s, episode steps: 314, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002343, mae: 0.039425, mean_q: 0.053918, mean_eps: 0.936080\n",
            "   71408/1750000: episode: 286, duration: 4.199s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002175, mae: 0.039429, mean_q: 0.053551, mean_eps: 0.935837\n",
            "   71618/1750000: episode: 287, duration: 3.839s, episode steps: 210, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: 0.002136, mae: 0.040070, mean_q: 0.055278, mean_eps: 0.935639\n",
            "   71847/1750000: episode: 288, duration: 4.154s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.001912, mae: 0.039219, mean_q: 0.054186, mean_eps: 0.935441\n",
            "   72278/1750000: episode: 289, duration: 7.840s, episode steps: 431, steps per second:  55, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.001292, mae: 0.039743, mean_q: 0.053580, mean_eps: 0.935144\n",
            "   72518/1750000: episode: 290, duration: 4.343s, episode steps: 240, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001337, mae: 0.038172, mean_q: 0.051294, mean_eps: 0.934842\n",
            "   72754/1750000: episode: 291, duration: 4.280s, episode steps: 236, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001728, mae: 0.041604, mean_q: 0.055637, mean_eps: 0.934628\n",
            "   73028/1750000: episode: 292, duration: 4.994s, episode steps: 274, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.001781, mae: 0.039754, mean_q: 0.054954, mean_eps: 0.934399\n",
            "   73263/1750000: episode: 293, duration: 4.282s, episode steps: 235, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.001762, mae: 0.040048, mean_q: 0.055085, mean_eps: 0.934170\n",
            "   73585/1750000: episode: 294, duration: 5.883s, episode steps: 322, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001697, mae: 0.042193, mean_q: 0.058847, mean_eps: 0.933918\n",
            "   73891/1750000: episode: 295, duration: 5.559s, episode steps: 306, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.001992, mae: 0.041372, mean_q: 0.056169, mean_eps: 0.933636\n",
            "   74163/1750000: episode: 296, duration: 4.957s, episode steps: 272, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001400, mae: 0.040890, mean_q: 0.055885, mean_eps: 0.933377\n",
            "   74487/1750000: episode: 297, duration: 5.864s, episode steps: 324, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001342, mae: 0.038965, mean_q: 0.052359, mean_eps: 0.933108\n",
            "   74831/1750000: episode: 298, duration: 6.193s, episode steps: 344, steps per second:  56, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001317, mae: 0.039412, mean_q: 0.053175, mean_eps: 0.932808\n",
            "   75114/1750000: episode: 299, duration: 5.199s, episode steps: 283, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.000834, mae: 0.038662, mean_q: 0.052280, mean_eps: 0.932525\n",
            "   75295/1750000: episode: 300, duration: 3.266s, episode steps: 181, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.000796, mae: 0.036215, mean_q: 0.048003, mean_eps: 0.932316\n",
            "   75467/1750000: episode: 301, duration: 3.115s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000753, mae: 0.034340, mean_q: 0.046222, mean_eps: 0.932158\n",
            "   75745/1750000: episode: 302, duration: 5.049s, episode steps: 278, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000850, mae: 0.036416, mean_q: 0.050070, mean_eps: 0.931955\n",
            "   76118/1750000: episode: 303, duration: 6.755s, episode steps: 373, steps per second:  55, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.001322, mae: 0.042848, mean_q: 0.057820, mean_eps: 0.931661\n",
            "   76298/1750000: episode: 304, duration: 3.255s, episode steps: 180, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.001101, mae: 0.039816, mean_q: 0.054736, mean_eps: 0.931413\n",
            "   76707/1750000: episode: 305, duration: 7.365s, episode steps: 409, steps per second:  56, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001241, mae: 0.038023, mean_q: 0.052334, mean_eps: 0.931148\n",
            "   76885/1750000: episode: 306, duration: 3.253s, episode steps: 178, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001050, mae: 0.039076, mean_q: 0.053724, mean_eps: 0.930884\n",
            "   77064/1750000: episode: 307, duration: 3.240s, episode steps: 179, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000938, mae: 0.037698, mean_q: 0.051439, mean_eps: 0.930723\n",
            "   77449/1750000: episode: 308, duration: 7.026s, episode steps: 385, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001844, mae: 0.042711, mean_q: 0.057109, mean_eps: 0.930470\n",
            "   77727/1750000: episode: 309, duration: 5.042s, episode steps: 278, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.360 [0.000, 3.000],  loss: 0.001376, mae: 0.037863, mean_q: 0.049810, mean_eps: 0.930171\n",
            "   77917/1750000: episode: 310, duration: 3.474s, episode steps: 190, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.321 [0.000, 3.000],  loss: 0.000969, mae: 0.037451, mean_q: 0.052519, mean_eps: 0.929960\n",
            "   78150/1750000: episode: 311, duration: 4.249s, episode steps: 233, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.001117, mae: 0.037406, mean_q: 0.050221, mean_eps: 0.929769\n",
            "   78388/1750000: episode: 312, duration: 4.375s, episode steps: 238, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001870, mae: 0.043024, mean_q: 0.056280, mean_eps: 0.929559\n",
            "   78564/1750000: episode: 313, duration: 3.300s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.000627, mae: 0.040931, mean_q: 0.056735, mean_eps: 0.929373\n",
            "   78779/1750000: episode: 314, duration: 3.976s, episode steps: 215, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000975, mae: 0.039859, mean_q: 0.054651, mean_eps: 0.929197\n",
            "   78996/1750000: episode: 315, duration: 4.006s, episode steps: 217, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.001617, mae: 0.040033, mean_q: 0.054701, mean_eps: 0.929003\n",
            "   79316/1750000: episode: 316, duration: 5.865s, episode steps: 320, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001048, mae: 0.039946, mean_q: 0.053847, mean_eps: 0.928761\n",
            "   79558/1750000: episode: 317, duration: 4.422s, episode steps: 242, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001152, mae: 0.041000, mean_q: 0.055293, mean_eps: 0.928508\n",
            "   79738/1750000: episode: 318, duration: 3.240s, episode steps: 180, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000865, mae: 0.040570, mean_q: 0.054353, mean_eps: 0.928317\n",
            "   79926/1750000: episode: 319, duration: 3.414s, episode steps: 188, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000845, mae: 0.037612, mean_q: 0.049704, mean_eps: 0.928151\n",
            "   80115/1750000: episode: 320, duration: 3.407s, episode steps: 189, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.003199, mae: 0.049334, mean_q: 0.065309, mean_eps: 0.927982\n",
            "   80322/1750000: episode: 321, duration: 3.770s, episode steps: 207, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.362 [0.000, 3.000],  loss: 0.003231, mae: 0.050011, mean_q: 0.069391, mean_eps: 0.927804\n",
            "   80500/1750000: episode: 322, duration: 3.260s, episode steps: 178, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.001746, mae: 0.044848, mean_q: 0.061234, mean_eps: 0.927631\n",
            "   80732/1750000: episode: 323, duration: 4.270s, episode steps: 232, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.002397, mae: 0.049734, mean_q: 0.066261, mean_eps: 0.927447\n",
            "   80922/1750000: episode: 324, duration: 3.462s, episode steps: 190, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.003038, mae: 0.047636, mean_q: 0.062227, mean_eps: 0.927257\n",
            "   81157/1750000: episode: 325, duration: 4.260s, episode steps: 235, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002231, mae: 0.052795, mean_q: 0.069095, mean_eps: 0.927064\n",
            "   81433/1750000: episode: 326, duration: 5.019s, episode steps: 276, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001219, mae: 0.043060, mean_q: 0.057097, mean_eps: 0.926834\n",
            "   81718/1750000: episode: 327, duration: 5.137s, episode steps: 285, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001282, mae: 0.046094, mean_q: 0.061364, mean_eps: 0.926582\n",
            "   82050/1750000: episode: 328, duration: 5.997s, episode steps: 332, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000954, mae: 0.046562, mean_q: 0.061763, mean_eps: 0.926304\n",
            "   82218/1750000: episode: 329, duration: 3.051s, episode steps: 168, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001242, mae: 0.044981, mean_q: 0.058785, mean_eps: 0.926079\n",
            "   82490/1750000: episode: 330, duration: 4.914s, episode steps: 272, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001635, mae: 0.048191, mean_q: 0.063818, mean_eps: 0.925881\n",
            "   82725/1750000: episode: 331, duration: 4.307s, episode steps: 235, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.001376, mae: 0.046040, mean_q: 0.059915, mean_eps: 0.925653\n",
            "   83038/1750000: episode: 332, duration: 5.688s, episode steps: 313, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001355, mae: 0.049422, mean_q: 0.066723, mean_eps: 0.925406\n",
            "   83208/1750000: episode: 333, duration: 3.123s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001187, mae: 0.044249, mean_q: 0.061933, mean_eps: 0.925190\n",
            "   83377/1750000: episode: 334, duration: 3.102s, episode steps: 169, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.249 [0.000, 3.000],  loss: 0.001748, mae: 0.047408, mean_q: 0.064935, mean_eps: 0.925037\n",
            "   83662/1750000: episode: 335, duration: 5.135s, episode steps: 285, steps per second:  56, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.001230, mae: 0.047481, mean_q: 0.063858, mean_eps: 0.924832\n",
            "   83997/1750000: episode: 336, duration: 6.080s, episode steps: 335, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.001069, mae: 0.050987, mean_q: 0.067492, mean_eps: 0.924553\n",
            "   84311/1750000: episode: 337, duration: 5.663s, episode steps: 314, steps per second:  55, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.001493, mae: 0.049268, mean_q: 0.067155, mean_eps: 0.924261\n",
            "   84580/1750000: episode: 338, duration: 4.896s, episode steps: 269, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000797, mae: 0.043859, mean_q: 0.059556, mean_eps: 0.924000\n",
            "   84744/1750000: episode: 339, duration: 3.046s, episode steps: 164, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000848, mae: 0.040975, mean_q: 0.057135, mean_eps: 0.923806\n",
            "   84909/1750000: episode: 340, duration: 3.059s, episode steps: 165, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000546, mae: 0.044668, mean_q: 0.060102, mean_eps: 0.923657\n",
            "   85302/1750000: episode: 341, duration: 7.125s, episode steps: 393, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.000976, mae: 0.047014, mean_q: 0.064307, mean_eps: 0.923405\n",
            "   85479/1750000: episode: 342, duration: 3.199s, episode steps: 177, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000990, mae: 0.046090, mean_q: 0.062427, mean_eps: 0.923149\n",
            "   85758/1750000: episode: 343, duration: 5.113s, episode steps: 279, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000664, mae: 0.045922, mean_q: 0.062824, mean_eps: 0.922944\n",
            "   86092/1750000: episode: 344, duration: 6.098s, episode steps: 334, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000677, mae: 0.050257, mean_q: 0.067397, mean_eps: 0.922668\n",
            "   86325/1750000: episode: 345, duration: 4.276s, episode steps: 233, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001102, mae: 0.051180, mean_q: 0.069001, mean_eps: 0.922413\n",
            "   86554/1750000: episode: 346, duration: 4.147s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.001334, mae: 0.048727, mean_q: 0.063892, mean_eps: 0.922204\n",
            "   86759/1750000: episode: 347, duration: 3.706s, episode steps: 205, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 0.000976, mae: 0.046192, mean_q: 0.062434, mean_eps: 0.922010\n",
            "   86938/1750000: episode: 348, duration: 3.287s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000831, mae: 0.042315, mean_q: 0.057595, mean_eps: 0.921837\n",
            "   87193/1750000: episode: 349, duration: 4.685s, episode steps: 255, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001098, mae: 0.043379, mean_q: 0.058611, mean_eps: 0.921641\n",
            "   87443/1750000: episode: 350, duration: 4.545s, episode steps: 250, steps per second:  55, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.000393, mae: 0.040627, mean_q: 0.054262, mean_eps: 0.921414\n",
            "   87698/1750000: episode: 351, duration: 4.635s, episode steps: 255, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001283, mae: 0.046969, mean_q: 0.063625, mean_eps: 0.921187\n",
            "   87992/1750000: episode: 352, duration: 5.337s, episode steps: 294, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000675, mae: 0.050081, mean_q: 0.068064, mean_eps: 0.920940\n",
            "   88343/1750000: episode: 353, duration: 6.375s, episode steps: 351, steps per second:  55, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000649, mae: 0.044671, mean_q: 0.059878, mean_eps: 0.920651\n",
            "   88625/1750000: episode: 354, duration: 5.135s, episode steps: 282, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000846, mae: 0.048218, mean_q: 0.064817, mean_eps: 0.920364\n",
            "   88855/1750000: episode: 355, duration: 4.137s, episode steps: 230, steps per second:  56, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000600, mae: 0.045816, mean_q: 0.060900, mean_eps: 0.920134\n",
            "   89065/1750000: episode: 356, duration: 3.818s, episode steps: 210, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000658, mae: 0.045357, mean_q: 0.061919, mean_eps: 0.919936\n",
            "   89240/1750000: episode: 357, duration: 3.186s, episode steps: 175, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.371 [0.000, 3.000],  loss: 0.001187, mae: 0.050207, mean_q: 0.068328, mean_eps: 0.919763\n",
            "   89453/1750000: episode: 358, duration: 3.885s, episode steps: 213, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.000592, mae: 0.043707, mean_q: 0.059295, mean_eps: 0.919589\n",
            "   89670/1750000: episode: 359, duration: 3.913s, episode steps: 217, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001085, mae: 0.048399, mean_q: 0.064003, mean_eps: 0.919394\n",
            "   89976/1750000: episode: 360, duration: 5.565s, episode steps: 306, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 0.000917, mae: 0.043776, mean_q: 0.058345, mean_eps: 0.919160\n",
            "   90206/1750000: episode: 361, duration: 4.190s, episode steps: 230, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.003989, mae: 0.058629, mean_q: 0.077808, mean_eps: 0.918919\n",
            "   90382/1750000: episode: 362, duration: 3.209s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002476, mae: 0.053110, mean_q: 0.070124, mean_eps: 0.918735\n",
            "   90564/1750000: episode: 363, duration: 3.305s, episode steps: 182, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001803, mae: 0.056678, mean_q: 0.073516, mean_eps: 0.918575\n",
            "   90738/1750000: episode: 364, duration: 3.163s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.374 [0.000, 3.000],  loss: 0.001771, mae: 0.052859, mean_q: 0.071997, mean_eps: 0.918415\n",
            "   90941/1750000: episode: 365, duration: 3.707s, episode steps: 203, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.003007, mae: 0.049656, mean_q: 0.066147, mean_eps: 0.918244\n",
            "   91113/1750000: episode: 366, duration: 3.110s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.001599, mae: 0.052363, mean_q: 0.070889, mean_eps: 0.918075\n",
            "   91391/1750000: episode: 367, duration: 5.013s, episode steps: 278, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001799, mae: 0.054284, mean_q: 0.074678, mean_eps: 0.917873\n",
            "   91575/1750000: episode: 368, duration: 3.355s, episode steps: 184, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000765, mae: 0.050117, mean_q: 0.067619, mean_eps: 0.917666\n",
            "   92020/1750000: episode: 369, duration: 8.108s, episode steps: 445, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.001559, mae: 0.049301, mean_q: 0.065055, mean_eps: 0.917384\n",
            "   92208/1750000: episode: 370, duration: 3.491s, episode steps: 188, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.378 [0.000, 3.000],  loss: 0.001527, mae: 0.047850, mean_q: 0.063155, mean_eps: 0.917099\n",
            "   92449/1750000: episode: 371, duration: 4.395s, episode steps: 241, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.001318, mae: 0.053383, mean_q: 0.071562, mean_eps: 0.916905\n",
            "   92631/1750000: episode: 372, duration: 3.267s, episode steps: 182, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001024, mae: 0.049655, mean_q: 0.067991, mean_eps: 0.916714\n",
            "   92818/1750000: episode: 373, duration: 3.383s, episode steps: 187, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000815, mae: 0.047025, mean_q: 0.064695, mean_eps: 0.916548\n",
            "   93154/1750000: episode: 374, duration: 6.063s, episode steps: 336, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001436, mae: 0.054300, mean_q: 0.072874, mean_eps: 0.916313\n",
            "   93358/1750000: episode: 375, duration: 3.691s, episode steps: 204, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.000993, mae: 0.052836, mean_q: 0.072897, mean_eps: 0.916070\n",
            "   93577/1750000: episode: 376, duration: 3.963s, episode steps: 219, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.001367, mae: 0.055934, mean_q: 0.075293, mean_eps: 0.915879\n",
            "   93824/1750000: episode: 377, duration: 4.486s, episode steps: 247, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001483, mae: 0.049617, mean_q: 0.066663, mean_eps: 0.915670\n",
            "   94016/1750000: episode: 378, duration: 3.561s, episode steps: 192, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001489, mae: 0.048769, mean_q: 0.065737, mean_eps: 0.915474\n",
            "   94186/1750000: episode: 379, duration: 3.109s, episode steps: 170, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.001478, mae: 0.051482, mean_q: 0.069012, mean_eps: 0.915310\n",
            "   94466/1750000: episode: 380, duration: 5.064s, episode steps: 280, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000803, mae: 0.052703, mean_q: 0.071650, mean_eps: 0.915107\n",
            "   94651/1750000: episode: 381, duration: 3.342s, episode steps: 185, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 0.001472, mae: 0.050683, mean_q: 0.067151, mean_eps: 0.914898\n",
            "   95112/1750000: episode: 382, duration: 8.401s, episode steps: 461, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001539, mae: 0.056180, mean_q: 0.077424, mean_eps: 0.914608\n",
            "   95492/1750000: episode: 383, duration: 6.973s, episode steps: 380, steps per second:  54, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.000986, mae: 0.049729, mean_q: 0.066220, mean_eps: 0.914230\n",
            "   95727/1750000: episode: 384, duration: 4.329s, episode steps: 235, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000768, mae: 0.048326, mean_q: 0.065152, mean_eps: 0.913953\n",
            "   95975/1750000: episode: 385, duration: 4.565s, episode steps: 248, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.001825, mae: 0.058503, mean_q: 0.077109, mean_eps: 0.913735\n",
            "   96169/1750000: episode: 386, duration: 3.597s, episode steps: 194, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001311, mae: 0.056802, mean_q: 0.075737, mean_eps: 0.913535\n",
            "   96358/1750000: episode: 387, duration: 3.442s, episode steps: 189, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000600, mae: 0.050291, mean_q: 0.068258, mean_eps: 0.913362\n",
            "   96710/1750000: episode: 388, duration: 6.375s, episode steps: 352, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001011, mae: 0.051359, mean_q: 0.068193, mean_eps: 0.913119\n",
            "   96970/1750000: episode: 389, duration: 4.680s, episode steps: 260, steps per second:  56, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001644, mae: 0.051693, mean_q: 0.072057, mean_eps: 0.912844\n",
            "   97154/1750000: episode: 390, duration: 3.330s, episode steps: 184, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000863, mae: 0.050062, mean_q: 0.068138, mean_eps: 0.912644\n",
            "   97364/1750000: episode: 391, duration: 3.831s, episode steps: 210, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001159, mae: 0.053708, mean_q: 0.072768, mean_eps: 0.912468\n",
            "   97589/1750000: episode: 392, duration: 4.133s, episode steps: 225, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000806, mae: 0.054935, mean_q: 0.074058, mean_eps: 0.912272\n",
            "   97944/1750000: episode: 393, duration: 6.430s, episode steps: 355, steps per second:  55, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001057, mae: 0.053624, mean_q: 0.071950, mean_eps: 0.912011\n",
            "   98188/1750000: episode: 394, duration: 4.469s, episode steps: 244, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000738, mae: 0.058018, mean_q: 0.077147, mean_eps: 0.911742\n",
            "   98368/1750000: episode: 395, duration: 3.319s, episode steps: 180, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000749, mae: 0.052238, mean_q: 0.070370, mean_eps: 0.911552\n",
            "   98547/1750000: episode: 396, duration: 3.250s, episode steps: 179, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.257 [0.000, 3.000],  loss: 0.000368, mae: 0.044919, mean_q: 0.062218, mean_eps: 0.911390\n",
            "   98776/1750000: episode: 397, duration: 4.175s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.001192, mae: 0.047322, mean_q: 0.063424, mean_eps: 0.911206\n",
            "   98950/1750000: episode: 398, duration: 3.176s, episode steps: 174, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000784, mae: 0.048793, mean_q: 0.065208, mean_eps: 0.911024\n",
            "   99243/1750000: episode: 399, duration: 5.296s, episode steps: 293, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001056, mae: 0.052956, mean_q: 0.071253, mean_eps: 0.910814\n",
            "   99484/1750000: episode: 400, duration: 4.428s, episode steps: 241, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.001145, mae: 0.049692, mean_q: 0.068445, mean_eps: 0.910574\n",
            "   99717/1750000: episode: 401, duration: 4.269s, episode steps: 233, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.000981, mae: 0.050453, mean_q: 0.067931, mean_eps: 0.910360\n",
            "  100169/1750000: episode: 402, duration: 8.184s, episode steps: 452, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.001087, mae: 0.052503, mean_q: 0.069857, mean_eps: 0.910050\n",
            "  100420/1750000: episode: 403, duration: 4.609s, episode steps: 251, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.002261, mae: 0.055877, mean_q: 0.074366, mean_eps: 0.909735\n",
            "  100599/1750000: episode: 404, duration: 3.289s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001762, mae: 0.060298, mean_q: 0.081047, mean_eps: 0.909543\n",
            "  100861/1750000: episode: 405, duration: 4.813s, episode steps: 262, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001616, mae: 0.054308, mean_q: 0.072716, mean_eps: 0.909343\n",
            "  101046/1750000: episode: 406, duration: 3.340s, episode steps: 185, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.314 [0.000, 3.000],  loss: 0.001055, mae: 0.053312, mean_q: 0.071385, mean_eps: 0.909141\n",
            "  101217/1750000: episode: 407, duration: 3.136s, episode steps: 171, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.363 [0.000, 3.000],  loss: 0.002082, mae: 0.057400, mean_q: 0.075700, mean_eps: 0.908981\n",
            "  101396/1750000: episode: 408, duration: 3.262s, episode steps: 179, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.001963, mae: 0.057452, mean_q: 0.078434, mean_eps: 0.908825\n",
            "  101643/1750000: episode: 409, duration: 4.523s, episode steps: 247, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001394, mae: 0.056455, mean_q: 0.075807, mean_eps: 0.908634\n",
            "  101937/1750000: episode: 410, duration: 5.342s, episode steps: 294, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.002087, mae: 0.058216, mean_q: 0.077148, mean_eps: 0.908389\n",
            "  102121/1750000: episode: 411, duration: 3.337s, episode steps: 184, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.001290, mae: 0.052366, mean_q: 0.071007, mean_eps: 0.908173\n",
            "  102378/1750000: episode: 412, duration: 4.657s, episode steps: 257, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.374 [0.000, 3.000],  loss: 0.002074, mae: 0.057636, mean_q: 0.077361, mean_eps: 0.907975\n",
            "  102550/1750000: episode: 413, duration: 3.207s, episode steps: 172, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001316, mae: 0.062494, mean_q: 0.082746, mean_eps: 0.907782\n",
            "  102809/1750000: episode: 414, duration: 4.776s, episode steps: 259, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001535, mae: 0.056925, mean_q: 0.075226, mean_eps: 0.907588\n",
            "  103074/1750000: episode: 415, duration: 4.839s, episode steps: 265, steps per second:  55, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.001980, mae: 0.059397, mean_q: 0.081756, mean_eps: 0.907352\n",
            "  103283/1750000: episode: 416, duration: 3.819s, episode steps: 209, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.335 [0.000, 3.000],  loss: 0.001197, mae: 0.055900, mean_q: 0.074928, mean_eps: 0.907140\n",
            "  103461/1750000: episode: 417, duration: 3.295s, episode steps: 178, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.337 [0.000, 3.000],  loss: 0.001483, mae: 0.061657, mean_q: 0.082089, mean_eps: 0.906965\n",
            "  103737/1750000: episode: 418, duration: 5.124s, episode steps: 276, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000797, mae: 0.057122, mean_q: 0.077018, mean_eps: 0.906760\n",
            "  104147/1750000: episode: 419, duration: 7.491s, episode steps: 410, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000934, mae: 0.054752, mean_q: 0.073593, mean_eps: 0.906452\n",
            "  104441/1750000: episode: 420, duration: 5.465s, episode steps: 294, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001014, mae: 0.053865, mean_q: 0.071956, mean_eps: 0.906135\n",
            "  104678/1750000: episode: 421, duration: 4.388s, episode steps: 237, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000534, mae: 0.047753, mean_q: 0.063771, mean_eps: 0.905896\n",
            "  104900/1750000: episode: 422, duration: 4.167s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001789, mae: 0.057181, mean_q: 0.076390, mean_eps: 0.905691\n",
            "  105086/1750000: episode: 423, duration: 3.548s, episode steps: 186, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000949, mae: 0.053808, mean_q: 0.072519, mean_eps: 0.905507\n",
            "  105258/1750000: episode: 424, duration: 3.206s, episode steps: 172, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.001058, mae: 0.051010, mean_q: 0.067978, mean_eps: 0.905345\n",
            "  105424/1750000: episode: 425, duration: 3.137s, episode steps: 166, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001092, mae: 0.056529, mean_q: 0.075790, mean_eps: 0.905194\n",
            "  105725/1750000: episode: 426, duration: 5.628s, episode steps: 301, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000776, mae: 0.049190, mean_q: 0.065252, mean_eps: 0.904983\n",
            "  106017/1750000: episode: 427, duration: 5.455s, episode steps: 292, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001059, mae: 0.053831, mean_q: 0.072179, mean_eps: 0.904715\n",
            "  106234/1750000: episode: 428, duration: 4.032s, episode steps: 217, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000577, mae: 0.053167, mean_q: 0.070596, mean_eps: 0.904487\n",
            "  106407/1750000: episode: 429, duration: 3.129s, episode steps: 173, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000611, mae: 0.058163, mean_q: 0.077909, mean_eps: 0.904312\n",
            "  106616/1750000: episode: 430, duration: 3.879s, episode steps: 209, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.001081, mae: 0.056645, mean_q: 0.077288, mean_eps: 0.904141\n",
            "  106790/1750000: episode: 431, duration: 3.194s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000578, mae: 0.051899, mean_q: 0.070055, mean_eps: 0.903968\n",
            "  107195/1750000: episode: 432, duration: 7.320s, episode steps: 405, steps per second:  55, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000541, mae: 0.050129, mean_q: 0.067498, mean_eps: 0.903707\n",
            "  107511/1750000: episode: 433, duration: 5.726s, episode steps: 316, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.000880, mae: 0.055171, mean_q: 0.074637, mean_eps: 0.903383\n",
            "  107690/1750000: episode: 434, duration: 3.287s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000947, mae: 0.051407, mean_q: 0.069230, mean_eps: 0.903160\n",
            "  107973/1750000: episode: 435, duration: 5.180s, episode steps: 283, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000935, mae: 0.058072, mean_q: 0.078204, mean_eps: 0.902951\n",
            "  108158/1750000: episode: 436, duration: 3.360s, episode steps: 185, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.232 [0.000, 3.000],  loss: 0.001006, mae: 0.054506, mean_q: 0.073113, mean_eps: 0.902741\n",
            "  108452/1750000: episode: 437, duration: 5.387s, episode steps: 294, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001084, mae: 0.053470, mean_q: 0.071448, mean_eps: 0.902526\n",
            "  108690/1750000: episode: 438, duration: 4.335s, episode steps: 238, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.000667, mae: 0.055941, mean_q: 0.075100, mean_eps: 0.902287\n",
            "  108957/1750000: episode: 439, duration: 4.877s, episode steps: 267, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001238, mae: 0.058053, mean_q: 0.078989, mean_eps: 0.902058\n",
            "  109209/1750000: episode: 440, duration: 4.629s, episode steps: 252, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.000805, mae: 0.055181, mean_q: 0.074934, mean_eps: 0.901824\n",
            "  109631/1750000: episode: 441, duration: 7.677s, episode steps: 422, steps per second:  55, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.000607, mae: 0.055167, mean_q: 0.075349, mean_eps: 0.901522\n",
            "  109971/1750000: episode: 442, duration: 6.152s, episode steps: 340, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000825, mae: 0.049974, mean_q: 0.067484, mean_eps: 0.901180\n",
            "  110145/1750000: episode: 443, duration: 3.208s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001866, mae: 0.062607, mean_q: 0.084153, mean_eps: 0.900948\n",
            "  110413/1750000: episode: 444, duration: 4.899s, episode steps: 268, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001419, mae: 0.065902, mean_q: 0.087830, mean_eps: 0.900748\n",
            "  110682/1750000: episode: 445, duration: 4.871s, episode steps: 269, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.001488, mae: 0.062140, mean_q: 0.084310, mean_eps: 0.900507\n",
            "  110959/1750000: episode: 446, duration: 5.004s, episode steps: 277, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001317, mae: 0.062062, mean_q: 0.083742, mean_eps: 0.900262\n",
            "  111322/1750000: episode: 447, duration: 6.652s, episode steps: 363, steps per second:  55, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001358, mae: 0.068713, mean_q: 0.092182, mean_eps: 0.899974\n",
            "  111644/1750000: episode: 448, duration: 5.958s, episode steps: 322, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.320 [0.000, 3.000],  loss: 0.000984, mae: 0.058934, mean_q: 0.079998, mean_eps: 0.899666\n",
            "  111885/1750000: episode: 449, duration: 4.449s, episode steps: 241, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000927, mae: 0.061599, mean_q: 0.082478, mean_eps: 0.899412\n",
            "  112164/1750000: episode: 450, duration: 5.172s, episode steps: 279, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000841, mae: 0.059689, mean_q: 0.079071, mean_eps: 0.899178\n",
            "  112409/1750000: episode: 451, duration: 4.503s, episode steps: 245, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.000936, mae: 0.063833, mean_q: 0.085236, mean_eps: 0.898943\n",
            "  112812/1750000: episode: 452, duration: 7.395s, episode steps: 403, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.001136, mae: 0.067895, mean_q: 0.091689, mean_eps: 0.898651\n",
            "  113054/1750000: episode: 453, duration: 4.477s, episode steps: 242, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001042, mae: 0.062286, mean_q: 0.083661, mean_eps: 0.898361\n",
            "  113226/1750000: episode: 454, duration: 3.129s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.401 [0.000, 3.000],  loss: 0.000907, mae: 0.068510, mean_q: 0.089903, mean_eps: 0.898174\n",
            "  113713/1750000: episode: 455, duration: 8.922s, episode steps: 487, steps per second:  55, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001179, mae: 0.065946, mean_q: 0.088477, mean_eps: 0.897877\n",
            "  113946/1750000: episode: 456, duration: 4.279s, episode steps: 233, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001219, mae: 0.061350, mean_q: 0.081247, mean_eps: 0.897553\n",
            "  114216/1750000: episode: 457, duration: 4.939s, episode steps: 270, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.000896, mae: 0.058600, mean_q: 0.078313, mean_eps: 0.897328\n",
            "  114555/1750000: episode: 458, duration: 6.199s, episode steps: 339, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 0.001036, mae: 0.058317, mean_q: 0.077913, mean_eps: 0.897054\n",
            "  114726/1750000: episode: 459, duration: 3.158s, episode steps: 171, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000699, mae: 0.066021, mean_q: 0.087099, mean_eps: 0.896824\n",
            "  114931/1750000: episode: 460, duration: 3.729s, episode steps: 205, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000820, mae: 0.066352, mean_q: 0.088511, mean_eps: 0.896655\n",
            "  115229/1750000: episode: 461, duration: 5.453s, episode steps: 298, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000563, mae: 0.065108, mean_q: 0.086940, mean_eps: 0.896428\n",
            "  115456/1750000: episode: 462, duration: 4.130s, episode steps: 227, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000993, mae: 0.060379, mean_q: 0.080210, mean_eps: 0.896192\n",
            "  115670/1750000: episode: 463, duration: 3.963s, episode steps: 214, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001265, mae: 0.067167, mean_q: 0.090640, mean_eps: 0.895994\n",
            "  115840/1750000: episode: 464, duration: 3.122s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: 0.000754, mae: 0.064186, mean_q: 0.085074, mean_eps: 0.895821\n",
            "  116009/1750000: episode: 465, duration: 3.160s, episode steps: 169, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001156, mae: 0.061113, mean_q: 0.081751, mean_eps: 0.895668\n",
            "  116283/1750000: episode: 466, duration: 4.996s, episode steps: 274, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000967, mae: 0.067846, mean_q: 0.090763, mean_eps: 0.895469\n",
            "  116725/1750000: episode: 467, duration: 8.085s, episode steps: 442, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000718, mae: 0.065245, mean_q: 0.087477, mean_eps: 0.895146\n",
            "  116901/1750000: episode: 468, duration: 3.211s, episode steps: 176, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.767 [0.000, 3.000],  loss: 0.000971, mae: 0.060908, mean_q: 0.082108, mean_eps: 0.894867\n",
            "  117183/1750000: episode: 469, duration: 5.117s, episode steps: 282, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000733, mae: 0.061430, mean_q: 0.083453, mean_eps: 0.894662\n",
            "  117427/1750000: episode: 470, duration: 4.547s, episode steps: 244, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000706, mae: 0.057744, mean_q: 0.077842, mean_eps: 0.894426\n",
            "  117709/1750000: episode: 471, duration: 5.164s, episode steps: 282, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000669, mae: 0.069847, mean_q: 0.094603, mean_eps: 0.894189\n",
            "  117923/1750000: episode: 472, duration: 3.914s, episode steps: 214, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000628, mae: 0.066760, mean_q: 0.088343, mean_eps: 0.893966\n",
            "  118289/1750000: episode: 473, duration: 6.732s, episode steps: 366, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000557, mae: 0.062392, mean_q: 0.082858, mean_eps: 0.893705\n",
            "  118490/1750000: episode: 474, duration: 3.690s, episode steps: 201, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.000795, mae: 0.070532, mean_q: 0.094242, mean_eps: 0.893449\n",
            "  118933/1750000: episode: 475, duration: 8.124s, episode steps: 443, steps per second:  55, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000785, mae: 0.067440, mean_q: 0.090275, mean_eps: 0.893159\n",
            "  119141/1750000: episode: 476, duration: 3.797s, episode steps: 208, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.001082, mae: 0.062501, mean_q: 0.083919, mean_eps: 0.892866\n",
            "  119354/1750000: episode: 477, duration: 3.893s, episode steps: 213, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.000485, mae: 0.062399, mean_q: 0.083543, mean_eps: 0.892677\n",
            "  119660/1750000: episode: 478, duration: 5.731s, episode steps: 306, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000938, mae: 0.069640, mean_q: 0.092842, mean_eps: 0.892445\n",
            "  120058/1750000: episode: 479, duration: 7.385s, episode steps: 398, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.000588, mae: 0.060394, mean_q: 0.081202, mean_eps: 0.892128\n",
            "  120228/1750000: episode: 480, duration: 3.195s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002752, mae: 0.069936, mean_q: 0.096105, mean_eps: 0.891872\n",
            "  120605/1750000: episode: 481, duration: 7.018s, episode steps: 377, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001399, mae: 0.078719, mean_q: 0.105490, mean_eps: 0.891626\n",
            "  120852/1750000: episode: 482, duration: 4.661s, episode steps: 247, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000939, mae: 0.074337, mean_q: 0.097902, mean_eps: 0.891345\n",
            "  121041/1750000: episode: 483, duration: 3.618s, episode steps: 189, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001618, mae: 0.077355, mean_q: 0.102511, mean_eps: 0.891149\n",
            "  121369/1750000: episode: 484, duration: 6.141s, episode steps: 328, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001300, mae: 0.076365, mean_q: 0.100525, mean_eps: 0.890915\n",
            "  121536/1750000: episode: 485, duration: 3.146s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001335, mae: 0.075243, mean_q: 0.100115, mean_eps: 0.890693\n",
            "  121701/1750000: episode: 486, duration: 3.141s, episode steps: 165, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.388 [0.000, 3.000],  loss: 0.001222, mae: 0.076386, mean_q: 0.104617, mean_eps: 0.890544\n",
            "  121867/1750000: episode: 487, duration: 3.090s, episode steps: 166, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001295, mae: 0.074321, mean_q: 0.101024, mean_eps: 0.890394\n",
            "  122032/1750000: episode: 488, duration: 3.071s, episode steps: 165, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001234, mae: 0.078770, mean_q: 0.105316, mean_eps: 0.890247\n",
            "  122271/1750000: episode: 489, duration: 4.410s, episode steps: 239, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000617, mae: 0.067448, mean_q: 0.092604, mean_eps: 0.890065\n",
            "  122523/1750000: episode: 490, duration: 4.630s, episode steps: 252, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.000943, mae: 0.075610, mean_q: 0.101409, mean_eps: 0.889844\n",
            "  122701/1750000: episode: 491, duration: 3.338s, episode steps: 178, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001022, mae: 0.072614, mean_q: 0.096914, mean_eps: 0.889649\n",
            "  122988/1750000: episode: 492, duration: 5.249s, episode steps: 287, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000891, mae: 0.070355, mean_q: 0.093746, mean_eps: 0.889440\n",
            "  123159/1750000: episode: 493, duration: 3.138s, episode steps: 171, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.001254, mae: 0.066071, mean_q: 0.088344, mean_eps: 0.889235\n",
            "  123466/1750000: episode: 494, duration: 5.601s, episode steps: 307, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000926, mae: 0.074676, mean_q: 0.100299, mean_eps: 0.889019\n",
            "  123768/1750000: episode: 495, duration: 5.520s, episode steps: 302, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.000705, mae: 0.072107, mean_q: 0.097061, mean_eps: 0.888746\n",
            "  124087/1750000: episode: 496, duration: 5.836s, episode steps: 319, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.000826, mae: 0.076286, mean_q: 0.101980, mean_eps: 0.888467\n",
            "  124428/1750000: episode: 497, duration: 6.279s, episode steps: 341, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.000933, mae: 0.078111, mean_q: 0.103726, mean_eps: 0.888170\n",
            "  124746/1750000: episode: 498, duration: 5.829s, episode steps: 318, steps per second:  55, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000610, mae: 0.075685, mean_q: 0.101061, mean_eps: 0.887873\n",
            "  124982/1750000: episode: 499, duration: 4.296s, episode steps: 236, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 0.001015, mae: 0.070982, mean_q: 0.094516, mean_eps: 0.887622\n",
            "  125187/1750000: episode: 500, duration: 3.721s, episode steps: 205, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 0.000519, mae: 0.063901, mean_q: 0.085087, mean_eps: 0.887424\n",
            "  125425/1750000: episode: 501, duration: 4.364s, episode steps: 238, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.000479, mae: 0.069232, mean_q: 0.092346, mean_eps: 0.887225\n",
            "  125660/1750000: episode: 502, duration: 4.300s, episode steps: 235, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000790, mae: 0.071325, mean_q: 0.095453, mean_eps: 0.887012\n",
            "  125932/1750000: episode: 503, duration: 5.106s, episode steps: 272, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000831, mae: 0.074263, mean_q: 0.099129, mean_eps: 0.886785\n",
            "  126202/1750000: episode: 504, duration: 4.985s, episode steps: 270, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.000575, mae: 0.066766, mean_q: 0.090388, mean_eps: 0.886541\n",
            "  126380/1750000: episode: 505, duration: 3.302s, episode steps: 178, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000545, mae: 0.074711, mean_q: 0.100577, mean_eps: 0.886339\n",
            "  126562/1750000: episode: 506, duration: 3.370s, episode steps: 182, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000653, mae: 0.065679, mean_q: 0.088274, mean_eps: 0.886177\n",
            "  126799/1750000: episode: 507, duration: 4.347s, episode steps: 237, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000814, mae: 0.068047, mean_q: 0.090265, mean_eps: 0.885988\n",
            "  126965/1750000: episode: 508, duration: 3.087s, episode steps: 166, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001127, mae: 0.074418, mean_q: 0.098534, mean_eps: 0.885806\n",
            "  127234/1750000: episode: 509, duration: 4.945s, episode steps: 269, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000964, mae: 0.067317, mean_q: 0.089564, mean_eps: 0.885610\n",
            "  127673/1750000: episode: 510, duration: 8.058s, episode steps: 439, steps per second:  54, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000689, mae: 0.067245, mean_q: 0.090611, mean_eps: 0.885291\n",
            "  127856/1750000: episode: 511, duration: 3.354s, episode steps: 183, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000464, mae: 0.073457, mean_q: 0.098496, mean_eps: 0.885012\n",
            "  128111/1750000: episode: 512, duration: 4.693s, episode steps: 255, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.000553, mae: 0.071103, mean_q: 0.094049, mean_eps: 0.884816\n",
            "  128295/1750000: episode: 513, duration: 3.362s, episode steps: 184, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000925, mae: 0.074417, mean_q: 0.098796, mean_eps: 0.884618\n",
            "  128489/1750000: episode: 514, duration: 3.579s, episode steps: 194, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000810, mae: 0.076099, mean_q: 0.100419, mean_eps: 0.884447\n",
            "  128661/1750000: episode: 515, duration: 3.153s, episode steps: 172, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.000574, mae: 0.068167, mean_q: 0.090384, mean_eps: 0.884282\n",
            "  128879/1750000: episode: 516, duration: 3.964s, episode steps: 218, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000555, mae: 0.072590, mean_q: 0.096503, mean_eps: 0.884107\n",
            "  129049/1750000: episode: 517, duration: 3.151s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000474, mae: 0.075495, mean_q: 0.101707, mean_eps: 0.883932\n",
            "  129291/1750000: episode: 518, duration: 4.461s, episode steps: 242, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000576, mae: 0.068321, mean_q: 0.091657, mean_eps: 0.883747\n",
            "  129493/1750000: episode: 519, duration: 3.842s, episode steps: 202, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000775, mae: 0.076721, mean_q: 0.103725, mean_eps: 0.883547\n",
            "  129714/1750000: episode: 520, duration: 4.145s, episode steps: 221, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000784, mae: 0.064812, mean_q: 0.087148, mean_eps: 0.883356\n",
            "  129949/1750000: episode: 521, duration: 4.427s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000493, mae: 0.074617, mean_q: 0.100398, mean_eps: 0.883151\n",
            "  130122/1750000: episode: 522, duration: 3.163s, episode steps: 173, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.002399, mae: 0.075685, mean_q: 0.100132, mean_eps: 0.882968\n",
            "  130288/1750000: episode: 523, duration: 3.079s, episode steps: 166, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001035, mae: 0.078513, mean_q: 0.104305, mean_eps: 0.882816\n",
            "  130558/1750000: episode: 524, duration: 4.975s, episode steps: 270, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001136, mae: 0.074754, mean_q: 0.098720, mean_eps: 0.882620\n",
            "  130851/1750000: episode: 525, duration: 5.393s, episode steps: 293, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001285, mae: 0.083277, mean_q: 0.110368, mean_eps: 0.882366\n",
            "  131183/1750000: episode: 526, duration: 6.281s, episode steps: 332, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.001213, mae: 0.082364, mean_q: 0.110478, mean_eps: 0.882086\n",
            "  131470/1750000: episode: 527, duration: 5.442s, episode steps: 287, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.000838, mae: 0.077023, mean_q: 0.103036, mean_eps: 0.881807\n",
            "  131797/1750000: episode: 528, duration: 6.190s, episode steps: 327, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000924, mae: 0.076470, mean_q: 0.100729, mean_eps: 0.881529\n",
            "  132019/1750000: episode: 529, duration: 4.172s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.001009, mae: 0.076956, mean_q: 0.101740, mean_eps: 0.881283\n",
            "  132198/1750000: episode: 530, duration: 3.376s, episode steps: 179, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001055, mae: 0.080227, mean_q: 0.106534, mean_eps: 0.881103\n",
            "  132425/1750000: episode: 531, duration: 4.272s, episode steps: 227, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000636, mae: 0.078873, mean_q: 0.104173, mean_eps: 0.880919\n",
            "  132668/1750000: episode: 532, duration: 4.639s, episode steps: 243, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.001052, mae: 0.081405, mean_q: 0.107900, mean_eps: 0.880709\n",
            "  133031/1750000: episode: 533, duration: 6.886s, episode steps: 363, steps per second:  53, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.000853, mae: 0.075984, mean_q: 0.100507, mean_eps: 0.880437\n",
            "  133317/1750000: episode: 534, duration: 5.395s, episode steps: 286, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.000596, mae: 0.080854, mean_q: 0.108533, mean_eps: 0.880143\n",
            "  133608/1750000: episode: 535, duration: 5.499s, episode steps: 291, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.000801, mae: 0.082343, mean_q: 0.109654, mean_eps: 0.879884\n",
            "  133779/1750000: episode: 536, duration: 3.208s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 0.000450, mae: 0.072372, mean_q: 0.095672, mean_eps: 0.879677\n",
            "  133941/1750000: episode: 537, duration: 3.059s, episode steps: 162, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001051, mae: 0.079782, mean_q: 0.105439, mean_eps: 0.879526\n",
            "  134155/1750000: episode: 538, duration: 4.065s, episode steps: 214, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000425, mae: 0.069595, mean_q: 0.092590, mean_eps: 0.879357\n",
            "  134517/1750000: episode: 539, duration: 6.841s, episode steps: 362, steps per second:  53, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000640, mae: 0.076105, mean_q: 0.101263, mean_eps: 0.879098\n",
            "  134953/1750000: episode: 540, duration: 8.119s, episode steps: 436, steps per second:  54, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000826, mae: 0.080541, mean_q: 0.106864, mean_eps: 0.878738\n",
            "  135129/1750000: episode: 541, duration: 3.302s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.466 [0.000, 3.000],  loss: 0.000871, mae: 0.085655, mean_q: 0.114680, mean_eps: 0.878462\n",
            "  135472/1750000: episode: 542, duration: 6.475s, episode steps: 343, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000418, mae: 0.074549, mean_q: 0.099279, mean_eps: 0.878230\n",
            "  135648/1750000: episode: 543, duration: 3.373s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000422, mae: 0.080017, mean_q: 0.106774, mean_eps: 0.877998\n",
            "  135816/1750000: episode: 544, duration: 3.238s, episode steps: 168, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000631, mae: 0.078905, mean_q: 0.104880, mean_eps: 0.877843\n",
            "  136112/1750000: episode: 545, duration: 5.613s, episode steps: 296, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000632, mae: 0.079936, mean_q: 0.106965, mean_eps: 0.877634\n",
            "  136394/1750000: episode: 546, duration: 5.314s, episode steps: 282, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.000876, mae: 0.081974, mean_q: 0.111396, mean_eps: 0.877373\n",
            "  136648/1750000: episode: 547, duration: 4.777s, episode steps: 254, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.000596, mae: 0.072519, mean_q: 0.096662, mean_eps: 0.877132\n",
            "  136899/1750000: episode: 548, duration: 4.712s, episode steps: 251, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000456, mae: 0.079268, mean_q: 0.105773, mean_eps: 0.876905\n",
            "  137181/1750000: episode: 549, duration: 5.288s, episode steps: 282, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000968, mae: 0.083585, mean_q: 0.112239, mean_eps: 0.876664\n",
            "  137364/1750000: episode: 550, duration: 3.451s, episode steps: 183, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.000560, mae: 0.074688, mean_q: 0.098308, mean_eps: 0.876455\n",
            "  137553/1750000: episode: 551, duration: 3.570s, episode steps: 189, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000400, mae: 0.075353, mean_q: 0.100215, mean_eps: 0.876288\n",
            "  137729/1750000: episode: 552, duration: 3.276s, episode steps: 176, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000588, mae: 0.084355, mean_q: 0.112301, mean_eps: 0.876122\n",
            "  137893/1750000: episode: 553, duration: 3.090s, episode steps: 164, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000579, mae: 0.073269, mean_q: 0.098904, mean_eps: 0.875969\n",
            "  138183/1750000: episode: 554, duration: 5.413s, episode steps: 290, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000645, mae: 0.075146, mean_q: 0.100971, mean_eps: 0.875766\n",
            "  138399/1750000: episode: 555, duration: 4.015s, episode steps: 216, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.000410, mae: 0.075534, mean_q: 0.100657, mean_eps: 0.875539\n",
            "  138748/1750000: episode: 556, duration: 6.525s, episode steps: 349, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000603, mae: 0.073646, mean_q: 0.098498, mean_eps: 0.875285\n",
            "  139157/1750000: episode: 557, duration: 7.633s, episode steps: 409, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000438, mae: 0.079725, mean_q: 0.106800, mean_eps: 0.874943\n",
            "  139324/1750000: episode: 558, duration: 3.106s, episode steps: 167, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000756, mae: 0.082230, mean_q: 0.109287, mean_eps: 0.874684\n",
            "  139685/1750000: episode: 559, duration: 6.693s, episode steps: 361, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000401, mae: 0.080800, mean_q: 0.107747, mean_eps: 0.874446\n",
            "  140075/1750000: episode: 560, duration: 7.182s, episode steps: 390, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.001092, mae: 0.080081, mean_q: 0.106708, mean_eps: 0.874108\n",
            "  140412/1750000: episode: 561, duration: 6.309s, episode steps: 337, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001526, mae: 0.090604, mean_q: 0.121821, mean_eps: 0.873782\n",
            "  140904/1750000: episode: 562, duration: 9.264s, episode steps: 492, steps per second:  53, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000907, mae: 0.088640, mean_q: 0.117070, mean_eps: 0.873410\n",
            "  141195/1750000: episode: 563, duration: 5.452s, episode steps: 291, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000829, mae: 0.089096, mean_q: 0.118932, mean_eps: 0.873057\n",
            "  141472/1750000: episode: 564, duration: 5.217s, episode steps: 277, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001230, mae: 0.091391, mean_q: 0.121160, mean_eps: 0.872801\n",
            "  141802/1750000: episode: 565, duration: 6.196s, episode steps: 330, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001134, mae: 0.092792, mean_q: 0.124341, mean_eps: 0.872528\n",
            "  141988/1750000: episode: 566, duration: 3.526s, episode steps: 186, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000713, mae: 0.100252, mean_q: 0.133638, mean_eps: 0.872295\n",
            "  142155/1750000: episode: 567, duration: 3.149s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000974, mae: 0.088428, mean_q: 0.118055, mean_eps: 0.872137\n",
            "  142422/1750000: episode: 568, duration: 5.074s, episode steps: 267, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000632, mae: 0.083977, mean_q: 0.112730, mean_eps: 0.871941\n",
            "  142699/1750000: episode: 569, duration: 5.147s, episode steps: 277, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000758, mae: 0.089996, mean_q: 0.119803, mean_eps: 0.871696\n",
            "  143183/1750000: episode: 570, duration: 9.057s, episode steps: 484, steps per second:  53, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000601, mae: 0.085221, mean_q: 0.113480, mean_eps: 0.871354\n",
            "  143403/1750000: episode: 571, duration: 4.100s, episode steps: 220, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000571, mae: 0.086769, mean_q: 0.115758, mean_eps: 0.871037\n",
            "  143748/1750000: episode: 572, duration: 6.541s, episode steps: 345, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000522, mae: 0.086715, mean_q: 0.115364, mean_eps: 0.870783\n",
            "  143912/1750000: episode: 573, duration: 3.110s, episode steps: 164, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001821, mae: 0.093626, mean_q: 0.125148, mean_eps: 0.870555\n",
            "  144088/1750000: episode: 574, duration: 3.396s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001415, mae: 0.086083, mean_q: 0.115799, mean_eps: 0.870402\n",
            "  144383/1750000: episode: 575, duration: 5.563s, episode steps: 295, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000840, mae: 0.093489, mean_q: 0.124658, mean_eps: 0.870189\n",
            "  144554/1750000: episode: 576, duration: 3.223s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000522, mae: 0.089249, mean_q: 0.119563, mean_eps: 0.869979\n",
            "  144874/1750000: episode: 577, duration: 5.978s, episode steps: 320, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000746, mae: 0.089435, mean_q: 0.119124, mean_eps: 0.869757\n",
            "  145088/1750000: episode: 578, duration: 4.058s, episode steps: 214, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000418, mae: 0.086827, mean_q: 0.117265, mean_eps: 0.869518\n",
            "  145459/1750000: episode: 579, duration: 6.952s, episode steps: 371, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000758, mae: 0.089337, mean_q: 0.118798, mean_eps: 0.869255\n",
            "  145809/1750000: episode: 580, duration: 6.629s, episode steps: 350, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.000860, mae: 0.089284, mean_q: 0.120308, mean_eps: 0.868929\n",
            "  146025/1750000: episode: 581, duration: 4.060s, episode steps: 216, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.000583, mae: 0.088875, mean_q: 0.117889, mean_eps: 0.868674\n",
            "  146270/1750000: episode: 582, duration: 4.614s, episode steps: 245, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000742, mae: 0.093448, mean_q: 0.125618, mean_eps: 0.868467\n",
            "  146455/1750000: episode: 583, duration: 3.482s, episode steps: 185, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000663, mae: 0.087888, mean_q: 0.118256, mean_eps: 0.868274\n",
            "  146658/1750000: episode: 584, duration: 3.813s, episode steps: 203, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000665, mae: 0.090713, mean_q: 0.121386, mean_eps: 0.868100\n",
            "  147119/1750000: episode: 585, duration: 8.638s, episode steps: 461, steps per second:  53, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.000467, mae: 0.088565, mean_q: 0.117941, mean_eps: 0.867801\n",
            "  147301/1750000: episode: 586, duration: 3.442s, episode steps: 182, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000598, mae: 0.091817, mean_q: 0.122582, mean_eps: 0.867511\n",
            "  147538/1750000: episode: 587, duration: 4.439s, episode steps: 237, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000656, mae: 0.079176, mean_q: 0.104872, mean_eps: 0.867322\n",
            "  147718/1750000: episode: 588, duration: 3.366s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000535, mae: 0.084261, mean_q: 0.112479, mean_eps: 0.867135\n",
            "  148001/1750000: episode: 589, duration: 5.366s, episode steps: 283, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.000398, mae: 0.089626, mean_q: 0.119253, mean_eps: 0.866926\n",
            "  148168/1750000: episode: 590, duration: 3.149s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.575 [0.000, 3.000],  loss: 0.000642, mae: 0.094962, mean_q: 0.128703, mean_eps: 0.866724\n",
            "  148417/1750000: episode: 591, duration: 4.739s, episode steps: 249, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000473, mae: 0.088270, mean_q: 0.117526, mean_eps: 0.866537\n",
            "  148698/1750000: episode: 592, duration: 5.283s, episode steps: 281, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000695, mae: 0.089995, mean_q: 0.119274, mean_eps: 0.866298\n",
            "  148947/1750000: episode: 593, duration: 4.641s, episode steps: 249, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: 0.000919, mae: 0.096678, mean_q: 0.129042, mean_eps: 0.866060\n",
            "  149251/1750000: episode: 594, duration: 5.623s, episode steps: 304, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000686, mae: 0.091319, mean_q: 0.122586, mean_eps: 0.865812\n",
            "  149419/1750000: episode: 595, duration: 3.110s, episode steps: 168, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000333, mae: 0.085129, mean_q: 0.114993, mean_eps: 0.865599\n",
            "  149606/1750000: episode: 596, duration: 3.463s, episode steps: 187, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000545, mae: 0.097673, mean_q: 0.129967, mean_eps: 0.865439\n",
            "  149835/1750000: episode: 597, duration: 4.197s, episode steps: 229, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000271, mae: 0.084512, mean_q: 0.113050, mean_eps: 0.865252\n",
            "  150015/1750000: episode: 598, duration: 3.305s, episode steps: 180, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000607, mae: 0.093703, mean_q: 0.125225, mean_eps: 0.865068\n",
            "  150258/1750000: episode: 599, duration: 4.520s, episode steps: 243, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001822, mae: 0.103138, mean_q: 0.137640, mean_eps: 0.864878\n",
            "  150654/1750000: episode: 600, duration: 7.303s, episode steps: 396, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000825, mae: 0.096760, mean_q: 0.128675, mean_eps: 0.864590\n",
            "  150967/1750000: episode: 601, duration: 5.799s, episode steps: 313, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.000793, mae: 0.090067, mean_q: 0.119482, mean_eps: 0.864271\n",
            "  151240/1750000: episode: 602, duration: 5.208s, episode steps: 273, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000691, mae: 0.092702, mean_q: 0.123775, mean_eps: 0.864008\n",
            "  151419/1750000: episode: 603, duration: 3.381s, episode steps: 179, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001153, mae: 0.095973, mean_q: 0.127193, mean_eps: 0.863805\n",
            "  151599/1750000: episode: 604, duration: 3.366s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000544, mae: 0.091142, mean_q: 0.121768, mean_eps: 0.863643\n",
            "  151798/1750000: episode: 605, duration: 3.754s, episode steps: 199, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.000661, mae: 0.100851, mean_q: 0.134337, mean_eps: 0.863472\n",
            "  152157/1750000: episode: 606, duration: 6.738s, episode steps: 359, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000590, mae: 0.100998, mean_q: 0.134210, mean_eps: 0.863220\n",
            "  152403/1750000: episode: 607, duration: 4.603s, episode steps: 246, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000562, mae: 0.101393, mean_q: 0.134496, mean_eps: 0.862948\n",
            "  152814/1750000: episode: 608, duration: 7.738s, episode steps: 411, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.000626, mae: 0.093255, mean_q: 0.124993, mean_eps: 0.862653\n",
            "  153117/1750000: episode: 609, duration: 5.769s, episode steps: 303, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000553, mae: 0.094332, mean_q: 0.126345, mean_eps: 0.862331\n",
            "  153292/1750000: episode: 610, duration: 3.348s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000503, mae: 0.089549, mean_q: 0.119217, mean_eps: 0.862116\n",
            "  153466/1750000: episode: 611, duration: 3.242s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000581, mae: 0.101094, mean_q: 0.134409, mean_eps: 0.861960\n",
            "  153685/1750000: episode: 612, duration: 4.032s, episode steps: 219, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.000659, mae: 0.099447, mean_q: 0.131461, mean_eps: 0.861782\n",
            "  153873/1750000: episode: 613, duration: 3.456s, episode steps: 188, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000515, mae: 0.098170, mean_q: 0.130834, mean_eps: 0.861598\n",
            "  154052/1750000: episode: 614, duration: 3.306s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000767, mae: 0.096679, mean_q: 0.127529, mean_eps: 0.861434\n",
            "  154222/1750000: episode: 615, duration: 3.155s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.001516, mae: 0.101776, mean_q: 0.138277, mean_eps: 0.861278\n",
            "  154454/1750000: episode: 616, duration: 4.263s, episode steps: 232, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000690, mae: 0.095869, mean_q: 0.127817, mean_eps: 0.861096\n",
            "  154623/1750000: episode: 617, duration: 3.111s, episode steps: 169, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000833, mae: 0.092654, mean_q: 0.124658, mean_eps: 0.860916\n",
            "  154966/1750000: episode: 618, duration: 6.305s, episode steps: 343, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.000570, mae: 0.093096, mean_q: 0.124643, mean_eps: 0.860685\n",
            "  155185/1750000: episode: 619, duration: 4.053s, episode steps: 219, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.000369, mae: 0.086409, mean_q: 0.115744, mean_eps: 0.860432\n",
            "  155459/1750000: episode: 620, duration: 5.003s, episode steps: 274, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000588, mae: 0.103004, mean_q: 0.137433, mean_eps: 0.860210\n",
            "  155683/1750000: episode: 621, duration: 4.121s, episode steps: 224, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000470, mae: 0.092402, mean_q: 0.124037, mean_eps: 0.859987\n",
            "  155902/1750000: episode: 622, duration: 4.054s, episode steps: 219, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000631, mae: 0.092629, mean_q: 0.123027, mean_eps: 0.859787\n",
            "  156145/1750000: episode: 623, duration: 4.501s, episode steps: 243, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000754, mae: 0.100960, mean_q: 0.135127, mean_eps: 0.859578\n",
            "  156464/1750000: episode: 624, duration: 5.921s, episode steps: 319, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000847, mae: 0.103862, mean_q: 0.138217, mean_eps: 0.859326\n",
            "  156842/1750000: episode: 625, duration: 7.005s, episode steps: 378, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000356, mae: 0.099126, mean_q: 0.133598, mean_eps: 0.859013\n",
            "  157017/1750000: episode: 626, duration: 3.257s, episode steps: 175, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000543, mae: 0.097701, mean_q: 0.130812, mean_eps: 0.858763\n",
            "  157317/1750000: episode: 627, duration: 5.568s, episode steps: 300, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.000398, mae: 0.095470, mean_q: 0.127317, mean_eps: 0.858549\n",
            "  157576/1750000: episode: 628, duration: 4.811s, episode steps: 259, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000476, mae: 0.089207, mean_q: 0.118170, mean_eps: 0.858299\n",
            "  158082/1750000: episode: 629, duration: 9.338s, episode steps: 506, steps per second:  54, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.000612, mae: 0.098272, mean_q: 0.131468, mean_eps: 0.857955\n",
            "  158356/1750000: episode: 630, duration: 5.114s, episode steps: 274, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000954, mae: 0.096841, mean_q: 0.129307, mean_eps: 0.857604\n",
            "  158530/1750000: episode: 631, duration: 3.234s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000608, mae: 0.097720, mean_q: 0.131259, mean_eps: 0.857402\n",
            "  158749/1750000: episode: 632, duration: 4.050s, episode steps: 219, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000566, mae: 0.093095, mean_q: 0.124203, mean_eps: 0.857224\n",
            "  159175/1750000: episode: 633, duration: 7.785s, episode steps: 426, steps per second:  55, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000638, mae: 0.097727, mean_q: 0.130476, mean_eps: 0.856934\n",
            "  159410/1750000: episode: 634, duration: 4.335s, episode steps: 235, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000485, mae: 0.102763, mean_q: 0.136231, mean_eps: 0.856637\n",
            "  159580/1750000: episode: 635, duration: 3.159s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.841 [0.000, 3.000],  loss: 0.000869, mae: 0.099363, mean_q: 0.132467, mean_eps: 0.856455\n",
            "  159761/1750000: episode: 636, duration: 3.413s, episode steps: 181, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.796 [0.000, 3.000],  loss: 0.000551, mae: 0.107303, mean_q: 0.143741, mean_eps: 0.856297\n",
            "  160055/1750000: episode: 637, duration: 5.419s, episode steps: 294, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000789, mae: 0.097666, mean_q: 0.129694, mean_eps: 0.856083\n",
            "  160275/1750000: episode: 638, duration: 4.040s, episode steps: 220, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.001581, mae: 0.110197, mean_q: 0.146408, mean_eps: 0.855852\n",
            "  160540/1750000: episode: 639, duration: 4.892s, episode steps: 265, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001098, mae: 0.105209, mean_q: 0.142190, mean_eps: 0.855635\n",
            "  160712/1750000: episode: 640, duration: 3.239s, episode steps: 172, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.721 [0.000, 3.000],  loss: 0.000962, mae: 0.099807, mean_q: 0.134033, mean_eps: 0.855438\n",
            "  160949/1750000: episode: 641, duration: 4.401s, episode steps: 237, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000964, mae: 0.105072, mean_q: 0.141112, mean_eps: 0.855253\n",
            "  161227/1750000: episode: 642, duration: 5.124s, episode steps: 278, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000804, mae: 0.110131, mean_q: 0.146549, mean_eps: 0.855021\n",
            "  161423/1750000: episode: 643, duration: 3.640s, episode steps: 196, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.357 [0.000, 3.000],  loss: 0.000714, mae: 0.114014, mean_q: 0.154273, mean_eps: 0.854808\n",
            "  161664/1750000: episode: 644, duration: 4.520s, episode steps: 241, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000702, mae: 0.102873, mean_q: 0.136364, mean_eps: 0.854612\n",
            "  162003/1750000: episode: 645, duration: 6.280s, episode steps: 339, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000655, mae: 0.101526, mean_q: 0.136852, mean_eps: 0.854351\n",
            "  162220/1750000: episode: 646, duration: 4.023s, episode steps: 217, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000744, mae: 0.104190, mean_q: 0.138580, mean_eps: 0.854101\n",
            "  162406/1750000: episode: 647, duration: 3.448s, episode steps: 186, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.000740, mae: 0.095857, mean_q: 0.129580, mean_eps: 0.853919\n",
            "  162581/1750000: episode: 648, duration: 3.260s, episode steps: 175, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000617, mae: 0.112900, mean_q: 0.150787, mean_eps: 0.853755\n",
            "  162794/1750000: episode: 649, duration: 3.932s, episode steps: 213, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.001121, mae: 0.097070, mean_q: 0.130057, mean_eps: 0.853581\n",
            "  163071/1750000: episode: 650, duration: 5.172s, episode steps: 277, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000706, mae: 0.110043, mean_q: 0.146324, mean_eps: 0.853361\n",
            "  163244/1750000: episode: 651, duration: 3.237s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000632, mae: 0.107841, mean_q: 0.144233, mean_eps: 0.853160\n",
            "  163487/1750000: episode: 652, duration: 4.485s, episode steps: 243, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000573, mae: 0.102415, mean_q: 0.136992, mean_eps: 0.852972\n",
            "  163659/1750000: episode: 653, duration: 3.188s, episode steps: 172, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000534, mae: 0.107764, mean_q: 0.144105, mean_eps: 0.852785\n",
            "  163854/1750000: episode: 654, duration: 3.601s, episode steps: 195, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.800 [0.000, 3.000],  loss: 0.000444, mae: 0.100556, mean_q: 0.133989, mean_eps: 0.852620\n",
            "  164115/1750000: episode: 655, duration: 4.779s, episode steps: 261, steps per second:  55, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.000481, mae: 0.098139, mean_q: 0.131717, mean_eps: 0.852414\n",
            "  164384/1750000: episode: 656, duration: 4.961s, episode steps: 269, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000468, mae: 0.097711, mean_q: 0.131172, mean_eps: 0.852177\n",
            "  164602/1750000: episode: 657, duration: 4.038s, episode steps: 218, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.000405, mae: 0.101843, mean_q: 0.136392, mean_eps: 0.851957\n",
            "  164844/1750000: episode: 658, duration: 4.488s, episode steps: 242, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.000467, mae: 0.103207, mean_q: 0.137599, mean_eps: 0.851750\n",
            "  165044/1750000: episode: 659, duration: 3.764s, episode steps: 200, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.000281, mae: 0.097316, mean_q: 0.129910, mean_eps: 0.851552\n",
            "  165316/1750000: episode: 660, duration: 5.134s, episode steps: 272, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000404, mae: 0.102423, mean_q: 0.136444, mean_eps: 0.851340\n",
            "  165483/1750000: episode: 661, duration: 3.110s, episode steps: 167, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.413 [0.000, 3.000],  loss: 0.000622, mae: 0.107523, mean_q: 0.143534, mean_eps: 0.851142\n",
            "  165754/1750000: episode: 662, duration: 5.030s, episode steps: 271, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000434, mae: 0.094736, mean_q: 0.125535, mean_eps: 0.850944\n",
            "  165919/1750000: episode: 663, duration: 3.059s, episode steps: 165, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.685 [0.000, 3.000],  loss: 0.000257, mae: 0.098108, mean_q: 0.131030, mean_eps: 0.850748\n",
            "  166093/1750000: episode: 664, duration: 3.272s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000368, mae: 0.108613, mean_q: 0.145440, mean_eps: 0.850595\n",
            "  166274/1750000: episode: 665, duration: 3.360s, episode steps: 181, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.365 [0.000, 3.000],  loss: 0.000673, mae: 0.097474, mean_q: 0.132232, mean_eps: 0.850434\n",
            "  166487/1750000: episode: 666, duration: 3.916s, episode steps: 213, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000413, mae: 0.100007, mean_q: 0.133341, mean_eps: 0.850258\n",
            "  166735/1750000: episode: 667, duration: 4.554s, episode steps: 248, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000362, mae: 0.099509, mean_q: 0.133158, mean_eps: 0.850051\n",
            "  167051/1750000: episode: 668, duration: 5.836s, episode steps: 316, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000440, mae: 0.101353, mean_q: 0.135879, mean_eps: 0.849797\n",
            "  167389/1750000: episode: 669, duration: 6.239s, episode steps: 338, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000424, mae: 0.098611, mean_q: 0.131565, mean_eps: 0.849502\n",
            "  167597/1750000: episode: 670, duration: 3.819s, episode steps: 208, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000730, mae: 0.100389, mean_q: 0.133503, mean_eps: 0.849255\n",
            "  167846/1750000: episode: 671, duration: 4.564s, episode steps: 249, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.000483, mae: 0.098270, mean_q: 0.131723, mean_eps: 0.849050\n",
            "  168067/1750000: episode: 672, duration: 4.060s, episode steps: 221, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000610, mae: 0.100972, mean_q: 0.135284, mean_eps: 0.848840\n",
            "  168239/1750000: episode: 673, duration: 3.174s, episode steps: 172, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.570 [0.000, 3.000],  loss: 0.000293, mae: 0.098944, mean_q: 0.132149, mean_eps: 0.848663\n",
            "  168610/1750000: episode: 674, duration: 6.874s, episode steps: 371, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000350, mae: 0.103214, mean_q: 0.137066, mean_eps: 0.848418\n",
            "  168950/1750000: episode: 675, duration: 6.264s, episode steps: 340, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.000657, mae: 0.095520, mean_q: 0.127753, mean_eps: 0.848098\n",
            "  169366/1750000: episode: 676, duration: 7.633s, episode steps: 416, steps per second:  55, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000426, mae: 0.101877, mean_q: 0.136187, mean_eps: 0.847758\n",
            "  169557/1750000: episode: 677, duration: 3.579s, episode steps: 191, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.000386, mae: 0.103532, mean_q: 0.138100, mean_eps: 0.847484\n",
            "  169858/1750000: episode: 678, duration: 5.517s, episode steps: 301, steps per second:  55, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000732, mae: 0.103043, mean_q: 0.138401, mean_eps: 0.847263\n",
            "  170040/1750000: episode: 679, duration: 3.391s, episode steps: 182, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000886, mae: 0.106760, mean_q: 0.143990, mean_eps: 0.847047\n",
            "  170210/1750000: episode: 680, duration: 3.198s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001521, mae: 0.123223, mean_q: 0.164219, mean_eps: 0.846888\n",
            "  170550/1750000: episode: 681, duration: 6.323s, episode steps: 340, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001021, mae: 0.126200, mean_q: 0.169358, mean_eps: 0.846658\n",
            "  170760/1750000: episode: 682, duration: 3.950s, episode steps: 210, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.886 [0.000, 3.000],  loss: 0.000920, mae: 0.121732, mean_q: 0.162671, mean_eps: 0.846411\n",
            "  170974/1750000: episode: 683, duration: 3.965s, episode steps: 214, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.360 [0.000, 3.000],  loss: 0.000625, mae: 0.119166, mean_q: 0.159341, mean_eps: 0.846221\n",
            "  171215/1750000: episode: 684, duration: 4.410s, episode steps: 241, steps per second:  55, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: 0.000792, mae: 0.122717, mean_q: 0.163235, mean_eps: 0.846015\n",
            "  171386/1750000: episode: 685, duration: 3.174s, episode steps: 171, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000594, mae: 0.115827, mean_q: 0.155737, mean_eps: 0.845830\n",
            "  171594/1750000: episode: 686, duration: 3.838s, episode steps: 208, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000532, mae: 0.124754, mean_q: 0.166977, mean_eps: 0.845659\n",
            "  171834/1750000: episode: 687, duration: 4.420s, episode steps: 240, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000636, mae: 0.121379, mean_q: 0.161980, mean_eps: 0.845457\n",
            "  172253/1750000: episode: 688, duration: 7.740s, episode steps: 419, steps per second:  54, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.000873, mae: 0.117069, mean_q: 0.155316, mean_eps: 0.845160\n",
            "  172656/1750000: episode: 689, duration: 7.483s, episode steps: 403, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000671, mae: 0.117803, mean_q: 0.156695, mean_eps: 0.844791\n",
            "  172866/1750000: episode: 690, duration: 3.925s, episode steps: 210, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000500, mae: 0.115449, mean_q: 0.154814, mean_eps: 0.844516\n",
            "  173080/1750000: episode: 691, duration: 3.963s, episode steps: 214, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000468, mae: 0.118229, mean_q: 0.158197, mean_eps: 0.844325\n",
            "  173257/1750000: episode: 692, duration: 3.290s, episode steps: 177, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.475 [0.000, 3.000],  loss: 0.000784, mae: 0.121519, mean_q: 0.162601, mean_eps: 0.844149\n",
            "  173563/1750000: episode: 693, duration: 5.585s, episode steps: 306, steps per second:  55, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000475, mae: 0.122633, mean_q: 0.164238, mean_eps: 0.843931\n",
            "  173870/1750000: episode: 694, duration: 5.646s, episode steps: 307, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000556, mae: 0.117783, mean_q: 0.156683, mean_eps: 0.843656\n",
            "  174109/1750000: episode: 695, duration: 4.425s, episode steps: 239, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000418, mae: 0.122777, mean_q: 0.164176, mean_eps: 0.843409\n",
            "  174285/1750000: episode: 696, duration: 3.274s, episode steps: 176, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000788, mae: 0.123642, mean_q: 0.165148, mean_eps: 0.843222\n",
            "  174602/1750000: episode: 697, duration: 5.864s, episode steps: 317, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.000635, mae: 0.119087, mean_q: 0.158644, mean_eps: 0.843000\n",
            "  174895/1750000: episode: 698, duration: 5.447s, episode steps: 293, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000686, mae: 0.125970, mean_q: 0.168146, mean_eps: 0.842727\n",
            "  175110/1750000: episode: 699, duration: 4.003s, episode steps: 215, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.000577, mae: 0.122836, mean_q: 0.163605, mean_eps: 0.842498\n",
            "  175546/1750000: episode: 700, duration: 8.021s, episode steps: 436, steps per second:  54, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000697, mae: 0.120051, mean_q: 0.159607, mean_eps: 0.842205\n",
            "  175805/1750000: episode: 701, duration: 4.816s, episode steps: 259, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000492, mae: 0.119302, mean_q: 0.158602, mean_eps: 0.841892\n",
            "  176061/1750000: episode: 702, duration: 4.773s, episode steps: 256, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000507, mae: 0.123550, mean_q: 0.167191, mean_eps: 0.841659\n",
            "  176457/1750000: episode: 703, duration: 7.300s, episode steps: 396, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000439, mae: 0.120447, mean_q: 0.160486, mean_eps: 0.841366\n",
            "  176662/1750000: episode: 704, duration: 3.797s, episode steps: 205, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000492, mae: 0.126103, mean_q: 0.167527, mean_eps: 0.841096\n",
            "  176936/1750000: episode: 705, duration: 5.063s, episode steps: 274, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000324, mae: 0.125781, mean_q: 0.167980, mean_eps: 0.840882\n",
            "  177244/1750000: episode: 706, duration: 5.758s, episode steps: 308, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000316, mae: 0.114362, mean_q: 0.152933, mean_eps: 0.840621\n",
            "  177424/1750000: episode: 707, duration: 3.400s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000590, mae: 0.113195, mean_q: 0.150173, mean_eps: 0.840401\n",
            "  177792/1750000: episode: 708, duration: 6.898s, episode steps: 368, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000525, mae: 0.113855, mean_q: 0.151719, mean_eps: 0.840155\n",
            "  178063/1750000: episode: 709, duration: 5.017s, episode steps: 271, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.000561, mae: 0.123371, mean_q: 0.163901, mean_eps: 0.839867\n",
            "  178236/1750000: episode: 710, duration: 3.253s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.000420, mae: 0.112122, mean_q: 0.150901, mean_eps: 0.839667\n",
            "  178406/1750000: episode: 711, duration: 3.154s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: 0.000356, mae: 0.114860, mean_q: 0.153559, mean_eps: 0.839512\n",
            "  178693/1750000: episode: 712, duration: 5.298s, episode steps: 287, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000446, mae: 0.113869, mean_q: 0.152439, mean_eps: 0.839305\n",
            "  178858/1750000: episode: 713, duration: 3.043s, episode steps: 165, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000731, mae: 0.129667, mean_q: 0.174268, mean_eps: 0.839102\n",
            "  179124/1750000: episode: 714, duration: 4.978s, episode steps: 266, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000621, mae: 0.119208, mean_q: 0.159707, mean_eps: 0.838909\n",
            "  179384/1750000: episode: 715, duration: 4.941s, episode steps: 260, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000602, mae: 0.118789, mean_q: 0.158925, mean_eps: 0.838673\n",
            "  179623/1750000: episode: 716, duration: 4.478s, episode steps: 239, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000392, mae: 0.120084, mean_q: 0.159938, mean_eps: 0.838448\n",
            "  179839/1750000: episode: 717, duration: 4.048s, episode steps: 216, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.000549, mae: 0.121981, mean_q: 0.162916, mean_eps: 0.838243\n",
            "  180259/1750000: episode: 718, duration: 7.748s, episode steps: 420, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001207, mae: 0.128076, mean_q: 0.171948, mean_eps: 0.837957\n",
            "  180567/1750000: episode: 719, duration: 5.682s, episode steps: 308, steps per second:  54, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001286, mae: 0.128537, mean_q: 0.170427, mean_eps: 0.837629\n",
            "  180747/1750000: episode: 720, duration: 3.321s, episode steps: 180, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000993, mae: 0.129228, mean_q: 0.173603, mean_eps: 0.837410\n",
            "  181039/1750000: episode: 721, duration: 5.387s, episode steps: 292, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.000897, mae: 0.117822, mean_q: 0.157996, mean_eps: 0.837197\n",
            "  181477/1750000: episode: 722, duration: 8.138s, episode steps: 438, steps per second:  54, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000720, mae: 0.125714, mean_q: 0.169604, mean_eps: 0.836868\n",
            "  181724/1750000: episode: 723, duration: 4.581s, episode steps: 247, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.000898, mae: 0.123454, mean_q: 0.164811, mean_eps: 0.836560\n",
            "  181893/1750000: episode: 724, duration: 3.165s, episode steps: 169, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.349 [0.000, 3.000],  loss: 0.000832, mae: 0.126258, mean_q: 0.170587, mean_eps: 0.836373\n",
            "  182059/1750000: episode: 725, duration: 3.041s, episode steps: 166, steps per second:  55, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.518 [0.000, 3.000],  loss: 0.000676, mae: 0.118468, mean_q: 0.158598, mean_eps: 0.836222\n",
            "  182226/1750000: episode: 726, duration: 3.110s, episode steps: 167, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.000689, mae: 0.128457, mean_q: 0.171695, mean_eps: 0.836072\n",
            "  182491/1750000: episode: 727, duration: 4.912s, episode steps: 265, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000442, mae: 0.122903, mean_q: 0.163414, mean_eps: 0.835878\n",
            "  182676/1750000: episode: 728, duration: 3.468s, episode steps: 185, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000417, mae: 0.129605, mean_q: 0.172194, mean_eps: 0.835676\n",
            "  182849/1750000: episode: 729, duration: 3.248s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.000474, mae: 0.124436, mean_q: 0.166321, mean_eps: 0.835514\n",
            "  183236/1750000: episode: 730, duration: 7.212s, episode steps: 387, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000422, mae: 0.129780, mean_q: 0.174223, mean_eps: 0.835262\n",
            "  183512/1750000: episode: 731, duration: 5.232s, episode steps: 276, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000664, mae: 0.132624, mean_q: 0.177656, mean_eps: 0.834965\n",
            "  183698/1750000: episode: 732, duration: 3.518s, episode steps: 186, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000584, mae: 0.134554, mean_q: 0.180469, mean_eps: 0.834756\n",
            "  183903/1750000: episode: 733, duration: 3.800s, episode steps: 205, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000270, mae: 0.127743, mean_q: 0.170145, mean_eps: 0.834580\n",
            "  184210/1750000: episode: 734, duration: 5.706s, episode steps: 307, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000316, mae: 0.128275, mean_q: 0.171719, mean_eps: 0.834350\n",
            "  184385/1750000: episode: 735, duration: 3.271s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000622, mae: 0.122821, mean_q: 0.162965, mean_eps: 0.834132\n",
            "  184617/1750000: episode: 736, duration: 4.318s, episode steps: 232, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000575, mae: 0.129115, mean_q: 0.173032, mean_eps: 0.833948\n",
            "  184956/1750000: episode: 737, duration: 6.338s, episode steps: 339, steps per second:  53, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000421, mae: 0.130927, mean_q: 0.174691, mean_eps: 0.833693\n",
            "  185222/1750000: episode: 738, duration: 4.949s, episode steps: 266, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000516, mae: 0.122404, mean_q: 0.164107, mean_eps: 0.833421\n",
            "  185404/1750000: episode: 739, duration: 3.404s, episode steps: 182, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000527, mae: 0.121196, mean_q: 0.162901, mean_eps: 0.833219\n",
            "  185798/1750000: episode: 740, duration: 7.332s, episode steps: 394, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.001070, mae: 0.132521, mean_q: 0.177823, mean_eps: 0.832960\n",
            "  185969/1750000: episode: 741, duration: 3.173s, episode steps: 171, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000457, mae: 0.125529, mean_q: 0.168068, mean_eps: 0.832704\n",
            "  186152/1750000: episode: 742, duration: 3.413s, episode steps: 183, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000765, mae: 0.129001, mean_q: 0.173045, mean_eps: 0.832546\n",
            "  186322/1750000: episode: 743, duration: 3.177s, episode steps: 170, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.712 [0.000, 3.000],  loss: 0.000658, mae: 0.128433, mean_q: 0.172929, mean_eps: 0.832388\n",
            "  186600/1750000: episode: 744, duration: 5.175s, episode steps: 278, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.000839, mae: 0.121319, mean_q: 0.165421, mean_eps: 0.832186\n",
            "  186978/1750000: episode: 745, duration: 7.008s, episode steps: 378, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000683, mae: 0.132959, mean_q: 0.177214, mean_eps: 0.831891\n",
            "  187251/1750000: episode: 746, duration: 5.030s, episode steps: 273, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.000428, mae: 0.134236, mean_q: 0.178672, mean_eps: 0.831597\n",
            "  187483/1750000: episode: 747, duration: 4.286s, episode steps: 232, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.000669, mae: 0.130605, mean_q: 0.174603, mean_eps: 0.831371\n",
            "  187717/1750000: episode: 748, duration: 4.393s, episode steps: 234, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000896, mae: 0.126911, mean_q: 0.170892, mean_eps: 0.831160\n",
            "  187922/1750000: episode: 749, duration: 3.818s, episode steps: 205, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.000305, mae: 0.121672, mean_q: 0.164812, mean_eps: 0.830962\n",
            "  188098/1750000: episode: 750, duration: 3.312s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.330 [0.000, 3.000],  loss: 0.001011, mae: 0.128047, mean_q: 0.170945, mean_eps: 0.830791\n",
            "  188342/1750000: episode: 751, duration: 4.562s, episode steps: 244, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.000870, mae: 0.137884, mean_q: 0.185587, mean_eps: 0.830602\n",
            "  188517/1750000: episode: 752, duration: 3.288s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000670, mae: 0.130960, mean_q: 0.174642, mean_eps: 0.830413\n",
            "  188702/1750000: episode: 753, duration: 3.426s, episode steps: 185, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 0.000605, mae: 0.133966, mean_q: 0.178827, mean_eps: 0.830251\n",
            "  188870/1750000: episode: 754, duration: 3.148s, episode steps: 168, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.756 [0.000, 3.000],  loss: 0.000428, mae: 0.134018, mean_q: 0.178438, mean_eps: 0.830093\n",
            "  189163/1750000: episode: 755, duration: 5.421s, episode steps: 293, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000411, mae: 0.126231, mean_q: 0.168386, mean_eps: 0.829886\n",
            "  189455/1750000: episode: 756, duration: 5.406s, episode steps: 292, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.000313, mae: 0.120629, mean_q: 0.161409, mean_eps: 0.829623\n",
            "  189725/1750000: episode: 757, duration: 5.029s, episode steps: 270, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000675, mae: 0.123068, mean_q: 0.164695, mean_eps: 0.829369\n",
            "  189929/1750000: episode: 758, duration: 3.795s, episode steps: 204, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000427, mae: 0.129857, mean_q: 0.173370, mean_eps: 0.829155\n",
            "  190108/1750000: episode: 759, duration: 3.340s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.000972, mae: 0.135071, mean_q: 0.178602, mean_eps: 0.828984\n",
            "  190351/1750000: episode: 760, duration: 4.547s, episode steps: 243, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.001655, mae: 0.137664, mean_q: 0.183576, mean_eps: 0.828795\n",
            "  190530/1750000: episode: 761, duration: 3.346s, episode steps: 179, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001330, mae: 0.126093, mean_q: 0.167212, mean_eps: 0.828604\n",
            "  190768/1750000: episode: 762, duration: 4.454s, episode steps: 238, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000701, mae: 0.131623, mean_q: 0.175522, mean_eps: 0.828417\n",
            "  191004/1750000: episode: 763, duration: 4.431s, episode steps: 236, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.000651, mae: 0.133167, mean_q: 0.178384, mean_eps: 0.828204\n",
            "  191389/1750000: episode: 764, duration: 7.178s, episode steps: 385, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.000768, mae: 0.129381, mean_q: 0.172136, mean_eps: 0.827924\n",
            "  191664/1750000: episode: 765, duration: 5.135s, episode steps: 275, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001232, mae: 0.126659, mean_q: 0.170910, mean_eps: 0.827627\n",
            "  191901/1750000: episode: 766, duration: 4.501s, episode steps: 237, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.000378, mae: 0.121160, mean_q: 0.162768, mean_eps: 0.827396\n",
            "  192214/1750000: episode: 767, duration: 5.873s, episode steps: 313, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001551, mae: 0.132868, mean_q: 0.176610, mean_eps: 0.827148\n",
            "  192487/1750000: episode: 768, duration: 5.088s, episode steps: 273, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.000992, mae: 0.141302, mean_q: 0.188255, mean_eps: 0.826885\n",
            "  192771/1750000: episode: 769, duration: 5.266s, episode steps: 284, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.000993, mae: 0.135040, mean_q: 0.180285, mean_eps: 0.826635\n",
            "  193154/1750000: episode: 770, duration: 7.104s, episode steps: 383, steps per second:  54, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.000729, mae: 0.132816, mean_q: 0.177419, mean_eps: 0.826334\n",
            "  193331/1750000: episode: 771, duration: 3.309s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.322 [0.000, 3.000],  loss: 0.000525, mae: 0.128609, mean_q: 0.172145, mean_eps: 0.826082\n",
            "  193567/1750000: episode: 772, duration: 4.386s, episode steps: 236, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.000429, mae: 0.138359, mean_q: 0.185752, mean_eps: 0.825897\n",
            "  193736/1750000: episode: 773, duration: 3.221s, episode steps: 169, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.001255, mae: 0.123388, mean_q: 0.164577, mean_eps: 0.825715\n",
            "  194005/1750000: episode: 774, duration: 5.081s, episode steps: 269, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000500, mae: 0.130985, mean_q: 0.174556, mean_eps: 0.825517\n",
            "  194367/1750000: episode: 775, duration: 6.722s, episode steps: 362, steps per second:  54, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000494, mae: 0.129837, mean_q: 0.173645, mean_eps: 0.825233\n",
            "  194633/1750000: episode: 776, duration: 4.969s, episode steps: 266, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000420, mae: 0.128213, mean_q: 0.170700, mean_eps: 0.824950\n",
            "  194880/1750000: episode: 777, duration: 4.615s, episode steps: 247, steps per second:  54, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.344 [0.000, 3.000],  loss: 0.000946, mae: 0.125164, mean_q: 0.167765, mean_eps: 0.824720\n",
            "  195116/1750000: episode: 778, duration: 4.453s, episode steps: 236, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000691, mae: 0.129495, mean_q: 0.172034, mean_eps: 0.824504\n",
            "  195291/1750000: episode: 779, duration: 3.310s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: 0.000574, mae: 0.132444, mean_q: 0.177391, mean_eps: 0.824318\n",
            "  195632/1750000: episode: 780, duration: 6.419s, episode steps: 341, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001174, mae: 0.140632, mean_q: 0.186600, mean_eps: 0.824086\n",
            "  195870/1750000: episode: 781, duration: 4.450s, episode steps: 238, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000633, mae: 0.128089, mean_q: 0.171099, mean_eps: 0.823825\n",
            "  196167/1750000: episode: 782, duration: 5.539s, episode steps: 297, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000734, mae: 0.135832, mean_q: 0.181795, mean_eps: 0.823584\n",
            "  196565/1750000: episode: 783, duration: 7.550s, episode steps: 398, steps per second:  53, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.000586, mae: 0.128750, mean_q: 0.171641, mean_eps: 0.823271\n",
            "  196808/1750000: episode: 784, duration: 4.564s, episode steps: 243, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.000792, mae: 0.135222, mean_q: 0.179787, mean_eps: 0.822983\n",
            "  196985/1750000: episode: 785, duration: 3.340s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.000911, mae: 0.138518, mean_q: 0.183758, mean_eps: 0.822794\n",
            "  197280/1750000: episode: 786, duration: 5.553s, episode steps: 295, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.001023, mae: 0.130519, mean_q: 0.173895, mean_eps: 0.822581\n",
            "  197455/1750000: episode: 787, duration: 3.274s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.349 [0.000, 3.000],  loss: 0.000811, mae: 0.132712, mean_q: 0.177896, mean_eps: 0.822371\n",
            "  197703/1750000: episode: 788, duration: 4.617s, episode steps: 248, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.000795, mae: 0.127237, mean_q: 0.169563, mean_eps: 0.822180\n",
            "  198034/1750000: episode: 789, duration: 6.160s, episode steps: 331, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000971, mae: 0.130899, mean_q: 0.175192, mean_eps: 0.821919\n",
            "  198211/1750000: episode: 790, duration: 3.284s, episode steps: 177, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000652, mae: 0.126540, mean_q: 0.168296, mean_eps: 0.821690\n",
            "  198455/1750000: episode: 791, duration: 4.529s, episode steps: 244, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.001010, mae: 0.133835, mean_q: 0.178387, mean_eps: 0.821501\n",
            "  198822/1750000: episode: 792, duration: 6.884s, episode steps: 367, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000522, mae: 0.135059, mean_q: 0.179727, mean_eps: 0.821226\n",
            "  198998/1750000: episode: 793, duration: 3.302s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.614 [0.000, 3.000],  loss: 0.000328, mae: 0.135147, mean_q: 0.179635, mean_eps: 0.820981\n",
            "  199165/1750000: episode: 794, duration: 3.129s, episode steps: 167, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.509 [0.000, 3.000],  loss: 0.000642, mae: 0.136593, mean_q: 0.182004, mean_eps: 0.820826\n",
            "  199394/1750000: episode: 795, duration: 4.244s, episode steps: 229, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000328, mae: 0.127429, mean_q: 0.170166, mean_eps: 0.820648\n",
            "  199627/1750000: episode: 796, duration: 4.317s, episode steps: 233, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.000663, mae: 0.136201, mean_q: 0.182583, mean_eps: 0.820441\n",
            "  199835/1750000: episode: 797, duration: 3.876s, episode steps: 208, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000380, mae: 0.132724, mean_q: 0.176635, mean_eps: 0.820243\n",
            "  200079/1750000: episode: 798, duration: 4.612s, episode steps: 244, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000704, mae: 0.130842, mean_q: 0.175081, mean_eps: 0.820040\n",
            "  200314/1750000: episode: 799, duration: 4.420s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.000942, mae: 0.135366, mean_q: 0.182230, mean_eps: 0.819824\n",
            "  200512/1750000: episode: 800, duration: 3.804s, episode steps: 198, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001169, mae: 0.144192, mean_q: 0.193969, mean_eps: 0.819629\n",
            "  200753/1750000: episode: 801, duration: 4.619s, episode steps: 241, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: 0.000573, mae: 0.140571, mean_q: 0.187368, mean_eps: 0.819431\n",
            "  201003/1750000: episode: 802, duration: 4.673s, episode steps: 250, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000547, mae: 0.135858, mean_q: 0.182549, mean_eps: 0.819210\n",
            "  201249/1750000: episode: 803, duration: 4.619s, episode steps: 246, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.000712, mae: 0.144874, mean_q: 0.193654, mean_eps: 0.818987\n",
            "  201414/1750000: episode: 804, duration: 3.077s, episode steps: 165, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.309 [0.000, 3.000],  loss: 0.000773, mae: 0.126360, mean_q: 0.169134, mean_eps: 0.818801\n",
            "  201597/1750000: episode: 805, duration: 3.445s, episode steps: 183, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000913, mae: 0.129934, mean_q: 0.174269, mean_eps: 0.818645\n",
            "  201833/1750000: episode: 806, duration: 4.405s, episode steps: 236, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.000847, mae: 0.137896, mean_q: 0.183694, mean_eps: 0.818456\n",
            "  202046/1750000: episode: 807, duration: 3.957s, episode steps: 213, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.000476, mae: 0.133367, mean_q: 0.176770, mean_eps: 0.818254\n",
            "  202240/1750000: episode: 808, duration: 3.648s, episode steps: 194, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.216 [0.000, 3.000],  loss: 0.001144, mae: 0.141746, mean_q: 0.190510, mean_eps: 0.818072\n",
            "  202546/1750000: episode: 809, duration: 5.785s, episode steps: 306, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.000959, mae: 0.139897, mean_q: 0.187144, mean_eps: 0.817847\n",
            "  202720/1750000: episode: 810, duration: 3.274s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.609 [0.000, 3.000],  loss: 0.000923, mae: 0.141883, mean_q: 0.189090, mean_eps: 0.817631\n",
            "  203110/1750000: episode: 811, duration: 7.299s, episode steps: 390, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.318 [0.000, 3.000],  loss: 0.000660, mae: 0.131285, mean_q: 0.175876, mean_eps: 0.817377\n",
            "  203289/1750000: episode: 812, duration: 3.355s, episode steps: 179, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000949, mae: 0.136239, mean_q: 0.181617, mean_eps: 0.817120\n",
            "  203463/1750000: episode: 813, duration: 3.235s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.000844, mae: 0.136954, mean_q: 0.182517, mean_eps: 0.816962\n",
            "  203671/1750000: episode: 814, duration: 3.896s, episode steps: 208, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.000627, mae: 0.129651, mean_q: 0.172245, mean_eps: 0.816791\n",
            "  203850/1750000: episode: 815, duration: 3.349s, episode steps: 179, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000579, mae: 0.138374, mean_q: 0.184879, mean_eps: 0.816616\n",
            "  204021/1750000: episode: 816, duration: 3.275s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: 0.000401, mae: 0.136322, mean_q: 0.181590, mean_eps: 0.816458\n",
            "  204210/1750000: episode: 817, duration: 3.552s, episode steps: 189, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.001177, mae: 0.147368, mean_q: 0.196512, mean_eps: 0.816296\n",
            "  204388/1750000: episode: 818, duration: 3.405s, episode steps: 178, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001029, mae: 0.136625, mean_q: 0.182334, mean_eps: 0.816132\n",
            "  204801/1750000: episode: 819, duration: 7.787s, episode steps: 413, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000816, mae: 0.134438, mean_q: 0.179202, mean_eps: 0.815865\n",
            "  204978/1750000: episode: 820, duration: 3.330s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000631, mae: 0.140209, mean_q: 0.187558, mean_eps: 0.815599\n",
            "  205382/1750000: episode: 821, duration: 7.583s, episode steps: 404, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000646, mae: 0.134687, mean_q: 0.179811, mean_eps: 0.815338\n",
            "  205596/1750000: episode: 822, duration: 4.034s, episode steps: 214, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000309, mae: 0.129799, mean_q: 0.173224, mean_eps: 0.815061\n",
            "  205818/1750000: episode: 823, duration: 4.171s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.000528, mae: 0.134274, mean_q: 0.179213, mean_eps: 0.814865\n",
            "  205989/1750000: episode: 824, duration: 3.204s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000248, mae: 0.134462, mean_q: 0.180075, mean_eps: 0.814686\n",
            "  206228/1750000: episode: 825, duration: 4.479s, episode steps: 239, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.000488, mae: 0.140436, mean_q: 0.187271, mean_eps: 0.814503\n",
            "  206573/1750000: episode: 826, duration: 6.478s, episode steps: 345, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000632, mae: 0.136396, mean_q: 0.181623, mean_eps: 0.814240\n",
            "  206807/1750000: episode: 827, duration: 4.328s, episode steps: 234, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.001036, mae: 0.141502, mean_q: 0.189403, mean_eps: 0.813979\n",
            "  207051/1750000: episode: 828, duration: 4.555s, episode steps: 244, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000665, mae: 0.132562, mean_q: 0.178583, mean_eps: 0.813765\n",
            "  207286/1750000: episode: 829, duration: 4.400s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000366, mae: 0.131673, mean_q: 0.176532, mean_eps: 0.813549\n",
            "  207460/1750000: episode: 830, duration: 3.276s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.000396, mae: 0.140814, mean_q: 0.187791, mean_eps: 0.813365\n",
            "  207711/1750000: episode: 831, duration: 4.706s, episode steps: 251, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000849, mae: 0.129134, mean_q: 0.172885, mean_eps: 0.813174\n",
            "  207899/1750000: episode: 832, duration: 3.541s, episode steps: 188, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000598, mae: 0.141592, mean_q: 0.188819, mean_eps: 0.812976\n",
            "  208134/1750000: episode: 833, duration: 4.406s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000857, mae: 0.130801, mean_q: 0.173483, mean_eps: 0.812786\n",
            "  208399/1750000: episode: 834, duration: 4.915s, episode steps: 265, steps per second:  54, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000900, mae: 0.127179, mean_q: 0.169394, mean_eps: 0.812561\n",
            "  208573/1750000: episode: 835, duration: 3.272s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.000567, mae: 0.130579, mean_q: 0.174628, mean_eps: 0.812363\n",
            "  208878/1750000: episode: 836, duration: 5.673s, episode steps: 305, steps per second:  54, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.000490, mae: 0.130452, mean_q: 0.173396, mean_eps: 0.812147\n",
            "  209049/1750000: episode: 837, duration: 3.205s, episode steps: 171, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.000502, mae: 0.129992, mean_q: 0.173056, mean_eps: 0.811932\n",
            "  209280/1750000: episode: 838, duration: 4.353s, episode steps: 231, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.000345, mae: 0.138278, mean_q: 0.183978, mean_eps: 0.811752\n",
            "  209452/1750000: episode: 839, duration: 3.315s, episode steps: 172, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000438, mae: 0.146976, mean_q: 0.196180, mean_eps: 0.811572\n",
            "  209625/1750000: episode: 840, duration: 3.328s, episode steps: 173, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.555 [0.000, 3.000],  loss: 0.000511, mae: 0.134858, mean_q: 0.180641, mean_eps: 0.811416\n",
            "  209801/1750000: episode: 841, duration: 3.321s, episode steps: 176, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.733 [0.000, 3.000],  loss: 0.000827, mae: 0.142543, mean_q: 0.191361, mean_eps: 0.811257\n",
            "  210090/1750000: episode: 842, duration: 5.396s, episode steps: 289, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: 0.000703, mae: 0.133112, mean_q: 0.176418, mean_eps: 0.811049\n",
            "  210260/1750000: episode: 843, duration: 3.239s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.001435, mae: 0.141538, mean_q: 0.187204, mean_eps: 0.810843\n",
            "  210570/1750000: episode: 844, duration: 5.832s, episode steps: 310, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.000869, mae: 0.132254, mean_q: 0.178020, mean_eps: 0.810627\n",
            "  210841/1750000: episode: 845, duration: 5.077s, episode steps: 271, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.001188, mae: 0.144092, mean_q: 0.191718, mean_eps: 0.810365\n",
            "  211004/1750000: episode: 846, duration: 3.092s, episode steps: 163, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.313 [0.000, 3.000],  loss: 0.000842, mae: 0.146887, mean_q: 0.196007, mean_eps: 0.810170\n",
            "  211273/1750000: episode: 847, duration: 5.098s, episode steps: 269, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000505, mae: 0.143832, mean_q: 0.191806, mean_eps: 0.809976\n",
            "  211443/1750000: episode: 848, duration: 3.191s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.000630, mae: 0.142672, mean_q: 0.190710, mean_eps: 0.809778\n",
            "  211645/1750000: episode: 849, duration: 3.801s, episode steps: 202, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000760, mae: 0.130045, mean_q: 0.175234, mean_eps: 0.809610\n",
            "  211822/1750000: episode: 850, duration: 3.288s, episode steps: 177, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.311 [0.000, 3.000],  loss: 0.000857, mae: 0.147301, mean_q: 0.196450, mean_eps: 0.809439\n",
            "  211992/1750000: episode: 851, duration: 3.229s, episode steps: 170, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000443, mae: 0.152107, mean_q: 0.202894, mean_eps: 0.809285\n",
            "  212193/1750000: episode: 852, duration: 3.801s, episode steps: 201, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000763, mae: 0.143963, mean_q: 0.191747, mean_eps: 0.809117\n",
            "  212367/1750000: episode: 853, duration: 3.221s, episode steps: 174, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.621 [0.000, 3.000],  loss: 0.000352, mae: 0.136577, mean_q: 0.182691, mean_eps: 0.808948\n",
            "  212623/1750000: episode: 854, duration: 4.817s, episode steps: 256, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000719, mae: 0.142962, mean_q: 0.190484, mean_eps: 0.808755\n",
            "  212830/1750000: episode: 855, duration: 3.919s, episode steps: 207, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000553, mae: 0.142215, mean_q: 0.189607, mean_eps: 0.808547\n",
            "  213141/1750000: episode: 856, duration: 5.904s, episode steps: 311, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001044, mae: 0.133791, mean_q: 0.178118, mean_eps: 0.808313\n",
            "  213347/1750000: episode: 857, duration: 3.837s, episode steps: 206, steps per second:  54, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000636, mae: 0.142064, mean_q: 0.188898, mean_eps: 0.808080\n",
            "  213537/1750000: episode: 858, duration: 3.606s, episode steps: 190, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.000858, mae: 0.134124, mean_q: 0.179817, mean_eps: 0.807902\n",
            "  213840/1750000: episode: 859, duration: 5.721s, episode steps: 303, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000806, mae: 0.145210, mean_q: 0.192475, mean_eps: 0.807681\n",
            "  214087/1750000: episode: 860, duration: 4.668s, episode steps: 247, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000996, mae: 0.135277, mean_q: 0.179680, mean_eps: 0.807434\n",
            "  214309/1750000: episode: 861, duration: 4.183s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000717, mae: 0.135868, mean_q: 0.181205, mean_eps: 0.807222\n",
            "  214489/1750000: episode: 862, duration: 3.395s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.633 [0.000, 3.000],  loss: 0.000983, mae: 0.127999, mean_q: 0.171485, mean_eps: 0.807040\n",
            "  214824/1750000: episode: 863, duration: 6.297s, episode steps: 335, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000580, mae: 0.139895, mean_q: 0.187062, mean_eps: 0.806810\n",
            "  215020/1750000: episode: 864, duration: 3.723s, episode steps: 196, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000999, mae: 0.144817, mean_q: 0.192650, mean_eps: 0.806572\n",
            "  215197/1750000: episode: 865, duration: 3.362s, episode steps: 177, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.508 [0.000, 3.000],  loss: 0.000662, mae: 0.154652, mean_q: 0.205684, mean_eps: 0.806403\n",
            "  215528/1750000: episode: 866, duration: 6.187s, episode steps: 331, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000754, mae: 0.145953, mean_q: 0.195466, mean_eps: 0.806174\n",
            "  215703/1750000: episode: 867, duration: 3.279s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.257 [0.000, 3.000],  loss: 0.000515, mae: 0.142077, mean_q: 0.189717, mean_eps: 0.805947\n",
            "  215876/1750000: episode: 868, duration: 3.291s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.775 [0.000, 3.000],  loss: 0.000759, mae: 0.139654, mean_q: 0.186965, mean_eps: 0.805791\n",
            "  216088/1750000: episode: 869, duration: 4.013s, episode steps: 212, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000597, mae: 0.134515, mean_q: 0.179316, mean_eps: 0.805618\n",
            "  216262/1750000: episode: 870, duration: 3.314s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000412, mae: 0.133896, mean_q: 0.178647, mean_eps: 0.805443\n",
            "  216580/1750000: episode: 871, duration: 6.003s, episode steps: 318, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000970, mae: 0.140481, mean_q: 0.187590, mean_eps: 0.805222\n",
            "  216760/1750000: episode: 872, duration: 3.439s, episode steps: 180, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000760, mae: 0.144471, mean_q: 0.193311, mean_eps: 0.804999\n",
            "  216972/1750000: episode: 873, duration: 4.026s, episode steps: 212, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000595, mae: 0.138033, mean_q: 0.184819, mean_eps: 0.804822\n",
            "  217263/1750000: episode: 874, duration: 5.464s, episode steps: 291, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001031, mae: 0.139533, mean_q: 0.187212, mean_eps: 0.804596\n",
            "  217603/1750000: episode: 875, duration: 6.357s, episode steps: 340, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000546, mae: 0.132614, mean_q: 0.176976, mean_eps: 0.804311\n",
            "  217876/1750000: episode: 876, duration: 5.208s, episode steps: 273, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000809, mae: 0.139804, mean_q: 0.185793, mean_eps: 0.804036\n",
            "  218111/1750000: episode: 877, duration: 4.452s, episode steps: 235, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000943, mae: 0.145043, mean_q: 0.193678, mean_eps: 0.803807\n",
            "  218299/1750000: episode: 878, duration: 3.561s, episode steps: 188, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.319 [0.000, 3.000],  loss: 0.000627, mae: 0.142053, mean_q: 0.189217, mean_eps: 0.803616\n",
            "  218522/1750000: episode: 879, duration: 4.193s, episode steps: 223, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000676, mae: 0.140814, mean_q: 0.188708, mean_eps: 0.803431\n",
            "  218797/1750000: episode: 880, duration: 5.147s, episode steps: 275, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000300, mae: 0.139566, mean_q: 0.186234, mean_eps: 0.803206\n",
            "  219086/1750000: episode: 881, duration: 5.389s, episode steps: 289, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000592, mae: 0.139130, mean_q: 0.186151, mean_eps: 0.802952\n",
            "  219261/1750000: episode: 882, duration: 3.287s, episode steps: 175, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000414, mae: 0.133081, mean_q: 0.178372, mean_eps: 0.802743\n",
            "  219584/1750000: episode: 883, duration: 6.049s, episode steps: 323, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000584, mae: 0.147569, mean_q: 0.196391, mean_eps: 0.802520\n",
            "  219940/1750000: episode: 884, duration: 6.750s, episode steps: 356, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000604, mae: 0.141304, mean_q: 0.189035, mean_eps: 0.802216\n",
            "  220148/1750000: episode: 885, duration: 3.991s, episode steps: 208, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000943, mae: 0.155876, mean_q: 0.207515, mean_eps: 0.801962\n",
            "  220398/1750000: episode: 886, duration: 4.725s, episode steps: 250, steps per second:  53, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001156, mae: 0.152325, mean_q: 0.203108, mean_eps: 0.801755\n",
            "  220682/1750000: episode: 887, duration: 5.328s, episode steps: 284, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001152, mae: 0.156013, mean_q: 0.208928, mean_eps: 0.801514\n",
            "  220922/1750000: episode: 888, duration: 4.543s, episode steps: 240, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001123, mae: 0.147486, mean_q: 0.196315, mean_eps: 0.801278\n",
            "  221264/1750000: episode: 889, duration: 6.417s, episode steps: 342, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000968, mae: 0.152785, mean_q: 0.202261, mean_eps: 0.801017\n",
            "  221603/1750000: episode: 890, duration: 6.365s, episode steps: 339, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.001143, mae: 0.152592, mean_q: 0.204427, mean_eps: 0.800711\n",
            "  221825/1750000: episode: 891, duration: 4.217s, episode steps: 222, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000631, mae: 0.145204, mean_q: 0.194100, mean_eps: 0.800457\n",
            "  222337/1750000: episode: 892, duration: 9.658s, episode steps: 512, steps per second:  53, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000951, mae: 0.150904, mean_q: 0.200456, mean_eps: 0.800126\n",
            "  222510/1750000: episode: 893, duration: 3.252s, episode steps: 173, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.075 [0.000, 3.000],  loss: 0.000895, mae: 0.141623, mean_q: 0.187587, mean_eps: 0.799818\n",
            "  222945/1750000: episode: 894, duration: 8.193s, episode steps: 435, steps per second:  53, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.000810, mae: 0.149563, mean_q: 0.198734, mean_eps: 0.799545\n",
            "  223140/1750000: episode: 895, duration: 3.683s, episode steps: 195, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.677 [0.000, 3.000],  loss: 0.000560, mae: 0.149044, mean_q: 0.198316, mean_eps: 0.799262\n",
            "  223355/1750000: episode: 896, duration: 4.071s, episode steps: 215, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.000964, mae: 0.143758, mean_q: 0.191305, mean_eps: 0.799079\n",
            "  223534/1750000: episode: 897, duration: 3.416s, episode steps: 179, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000550, mae: 0.161389, mean_q: 0.215530, mean_eps: 0.798900\n",
            "  223812/1750000: episode: 898, duration: 5.251s, episode steps: 278, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000863, mae: 0.152868, mean_q: 0.204807, mean_eps: 0.798695\n",
            "  224150/1750000: episode: 899, duration: 6.395s, episode steps: 338, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000669, mae: 0.154683, mean_q: 0.205690, mean_eps: 0.798418\n",
            "  224423/1750000: episode: 900, duration: 5.112s, episode steps: 273, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.000775, mae: 0.140915, mean_q: 0.186990, mean_eps: 0.798143\n",
            "  224700/1750000: episode: 901, duration: 5.231s, episode steps: 277, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.000831, mae: 0.148832, mean_q: 0.199266, mean_eps: 0.797896\n",
            "  225103/1750000: episode: 902, duration: 7.591s, episode steps: 403, steps per second:  53, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.000450, mae: 0.147834, mean_q: 0.196997, mean_eps: 0.797590\n",
            "  225427/1750000: episode: 903, duration: 6.186s, episode steps: 324, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000569, mae: 0.145198, mean_q: 0.193492, mean_eps: 0.797262\n",
            "  225613/1750000: episode: 904, duration: 3.527s, episode steps: 186, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000898, mae: 0.151082, mean_q: 0.201161, mean_eps: 0.797032\n",
            "  225951/1750000: episode: 905, duration: 6.306s, episode steps: 338, steps per second:  54, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.001004, mae: 0.155258, mean_q: 0.205873, mean_eps: 0.796796\n",
            "  226164/1750000: episode: 906, duration: 4.046s, episode steps: 213, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000298, mae: 0.140002, mean_q: 0.187089, mean_eps: 0.796550\n",
            "  226448/1750000: episode: 907, duration: 5.427s, episode steps: 284, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000796, mae: 0.150206, mean_q: 0.200626, mean_eps: 0.796326\n",
            "  226740/1750000: episode: 908, duration: 5.650s, episode steps: 292, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.322 [0.000, 3.000],  loss: 0.000876, mae: 0.148929, mean_q: 0.199308, mean_eps: 0.796067\n",
            "  226949/1750000: episode: 909, duration: 4.022s, episode steps: 209, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000637, mae: 0.144220, mean_q: 0.193175, mean_eps: 0.795840\n",
            "  227115/1750000: episode: 910, duration: 3.131s, episode steps: 166, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000445, mae: 0.140254, mean_q: 0.186382, mean_eps: 0.795671\n",
            "  227349/1750000: episode: 911, duration: 4.465s, episode steps: 234, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.000338, mae: 0.152544, mean_q: 0.202991, mean_eps: 0.795491\n",
            "  227594/1750000: episode: 912, duration: 4.592s, episode steps: 245, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.000513, mae: 0.145900, mean_q: 0.194486, mean_eps: 0.795275\n",
            "  227944/1750000: episode: 913, duration: 6.592s, episode steps: 350, steps per second:  53, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000888, mae: 0.155735, mean_q: 0.207245, mean_eps: 0.795009\n",
            "  228221/1750000: episode: 914, duration: 5.262s, episode steps: 277, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.325 [0.000, 3.000],  loss: 0.000490, mae: 0.154303, mean_q: 0.206220, mean_eps: 0.794726\n",
            "  228523/1750000: episode: 915, duration: 5.644s, episode steps: 302, steps per second:  54, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.344 [0.000, 3.000],  loss: 0.000673, mae: 0.147554, mean_q: 0.196232, mean_eps: 0.794465\n",
            "  228812/1750000: episode: 916, duration: 5.531s, episode steps: 289, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000531, mae: 0.162364, mean_q: 0.216388, mean_eps: 0.794201\n",
            "  229089/1750000: episode: 917, duration: 5.276s, episode steps: 277, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.000548, mae: 0.155037, mean_q: 0.206602, mean_eps: 0.793945\n",
            "  229409/1750000: episode: 918, duration: 6.108s, episode steps: 320, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000768, mae: 0.150972, mean_q: 0.200975, mean_eps: 0.793675\n",
            "  229592/1750000: episode: 919, duration: 3.476s, episode steps: 183, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000998, mae: 0.143189, mean_q: 0.192774, mean_eps: 0.793450\n",
            "  229896/1750000: episode: 920, duration: 5.774s, episode steps: 304, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000966, mae: 0.152551, mean_q: 0.204631, mean_eps: 0.793232\n",
            "  230171/1750000: episode: 921, duration: 5.191s, episode steps: 275, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000950, mae: 0.146556, mean_q: 0.195302, mean_eps: 0.792971\n",
            "  230412/1750000: episode: 922, duration: 4.637s, episode steps: 241, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001294, mae: 0.152679, mean_q: 0.205073, mean_eps: 0.792739\n",
            "  230592/1750000: episode: 923, duration: 3.489s, episode steps: 180, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.767 [0.000, 3.000],  loss: 0.001634, mae: 0.153529, mean_q: 0.208547, mean_eps: 0.792550\n",
            "  230852/1750000: episode: 924, duration: 5.014s, episode steps: 260, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001314, mae: 0.158283, mean_q: 0.210359, mean_eps: 0.792352\n",
            "  231030/1750000: episode: 925, duration: 3.405s, episode steps: 178, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.309 [0.000, 3.000],  loss: 0.000660, mae: 0.152401, mean_q: 0.204848, mean_eps: 0.792154\n",
            "  231192/1750000: episode: 926, duration: 3.123s, episode steps: 162, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.001201, mae: 0.143793, mean_q: 0.190159, mean_eps: 0.792001\n",
            "  231382/1750000: episode: 927, duration: 3.602s, episode steps: 190, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000619, mae: 0.145310, mean_q: 0.193562, mean_eps: 0.791843\n",
            "  231756/1750000: episode: 928, duration: 7.024s, episode steps: 374, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000491, mae: 0.143315, mean_q: 0.190560, mean_eps: 0.791589\n",
            "  232005/1750000: episode: 929, duration: 4.757s, episode steps: 249, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000909, mae: 0.153682, mean_q: 0.204898, mean_eps: 0.791308\n",
            "  232309/1750000: episode: 930, duration: 5.753s, episode steps: 304, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.184 [0.000, 3.000],  loss: 0.000557, mae: 0.150452, mean_q: 0.200083, mean_eps: 0.791058\n",
            "  232607/1750000: episode: 931, duration: 5.578s, episode steps: 298, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000784, mae: 0.158477, mean_q: 0.211099, mean_eps: 0.790788\n",
            "  232817/1750000: episode: 932, duration: 3.989s, episode steps: 210, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000740, mae: 0.144468, mean_q: 0.192305, mean_eps: 0.790559\n",
            "  233019/1750000: episode: 933, duration: 3.767s, episode steps: 202, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000586, mae: 0.138355, mean_q: 0.185149, mean_eps: 0.790374\n",
            "  233193/1750000: episode: 934, duration: 3.303s, episode steps: 174, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000975, mae: 0.144964, mean_q: 0.192265, mean_eps: 0.790205\n",
            "  233362/1750000: episode: 935, duration: 3.179s, episode steps: 169, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.000811, mae: 0.140869, mean_q: 0.189698, mean_eps: 0.790050\n",
            "  233589/1750000: episode: 936, duration: 4.330s, episode steps: 227, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.233 [0.000, 3.000],  loss: 0.000606, mae: 0.153858, mean_q: 0.205442, mean_eps: 0.789872\n",
            "  233959/1750000: episode: 937, duration: 6.953s, episode steps: 370, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000851, mae: 0.145185, mean_q: 0.193217, mean_eps: 0.789603\n",
            "  234250/1750000: episode: 938, duration: 5.506s, episode steps: 291, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.000964, mae: 0.152145, mean_q: 0.204002, mean_eps: 0.789306\n",
            "  234430/1750000: episode: 939, duration: 3.405s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000695, mae: 0.152602, mean_q: 0.202997, mean_eps: 0.789094\n",
            "  234832/1750000: episode: 940, duration: 7.580s, episode steps: 402, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 0.000691, mae: 0.156830, mean_q: 0.207962, mean_eps: 0.788833\n",
            "  235113/1750000: episode: 941, duration: 5.402s, episode steps: 281, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.000841, mae: 0.153712, mean_q: 0.205317, mean_eps: 0.788525\n",
            "  235354/1750000: episode: 942, duration: 4.565s, episode steps: 241, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000733, mae: 0.146360, mean_q: 0.196219, mean_eps: 0.788289\n",
            "  235585/1750000: episode: 943, duration: 4.408s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 0.000694, mae: 0.156420, mean_q: 0.208107, mean_eps: 0.788077\n",
            "  235813/1750000: episode: 944, duration: 4.329s, episode steps: 228, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.000381, mae: 0.154105, mean_q: 0.205438, mean_eps: 0.787870\n",
            "  236298/1750000: episode: 945, duration: 9.088s, episode steps: 485, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000398, mae: 0.150864, mean_q: 0.201090, mean_eps: 0.787550\n",
            "  236568/1750000: episode: 946, duration: 5.169s, episode steps: 270, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.000567, mae: 0.151721, mean_q: 0.202936, mean_eps: 0.787211\n",
            "  236726/1750000: episode: 947, duration: 3.070s, episode steps: 158, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.000603, mae: 0.151733, mean_q: 0.203356, mean_eps: 0.787019\n",
            "  236976/1750000: episode: 948, duration: 4.747s, episode steps: 250, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.000572, mae: 0.151149, mean_q: 0.201496, mean_eps: 0.786835\n",
            "  237352/1750000: episode: 949, duration: 7.134s, episode steps: 376, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001228, mae: 0.152826, mean_q: 0.204002, mean_eps: 0.786554\n",
            "  237681/1750000: episode: 950, duration: 6.275s, episode steps: 329, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 0.001459, mae: 0.148637, mean_q: 0.199486, mean_eps: 0.786236\n",
            "  237848/1750000: episode: 951, duration: 3.184s, episode steps: 167, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000673, mae: 0.137103, mean_q: 0.182380, mean_eps: 0.786012\n",
            "  238141/1750000: episode: 952, duration: 5.585s, episode steps: 293, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.000960, mae: 0.142200, mean_q: 0.189163, mean_eps: 0.785805\n",
            "  238327/1750000: episode: 953, duration: 3.483s, episode steps: 186, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000547, mae: 0.150610, mean_q: 0.200558, mean_eps: 0.785589\n",
            "  238493/1750000: episode: 954, duration: 3.169s, episode steps: 166, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000859, mae: 0.142127, mean_q: 0.188057, mean_eps: 0.785431\n",
            "  238779/1750000: episode: 955, duration: 5.365s, episode steps: 286, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000894, mae: 0.144881, mean_q: 0.192789, mean_eps: 0.785228\n",
            "  238983/1750000: episode: 956, duration: 3.852s, episode steps: 204, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.000231, mae: 0.137086, mean_q: 0.182842, mean_eps: 0.785008\n",
            "  239276/1750000: episode: 957, duration: 5.628s, episode steps: 293, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000679, mae: 0.148726, mean_q: 0.198041, mean_eps: 0.784785\n",
            "  239459/1750000: episode: 958, duration: 3.513s, episode steps: 183, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 0.000772, mae: 0.145443, mean_q: 0.194478, mean_eps: 0.784571\n",
            "  239649/1750000: episode: 959, duration: 3.668s, episode steps: 190, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.726 [0.000, 3.000],  loss: 0.000509, mae: 0.147864, mean_q: 0.197669, mean_eps: 0.784401\n",
            "  239875/1750000: episode: 960, duration: 4.278s, episode steps: 226, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.000790, mae: 0.145590, mean_q: 0.195557, mean_eps: 0.784214\n",
            "  240049/1750000: episode: 961, duration: 3.318s, episode steps: 174, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000774, mae: 0.151583, mean_q: 0.201196, mean_eps: 0.784034\n",
            "  240265/1750000: episode: 962, duration: 4.089s, episode steps: 216, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001472, mae: 0.145629, mean_q: 0.193009, mean_eps: 0.783858\n",
            "  240579/1750000: episode: 963, duration: 5.878s, episode steps: 314, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.001383, mae: 0.160212, mean_q: 0.214752, mean_eps: 0.783620\n",
            "  241006/1750000: episode: 964, duration: 8.091s, episode steps: 427, steps per second:  53, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001276, mae: 0.152727, mean_q: 0.204032, mean_eps: 0.783287\n",
            "  241198/1750000: episode: 965, duration: 3.625s, episode steps: 192, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.578 [0.000, 3.000],  loss: 0.000694, mae: 0.157793, mean_q: 0.210360, mean_eps: 0.783008\n",
            "  241404/1750000: episode: 966, duration: 3.915s, episode steps: 206, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000800, mae: 0.154023, mean_q: 0.205731, mean_eps: 0.782830\n",
            "  241620/1750000: episode: 967, duration: 4.139s, episode steps: 216, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.000647, mae: 0.142931, mean_q: 0.190206, mean_eps: 0.782641\n",
            "  241839/1750000: episode: 968, duration: 4.157s, episode steps: 219, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: 0.001029, mae: 0.144384, mean_q: 0.192145, mean_eps: 0.782445\n",
            "  242113/1750000: episode: 969, duration: 5.195s, episode steps: 274, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.001212, mae: 0.145493, mean_q: 0.193252, mean_eps: 0.782222\n",
            "  242452/1750000: episode: 970, duration: 6.438s, episode steps: 339, steps per second:  53, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 0.001425, mae: 0.158805, mean_q: 0.210942, mean_eps: 0.781946\n",
            "  242682/1750000: episode: 971, duration: 4.445s, episode steps: 230, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.001234, mae: 0.152608, mean_q: 0.203918, mean_eps: 0.781691\n",
            "  242983/1750000: episode: 972, duration: 5.713s, episode steps: 301, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001148, mae: 0.153629, mean_q: 0.204534, mean_eps: 0.781451\n",
            "  243172/1750000: episode: 973, duration: 3.626s, episode steps: 189, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.001107, mae: 0.159806, mean_q: 0.212321, mean_eps: 0.781232\n",
            "  243374/1750000: episode: 974, duration: 3.845s, episode steps: 202, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.000848, mae: 0.156690, mean_q: 0.207095, mean_eps: 0.781055\n",
            "  243605/1750000: episode: 975, duration: 4.397s, episode steps: 231, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.000532, mae: 0.138735, mean_q: 0.184654, mean_eps: 0.780859\n",
            "  243843/1750000: episode: 976, duration: 4.501s, episode steps: 238, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.001027, mae: 0.151350, mean_q: 0.201895, mean_eps: 0.780648\n",
            "  244064/1750000: episode: 977, duration: 4.249s, episode steps: 221, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000850, mae: 0.147446, mean_q: 0.197232, mean_eps: 0.780443\n",
            "  244303/1750000: episode: 978, duration: 4.551s, episode steps: 239, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001281, mae: 0.154336, mean_q: 0.205194, mean_eps: 0.780236\n",
            "  244510/1750000: episode: 979, duration: 3.968s, episode steps: 207, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000996, mae: 0.148099, mean_q: 0.198445, mean_eps: 0.780035\n",
            "  244797/1750000: episode: 980, duration: 5.438s, episode steps: 287, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000756, mae: 0.149859, mean_q: 0.199443, mean_eps: 0.779811\n",
            "  245080/1750000: episode: 981, duration: 5.377s, episode steps: 283, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.000690, mae: 0.148094, mean_q: 0.197350, mean_eps: 0.779556\n",
            "  245287/1750000: episode: 982, duration: 3.956s, episode steps: 207, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000664, mae: 0.146034, mean_q: 0.194541, mean_eps: 0.779336\n",
            "  245615/1750000: episode: 983, duration: 6.232s, episode steps: 328, steps per second:  53, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001027, mae: 0.151419, mean_q: 0.202642, mean_eps: 0.779095\n",
            "  245841/1750000: episode: 984, duration: 4.326s, episode steps: 226, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.000704, mae: 0.146320, mean_q: 0.194835, mean_eps: 0.778845\n",
            "  246093/1750000: episode: 985, duration: 4.770s, episode steps: 252, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.000706, mae: 0.146197, mean_q: 0.195790, mean_eps: 0.778629\n",
            "  246301/1750000: episode: 986, duration: 3.997s, episode steps: 208, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001040, mae: 0.149755, mean_q: 0.198654, mean_eps: 0.778422\n",
            "  246581/1750000: episode: 987, duration: 5.284s, episode steps: 280, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000742, mae: 0.153527, mean_q: 0.204240, mean_eps: 0.778202\n",
            "  246820/1750000: episode: 988, duration: 4.529s, episode steps: 239, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.001403, mae: 0.153490, mean_q: 0.204889, mean_eps: 0.777970\n",
            "  247065/1750000: episode: 989, duration: 4.688s, episode steps: 245, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001215, mae: 0.153634, mean_q: 0.203838, mean_eps: 0.777752\n",
            "  247303/1750000: episode: 990, duration: 4.475s, episode steps: 238, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: 0.001279, mae: 0.147263, mean_q: 0.196668, mean_eps: 0.777534\n",
            "  247587/1750000: episode: 991, duration: 5.376s, episode steps: 284, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001225, mae: 0.142118, mean_q: 0.189592, mean_eps: 0.777300\n",
            "  247760/1750000: episode: 992, duration: 3.335s, episode steps: 173, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000933, mae: 0.156384, mean_q: 0.208292, mean_eps: 0.777095\n",
            "  248028/1750000: episode: 993, duration: 5.197s, episode steps: 268, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.328 [0.000, 3.000],  loss: 0.000881, mae: 0.158226, mean_q: 0.210161, mean_eps: 0.776897\n",
            "  248310/1750000: episode: 994, duration: 5.414s, episode steps: 282, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 0.000623, mae: 0.153497, mean_q: 0.204355, mean_eps: 0.776649\n",
            "  248501/1750000: episode: 995, duration: 3.639s, episode steps: 191, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000684, mae: 0.144253, mean_q: 0.192075, mean_eps: 0.776435\n",
            "  248715/1750000: episode: 996, duration: 4.023s, episode steps: 214, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.000886, mae: 0.148739, mean_q: 0.198509, mean_eps: 0.776253\n",
            "  248946/1750000: episode: 997, duration: 4.418s, episode steps: 231, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.645 [0.000, 3.000],  loss: 0.000778, mae: 0.156781, mean_q: 0.208736, mean_eps: 0.776053\n",
            "  249299/1750000: episode: 998, duration: 6.672s, episode steps: 353, steps per second:  53, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.000408, mae: 0.152735, mean_q: 0.203517, mean_eps: 0.775790\n",
            "  249509/1750000: episode: 999, duration: 4.025s, episode steps: 210, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.000626, mae: 0.152851, mean_q: 0.204482, mean_eps: 0.775536\n",
            "  249753/1750000: episode: 1000, duration: 4.671s, episode steps: 244, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000508, mae: 0.147932, mean_q: 0.196617, mean_eps: 0.775331\n",
            "  250053/1750000: episode: 1001, duration: 5.657s, episode steps: 300, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.000619, mae: 0.152797, mean_q: 0.202966, mean_eps: 0.775086\n",
            "  250235/1750000: episode: 1002, duration: 3.410s, episode steps: 182, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001511, mae: 0.161818, mean_q: 0.214990, mean_eps: 0.774870\n",
            "  250447/1750000: episode: 1003, duration: 3.994s, episode steps: 212, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001097, mae: 0.142983, mean_q: 0.189809, mean_eps: 0.774694\n",
            "  250628/1750000: episode: 1004, duration: 3.465s, episode steps: 181, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.376 [0.000, 3.000],  loss: 0.001267, mae: 0.159367, mean_q: 0.211122, mean_eps: 0.774518\n",
            "  250953/1750000: episode: 1005, duration: 6.205s, episode steps: 325, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001119, mae: 0.152415, mean_q: 0.201988, mean_eps: 0.774289\n",
            "  251148/1750000: episode: 1006, duration: 3.729s, episode steps: 195, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: 0.000829, mae: 0.146443, mean_q: 0.195062, mean_eps: 0.774055\n",
            "  251338/1750000: episode: 1007, duration: 3.639s, episode steps: 190, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.579 [0.000, 3.000],  loss: 0.001130, mae: 0.155887, mean_q: 0.205907, mean_eps: 0.773882\n",
            "  251616/1750000: episode: 1008, duration: 5.316s, episode steps: 278, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.001051, mae: 0.160505, mean_q: 0.213316, mean_eps: 0.773672\n",
            "  251928/1750000: episode: 1009, duration: 5.988s, episode steps: 312, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001071, mae: 0.156085, mean_q: 0.207751, mean_eps: 0.773407\n",
            "  252115/1750000: episode: 1010, duration: 3.601s, episode steps: 187, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.000848, mae: 0.140322, mean_q: 0.188413, mean_eps: 0.773182\n",
            "  252286/1750000: episode: 1011, duration: 3.297s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.269 [0.000, 3.000],  loss: 0.000537, mae: 0.159351, mean_q: 0.213145, mean_eps: 0.773020\n",
            "  252591/1750000: episode: 1012, duration: 5.796s, episode steps: 305, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000995, mae: 0.157127, mean_q: 0.209175, mean_eps: 0.772806\n",
            "  252757/1750000: episode: 1013, duration: 3.175s, episode steps: 166, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.000809, mae: 0.138892, mean_q: 0.184377, mean_eps: 0.772593\n",
            "  253000/1750000: episode: 1014, duration: 4.609s, episode steps: 243, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.321 [0.000, 3.000],  loss: 0.001208, mae: 0.156231, mean_q: 0.205613, mean_eps: 0.772410\n",
            "  253274/1750000: episode: 1015, duration: 5.271s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.000901, mae: 0.146261, mean_q: 0.194138, mean_eps: 0.772178\n",
            "  253562/1750000: episode: 1016, duration: 5.480s, episode steps: 288, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001274, mae: 0.146396, mean_q: 0.195021, mean_eps: 0.771924\n",
            "  254176/1750000: episode: 1017, duration: 11.664s, episode steps: 614, steps per second:  53, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000693, mae: 0.152276, mean_q: 0.202931, mean_eps: 0.771519\n",
            "  254395/1750000: episode: 1018, duration: 4.188s, episode steps: 219, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.000894, mae: 0.149591, mean_q: 0.199000, mean_eps: 0.771144\n",
            "  254626/1750000: episode: 1019, duration: 4.407s, episode steps: 231, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.329 [0.000, 3.000],  loss: 0.000742, mae: 0.156440, mean_q: 0.209824, mean_eps: 0.770941\n",
            "  254810/1750000: episode: 1020, duration: 3.505s, episode steps: 184, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.391 [0.000, 3.000],  loss: 0.001495, mae: 0.162268, mean_q: 0.215045, mean_eps: 0.770754\n",
            "  255016/1750000: episode: 1021, duration: 4.007s, episode steps: 206, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.001788, mae: 0.159799, mean_q: 0.213502, mean_eps: 0.770579\n",
            "  255331/1750000: episode: 1022, duration: 5.984s, episode steps: 315, steps per second:  53, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.001527, mae: 0.148631, mean_q: 0.197637, mean_eps: 0.770345\n",
            "  255643/1750000: episode: 1023, duration: 5.950s, episode steps: 312, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000930, mae: 0.149348, mean_q: 0.199550, mean_eps: 0.770063\n",
            "  255947/1750000: episode: 1024, duration: 5.736s, episode steps: 304, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.000887, mae: 0.151395, mean_q: 0.200099, mean_eps: 0.769785\n",
            "  256117/1750000: episode: 1025, duration: 3.266s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.441 [0.000, 3.000],  loss: 0.001149, mae: 0.155861, mean_q: 0.208834, mean_eps: 0.769571\n",
            "  256360/1750000: episode: 1026, duration: 4.641s, episode steps: 243, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001901, mae: 0.158232, mean_q: 0.208697, mean_eps: 0.769386\n",
            "  256634/1750000: episode: 1027, duration: 5.280s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.000987, mae: 0.157849, mean_q: 0.210797, mean_eps: 0.769154\n",
            "  256842/1750000: episode: 1028, duration: 4.032s, episode steps: 208, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.000558, mae: 0.154576, mean_q: 0.208608, mean_eps: 0.768936\n",
            "  257083/1750000: episode: 1029, duration: 4.561s, episode steps: 241, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001028, mae: 0.157618, mean_q: 0.210230, mean_eps: 0.768734\n",
            "  257310/1750000: episode: 1030, duration: 4.316s, episode steps: 227, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000548, mae: 0.146433, mean_q: 0.194782, mean_eps: 0.768524\n",
            "  257496/1750000: episode: 1031, duration: 3.557s, episode steps: 186, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.699 [0.000, 3.000],  loss: 0.000478, mae: 0.151849, mean_q: 0.201534, mean_eps: 0.768338\n",
            "  257716/1750000: episode: 1032, duration: 4.245s, episode steps: 220, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.001156, mae: 0.150729, mean_q: 0.200323, mean_eps: 0.768156\n",
            "  258000/1750000: episode: 1033, duration: 5.453s, episode steps: 284, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.001121, mae: 0.153134, mean_q: 0.204603, mean_eps: 0.767930\n",
            "  258226/1750000: episode: 1034, duration: 4.318s, episode steps: 226, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001129, mae: 0.156075, mean_q: 0.206561, mean_eps: 0.767699\n",
            "  258397/1750000: episode: 1035, duration: 3.306s, episode steps: 171, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.000602, mae: 0.159232, mean_q: 0.212079, mean_eps: 0.767519\n",
            "  258700/1750000: episode: 1036, duration: 5.787s, episode steps: 303, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.000796, mae: 0.145264, mean_q: 0.194232, mean_eps: 0.767307\n",
            "  258944/1750000: episode: 1037, duration: 4.737s, episode steps: 244, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000734, mae: 0.148913, mean_q: 0.198692, mean_eps: 0.767062\n",
            "  259113/1750000: episode: 1038, duration: 3.283s, episode steps: 169, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000742, mae: 0.151296, mean_q: 0.202491, mean_eps: 0.766875\n",
            "  259305/1750000: episode: 1039, duration: 3.659s, episode steps: 192, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000598, mae: 0.148477, mean_q: 0.197556, mean_eps: 0.766711\n",
            "  259485/1750000: episode: 1040, duration: 3.423s, episode steps: 180, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.183 [0.000, 3.000],  loss: 0.000689, mae: 0.152133, mean_q: 0.203643, mean_eps: 0.766544\n",
            "  259759/1750000: episode: 1041, duration: 5.158s, episode steps: 274, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000861, mae: 0.148990, mean_q: 0.199773, mean_eps: 0.766340\n",
            "  259997/1750000: episode: 1042, duration: 4.591s, episode steps: 238, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001260, mae: 0.155619, mean_q: 0.206470, mean_eps: 0.766110\n",
            "  260326/1750000: episode: 1043, duration: 6.281s, episode steps: 329, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.001493, mae: 0.161614, mean_q: 0.214200, mean_eps: 0.765854\n",
            "  260631/1750000: episode: 1044, duration: 5.823s, episode steps: 305, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001225, mae: 0.155596, mean_q: 0.206434, mean_eps: 0.765570\n",
            "  260822/1750000: episode: 1045, duration: 3.710s, episode steps: 191, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.408 [0.000, 3.000],  loss: 0.001460, mae: 0.159087, mean_q: 0.210135, mean_eps: 0.765347\n",
            "  261006/1750000: episode: 1046, duration: 3.543s, episode steps: 184, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000983, mae: 0.162750, mean_q: 0.215684, mean_eps: 0.765177\n",
            "  261195/1750000: episode: 1047, duration: 3.571s, episode steps: 189, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.783 [0.000, 3.000],  loss: 0.002030, mae: 0.155147, mean_q: 0.205717, mean_eps: 0.765010\n",
            "  261500/1750000: episode: 1048, duration: 5.857s, episode steps: 305, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001339, mae: 0.168755, mean_q: 0.225015, mean_eps: 0.764789\n",
            "  261880/1750000: episode: 1049, duration: 7.386s, episode steps: 380, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001250, mae: 0.162665, mean_q: 0.217739, mean_eps: 0.764481\n",
            "  262057/1750000: episode: 1050, duration: 3.432s, episode steps: 177, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.153 [0.000, 3.000],  loss: 0.001120, mae: 0.166797, mean_q: 0.221503, mean_eps: 0.764229\n",
            "  262470/1750000: episode: 1051, duration: 7.802s, episode steps: 413, steps per second:  53, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001144, mae: 0.161821, mean_q: 0.214235, mean_eps: 0.763962\n",
            "  262718/1750000: episode: 1052, duration: 4.693s, episode steps: 248, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.887 [0.000, 3.000],  loss: 0.001236, mae: 0.152432, mean_q: 0.201951, mean_eps: 0.763665\n",
            "  262922/1750000: episode: 1053, duration: 3.878s, episode steps: 204, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.461 [0.000, 3.000],  loss: 0.001315, mae: 0.164063, mean_q: 0.217858, mean_eps: 0.763462\n",
            "  263158/1750000: episode: 1054, duration: 4.500s, episode steps: 236, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.000674, mae: 0.148775, mean_q: 0.198585, mean_eps: 0.763264\n",
            "  263359/1750000: episode: 1055, duration: 3.815s, episode steps: 201, steps per second:  53, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.299 [0.000, 3.000],  loss: 0.001395, mae: 0.151575, mean_q: 0.200543, mean_eps: 0.763068\n",
            "  263564/1750000: episode: 1056, duration: 3.990s, episode steps: 205, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000990, mae: 0.163168, mean_q: 0.216969, mean_eps: 0.762886\n",
            "  263752/1750000: episode: 1057, duration: 3.634s, episode steps: 188, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.335 [0.000, 3.000],  loss: 0.001000, mae: 0.154334, mean_q: 0.204331, mean_eps: 0.762710\n",
            "  264052/1750000: episode: 1058, duration: 5.755s, episode steps: 300, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001157, mae: 0.152149, mean_q: 0.202801, mean_eps: 0.762490\n",
            "  264293/1750000: episode: 1059, duration: 4.630s, episode steps: 241, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.001614, mae: 0.154340, mean_q: 0.204565, mean_eps: 0.762245\n",
            "  264596/1750000: episode: 1060, duration: 5.799s, episode steps: 303, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.001785, mae: 0.165375, mean_q: 0.220378, mean_eps: 0.762000\n",
            "  264766/1750000: episode: 1061, duration: 3.279s, episode steps: 170, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.253 [0.000, 3.000],  loss: 0.000756, mae: 0.161985, mean_q: 0.215433, mean_eps: 0.761788\n",
            "  265007/1750000: episode: 1062, duration: 4.626s, episode steps: 241, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.000864, mae: 0.156956, mean_q: 0.207704, mean_eps: 0.761603\n",
            "  265181/1750000: episode: 1063, duration: 3.367s, episode steps: 174, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.316 [0.000, 3.000],  loss: 0.001196, mae: 0.162833, mean_q: 0.215344, mean_eps: 0.761415\n",
            "  265349/1750000: episode: 1064, duration: 3.252s, episode steps: 168, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000823, mae: 0.156498, mean_q: 0.208553, mean_eps: 0.761261\n",
            "  265532/1750000: episode: 1065, duration: 3.501s, episode steps: 183, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.262 [0.000, 3.000],  loss: 0.001174, mae: 0.155010, mean_q: 0.206746, mean_eps: 0.761104\n",
            "  265738/1750000: episode: 1066, duration: 3.964s, episode steps: 206, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000973, mae: 0.157338, mean_q: 0.210149, mean_eps: 0.760929\n",
            "  265974/1750000: episode: 1067, duration: 4.516s, episode steps: 236, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001543, mae: 0.151336, mean_q: 0.203323, mean_eps: 0.760730\n",
            "  266221/1750000: episode: 1068, duration: 4.719s, episode steps: 247, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000487, mae: 0.150918, mean_q: 0.200000, mean_eps: 0.760512\n",
            "  266454/1750000: episode: 1069, duration: 4.412s, episode steps: 233, steps per second:  53, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001050, mae: 0.159507, mean_q: 0.212443, mean_eps: 0.760296\n",
            "  266694/1750000: episode: 1070, duration: 4.589s, episode steps: 240, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.000552, mae: 0.160388, mean_q: 0.213158, mean_eps: 0.760083\n",
            "  267039/1750000: episode: 1071, duration: 6.602s, episode steps: 345, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000636, mae: 0.160996, mean_q: 0.213522, mean_eps: 0.759821\n",
            "  267272/1750000: episode: 1072, duration: 4.508s, episode steps: 233, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.000403, mae: 0.145109, mean_q: 0.193372, mean_eps: 0.759561\n",
            "  267550/1750000: episode: 1073, duration: 5.340s, episode steps: 278, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001104, mae: 0.157484, mean_q: 0.208601, mean_eps: 0.759331\n",
            "  267725/1750000: episode: 1074, duration: 3.383s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001877, mae: 0.164629, mean_q: 0.218545, mean_eps: 0.759126\n",
            "  267998/1750000: episode: 1075, duration: 5.237s, episode steps: 273, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.381 [0.000, 3.000],  loss: 0.001263, mae: 0.155361, mean_q: 0.205452, mean_eps: 0.758924\n",
            "  268200/1750000: episode: 1076, duration: 3.920s, episode steps: 202, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.931 [0.000, 3.000],  loss: 0.000524, mae: 0.171929, mean_q: 0.228795, mean_eps: 0.758712\n",
            "  268369/1750000: episode: 1077, duration: 3.288s, episode steps: 169, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001928, mae: 0.171365, mean_q: 0.228758, mean_eps: 0.758544\n",
            "  268727/1750000: episode: 1078, duration: 6.835s, episode steps: 358, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001282, mae: 0.154609, mean_q: 0.204826, mean_eps: 0.758307\n",
            "  268904/1750000: episode: 1079, duration: 3.470s, episode steps: 177, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.232 [0.000, 3.000],  loss: 0.001104, mae: 0.155291, mean_q: 0.206213, mean_eps: 0.758067\n",
            "  269095/1750000: episode: 1080, duration: 3.687s, episode steps: 191, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001511, mae: 0.152277, mean_q: 0.200747, mean_eps: 0.757902\n",
            "  269260/1750000: episode: 1081, duration: 3.241s, episode steps: 165, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001179, mae: 0.149726, mean_q: 0.199897, mean_eps: 0.757742\n",
            "  269493/1750000: episode: 1082, duration: 4.510s, episode steps: 233, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000775, mae: 0.163119, mean_q: 0.216535, mean_eps: 0.757562\n",
            "  269828/1750000: episode: 1083, duration: 6.423s, episode steps: 335, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.001284, mae: 0.149029, mean_q: 0.198540, mean_eps: 0.757306\n",
            "  270102/1750000: episode: 1084, duration: 5.253s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.001547, mae: 0.153602, mean_q: 0.204413, mean_eps: 0.757032\n",
            "  270461/1750000: episode: 1085, duration: 6.917s, episode steps: 359, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001358, mae: 0.166321, mean_q: 0.220876, mean_eps: 0.756746\n",
            "  270754/1750000: episode: 1086, duration: 5.561s, episode steps: 293, steps per second:  53, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001799, mae: 0.170511, mean_q: 0.227283, mean_eps: 0.756453\n",
            "  271133/1750000: episode: 1087, duration: 7.288s, episode steps: 379, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: 0.001154, mae: 0.154716, mean_q: 0.206454, mean_eps: 0.756150\n",
            "  271302/1750000: episode: 1088, duration: 3.258s, episode steps: 169, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000997, mae: 0.166284, mean_q: 0.222176, mean_eps: 0.755904\n",
            "  271584/1750000: episode: 1089, duration: 5.417s, episode steps: 282, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001291, mae: 0.158890, mean_q: 0.210220, mean_eps: 0.755702\n",
            "  271802/1750000: episode: 1090, duration: 4.216s, episode steps: 218, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.001034, mae: 0.153982, mean_q: 0.204010, mean_eps: 0.755477\n",
            "  272006/1750000: episode: 1091, duration: 3.895s, episode steps: 204, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.456 [0.000, 3.000],  loss: 0.001475, mae: 0.167179, mean_q: 0.222483, mean_eps: 0.755286\n",
            "  272280/1750000: episode: 1092, duration: 5.290s, episode steps: 274, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.263 [0.000, 3.000],  loss: 0.001177, mae: 0.167311, mean_q: 0.222508, mean_eps: 0.755072\n",
            "  272629/1750000: episode: 1093, duration: 6.755s, episode steps: 349, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001020, mae: 0.162165, mean_q: 0.215344, mean_eps: 0.754791\n",
            "  272840/1750000: episode: 1094, duration: 4.056s, episode steps: 211, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.000326, mae: 0.160789, mean_q: 0.214213, mean_eps: 0.754539\n",
            "  273103/1750000: episode: 1095, duration: 5.043s, episode steps: 263, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001016, mae: 0.162660, mean_q: 0.217402, mean_eps: 0.754327\n",
            "  273384/1750000: episode: 1096, duration: 5.470s, episode steps: 281, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.001102, mae: 0.158490, mean_q: 0.210444, mean_eps: 0.754082\n",
            "  273610/1750000: episode: 1097, duration: 4.381s, episode steps: 226, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001734, mae: 0.161546, mean_q: 0.215567, mean_eps: 0.753854\n",
            "  273956/1750000: episode: 1098, duration: 6.664s, episode steps: 346, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.000732, mae: 0.155391, mean_q: 0.207130, mean_eps: 0.753596\n",
            "  274447/1750000: episode: 1099, duration: 9.456s, episode steps: 491, steps per second:  52, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001586, mae: 0.165130, mean_q: 0.219893, mean_eps: 0.753220\n",
            "  274831/1750000: episode: 1100, duration: 7.349s, episode steps: 384, steps per second:  52, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000916, mae: 0.154336, mean_q: 0.205534, mean_eps: 0.752826\n",
            "  275195/1750000: episode: 1101, duration: 6.953s, episode steps: 364, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.000778, mae: 0.164160, mean_q: 0.217447, mean_eps: 0.752489\n",
            "  275470/1750000: episode: 1102, duration: 5.281s, episode steps: 275, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.320 [0.000, 3.000],  loss: 0.001011, mae: 0.153156, mean_q: 0.204604, mean_eps: 0.752201\n",
            "  275646/1750000: episode: 1103, duration: 3.406s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.000648, mae: 0.166976, mean_q: 0.221523, mean_eps: 0.751998\n",
            "  275893/1750000: episode: 1104, duration: 4.774s, episode steps: 247, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.219 [0.000, 3.000],  loss: 0.001556, mae: 0.158211, mean_q: 0.208782, mean_eps: 0.751807\n",
            "  276209/1750000: episode: 1105, duration: 6.044s, episode steps: 316, steps per second:  52, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.000969, mae: 0.159487, mean_q: 0.212123, mean_eps: 0.751553\n",
            "  276541/1750000: episode: 1106, duration: 6.340s, episode steps: 332, steps per second:  52, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.001066, mae: 0.158614, mean_q: 0.210577, mean_eps: 0.751262\n",
            "  276737/1750000: episode: 1107, duration: 3.756s, episode steps: 196, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.291 [0.000, 3.000],  loss: 0.001060, mae: 0.157364, mean_q: 0.209423, mean_eps: 0.751024\n",
            "  277044/1750000: episode: 1108, duration: 5.908s, episode steps: 307, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 0.000863, mae: 0.151231, mean_q: 0.201945, mean_eps: 0.750799\n",
            "  277271/1750000: episode: 1109, duration: 4.642s, episode steps: 227, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001314, mae: 0.165091, mean_q: 0.219717, mean_eps: 0.750560\n",
            "  277540/1750000: episode: 1110, duration: 5.565s, episode steps: 269, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001281, mae: 0.164088, mean_q: 0.219007, mean_eps: 0.750336\n",
            "  277716/1750000: episode: 1111, duration: 3.512s, episode steps: 176, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.375 [0.000, 3.000],  loss: 0.001401, mae: 0.166438, mean_q: 0.221319, mean_eps: 0.750137\n",
            "  277911/1750000: episode: 1112, duration: 3.809s, episode steps: 195, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.001514, mae: 0.167032, mean_q: 0.222771, mean_eps: 0.749969\n",
            "  278144/1750000: episode: 1113, duration: 4.550s, episode steps: 233, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000869, mae: 0.158635, mean_q: 0.211482, mean_eps: 0.749777\n",
            "  278547/1750000: episode: 1114, duration: 7.740s, episode steps: 403, steps per second:  52, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001356, mae: 0.157297, mean_q: 0.209658, mean_eps: 0.749490\n",
            "  278919/1750000: episode: 1115, duration: 7.398s, episode steps: 372, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.001339, mae: 0.158640, mean_q: 0.211299, mean_eps: 0.749141\n",
            "  279199/1750000: episode: 1116, duration: 5.391s, episode steps: 280, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000986, mae: 0.162262, mean_q: 0.217289, mean_eps: 0.748848\n",
            "  279476/1750000: episode: 1117, duration: 5.389s, episode steps: 277, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.000558, mae: 0.164554, mean_q: 0.219560, mean_eps: 0.748598\n",
            "  279651/1750000: episode: 1118, duration: 3.375s, episode steps: 175, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.446 [0.000, 3.000],  loss: 0.001512, mae: 0.166840, mean_q: 0.221436, mean_eps: 0.748394\n",
            "  279926/1750000: episode: 1119, duration: 5.294s, episode steps: 275, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.001042, mae: 0.158464, mean_q: 0.210492, mean_eps: 0.748191\n",
            "  280108/1750000: episode: 1120, duration: 3.518s, episode steps: 182, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001665, mae: 0.159070, mean_q: 0.210403, mean_eps: 0.747986\n",
            "  280293/1750000: episode: 1121, duration: 3.611s, episode steps: 185, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001224, mae: 0.152073, mean_q: 0.202281, mean_eps: 0.747820\n",
            "  280469/1750000: episode: 1122, duration: 3.370s, episode steps: 176, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.199 [0.000, 3.000],  loss: 0.001725, mae: 0.166288, mean_q: 0.221246, mean_eps: 0.747656\n",
            "  280716/1750000: episode: 1123, duration: 4.831s, episode steps: 247, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.001987, mae: 0.163884, mean_q: 0.218359, mean_eps: 0.747467\n",
            "  281021/1750000: episode: 1124, duration: 5.878s, episode steps: 305, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001273, mae: 0.159227, mean_q: 0.211068, mean_eps: 0.747219\n",
            "  281326/1750000: episode: 1125, duration: 5.873s, episode steps: 305, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.266 [0.000, 3.000],  loss: 0.001331, mae: 0.167413, mean_q: 0.222693, mean_eps: 0.746943\n",
            "  281635/1750000: episode: 1126, duration: 5.940s, episode steps: 309, steps per second:  52, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001020, mae: 0.165514, mean_q: 0.219009, mean_eps: 0.746668\n",
            "  281879/1750000: episode: 1127, duration: 4.729s, episode steps: 244, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.001015, mae: 0.167900, mean_q: 0.223391, mean_eps: 0.746420\n",
            "  282073/1750000: episode: 1128, duration: 3.785s, episode steps: 194, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.644 [0.000, 3.000],  loss: 0.000788, mae: 0.158948, mean_q: 0.211787, mean_eps: 0.746222\n",
            "  282239/1750000: episode: 1129, duration: 3.168s, episode steps: 166, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000946, mae: 0.153351, mean_q: 0.203563, mean_eps: 0.746060\n",
            "  282455/1750000: episode: 1130, duration: 4.207s, episode steps: 216, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000872, mae: 0.166747, mean_q: 0.221470, mean_eps: 0.745889\n",
            "  282667/1750000: episode: 1131, duration: 4.084s, episode steps: 212, steps per second:  52, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.000828, mae: 0.160947, mean_q: 0.214016, mean_eps: 0.745696\n",
            "  282922/1750000: episode: 1132, duration: 4.905s, episode steps: 255, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.001251, mae: 0.155673, mean_q: 0.206694, mean_eps: 0.745485\n",
            "  283193/1750000: episode: 1133, duration: 5.305s, episode steps: 271, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.000803, mae: 0.155830, mean_q: 0.206956, mean_eps: 0.745248\n",
            "  283419/1750000: episode: 1134, duration: 4.373s, episode steps: 226, steps per second:  52, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001316, mae: 0.168865, mean_q: 0.225676, mean_eps: 0.745025\n",
            "  283684/1750000: episode: 1135, duration: 5.240s, episode steps: 265, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001954, mae: 0.160812, mean_q: 0.212957, mean_eps: 0.744805\n",
            "  283881/1750000: episode: 1136, duration: 3.892s, episode steps: 197, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.391 [0.000, 3.000],  loss: 0.001678, mae: 0.155038, mean_q: 0.206254, mean_eps: 0.744596\n",
            "  284190/1750000: episode: 1137, duration: 6.049s, episode steps: 309, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000891, mae: 0.155370, mean_q: 0.205545, mean_eps: 0.744368\n",
            "  284446/1750000: episode: 1138, duration: 4.982s, episode steps: 256, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001208, mae: 0.152336, mean_q: 0.203329, mean_eps: 0.744114\n",
            "  284799/1750000: episode: 1139, duration: 6.838s, episode steps: 353, steps per second:  52, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001075, mae: 0.157662, mean_q: 0.208902, mean_eps: 0.743840\n",
            "  285031/1750000: episode: 1140, duration: 4.510s, episode steps: 232, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000874, mae: 0.166011, mean_q: 0.220933, mean_eps: 0.743577\n",
            "  285214/1750000: episode: 1141, duration: 3.601s, episode steps: 183, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.001464, mae: 0.158654, mean_q: 0.209876, mean_eps: 0.743390\n",
            "  285480/1750000: episode: 1142, duration: 5.255s, episode steps: 266, steps per second:  51, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000699, mae: 0.163296, mean_q: 0.217432, mean_eps: 0.743189\n",
            "  285921/1750000: episode: 1143, duration: 8.706s, episode steps: 441, steps per second:  51, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.001156, mae: 0.156351, mean_q: 0.207751, mean_eps: 0.742870\n",
            "  286152/1750000: episode: 1144, duration: 4.561s, episode steps: 231, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001155, mae: 0.164339, mean_q: 0.216929, mean_eps: 0.742568\n",
            "  286532/1750000: episode: 1145, duration: 7.468s, episode steps: 380, steps per second:  51, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000906, mae: 0.159893, mean_q: 0.211408, mean_eps: 0.742294\n",
            "  286709/1750000: episode: 1146, duration: 3.507s, episode steps: 177, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.689 [0.000, 3.000],  loss: 0.000512, mae: 0.160262, mean_q: 0.213151, mean_eps: 0.742042\n",
            "  286890/1750000: episode: 1147, duration: 3.514s, episode steps: 181, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.000774, mae: 0.168017, mean_q: 0.224307, mean_eps: 0.741880\n",
            "  287300/1750000: episode: 1148, duration: 7.984s, episode steps: 410, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001395, mae: 0.159655, mean_q: 0.211827, mean_eps: 0.741615\n",
            "  287533/1750000: episode: 1149, duration: 4.630s, episode steps: 233, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001167, mae: 0.159176, mean_q: 0.210574, mean_eps: 0.741326\n",
            "  287930/1750000: episode: 1150, duration: 7.715s, episode steps: 397, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001093, mae: 0.161436, mean_q: 0.214201, mean_eps: 0.741041\n",
            "  288267/1750000: episode: 1151, duration: 6.551s, episode steps: 337, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001682, mae: 0.168382, mean_q: 0.223828, mean_eps: 0.740712\n",
            "  288450/1750000: episode: 1152, duration: 3.612s, episode steps: 183, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.765 [0.000, 3.000],  loss: 0.000964, mae: 0.171757, mean_q: 0.229551, mean_eps: 0.740478\n",
            "  288679/1750000: episode: 1153, duration: 4.468s, episode steps: 229, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001158, mae: 0.155844, mean_q: 0.207400, mean_eps: 0.740292\n",
            "  288891/1750000: episode: 1154, duration: 4.164s, episode steps: 212, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.000704, mae: 0.164044, mean_q: 0.218728, mean_eps: 0.740094\n",
            "  289219/1750000: episode: 1155, duration: 6.443s, episode steps: 328, steps per second:  51, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.001274, mae: 0.170092, mean_q: 0.225118, mean_eps: 0.739851\n",
            "  289399/1750000: episode: 1156, duration: 3.545s, episode steps: 180, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000980, mae: 0.153683, mean_q: 0.206033, mean_eps: 0.739623\n",
            "  289646/1750000: episode: 1157, duration: 4.836s, episode steps: 247, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.001027, mae: 0.172461, mean_q: 0.229320, mean_eps: 0.739430\n",
            "  289936/1750000: episode: 1158, duration: 5.768s, episode steps: 290, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.001597, mae: 0.163370, mean_q: 0.217968, mean_eps: 0.739189\n",
            "  290272/1750000: episode: 1159, duration: 6.719s, episode steps: 336, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.001762, mae: 0.169091, mean_q: 0.224619, mean_eps: 0.738908\n",
            "  290444/1750000: episode: 1160, duration: 3.415s, episode steps: 172, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.866 [0.000, 3.000],  loss: 0.001400, mae: 0.166731, mean_q: 0.222923, mean_eps: 0.738680\n",
            "  290723/1750000: episode: 1161, duration: 5.387s, episode steps: 279, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001377, mae: 0.166750, mean_q: 0.222225, mean_eps: 0.738476\n",
            "  290910/1750000: episode: 1162, duration: 3.709s, episode steps: 187, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.722 [0.000, 3.000],  loss: 0.001186, mae: 0.161069, mean_q: 0.214722, mean_eps: 0.738266\n",
            "  291181/1750000: episode: 1163, duration: 5.306s, episode steps: 271, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001961, mae: 0.172489, mean_q: 0.227304, mean_eps: 0.738059\n",
            "  291357/1750000: episode: 1164, duration: 3.442s, episode steps: 176, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001009, mae: 0.169607, mean_q: 0.224600, mean_eps: 0.737857\n",
            "  291627/1750000: episode: 1165, duration: 5.237s, episode steps: 270, steps per second:  52, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001083, mae: 0.164760, mean_q: 0.219645, mean_eps: 0.737657\n",
            "  291963/1750000: episode: 1166, duration: 6.546s, episode steps: 336, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 0.001902, mae: 0.163887, mean_q: 0.217415, mean_eps: 0.737385\n",
            "  292130/1750000: episode: 1167, duration: 3.301s, episode steps: 167, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001216, mae: 0.160832, mean_q: 0.213084, mean_eps: 0.737159\n",
            "  292342/1750000: episode: 1168, duration: 4.166s, episode steps: 212, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001167, mae: 0.151258, mean_q: 0.200710, mean_eps: 0.736988\n",
            "  292560/1750000: episode: 1169, duration: 4.359s, episode steps: 218, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.001079, mae: 0.163733, mean_q: 0.217406, mean_eps: 0.736795\n",
            "  292728/1750000: episode: 1170, duration: 3.357s, episode steps: 168, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.101 [0.000, 3.000],  loss: 0.002130, mae: 0.167057, mean_q: 0.221915, mean_eps: 0.736622\n",
            "  293112/1750000: episode: 1171, duration: 7.605s, episode steps: 384, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001531, mae: 0.168986, mean_q: 0.225077, mean_eps: 0.736374\n",
            "  293379/1750000: episode: 1172, duration: 5.228s, episode steps: 267, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.001088, mae: 0.158842, mean_q: 0.211547, mean_eps: 0.736080\n",
            "  293563/1750000: episode: 1173, duration: 3.642s, episode steps: 184, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.370 [0.000, 3.000],  loss: 0.001582, mae: 0.160233, mean_q: 0.213266, mean_eps: 0.735877\n",
            "  293890/1750000: episode: 1174, duration: 6.470s, episode steps: 327, steps per second:  51, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001228, mae: 0.164470, mean_q: 0.219536, mean_eps: 0.735647\n",
            "  294184/1750000: episode: 1175, duration: 5.882s, episode steps: 294, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.000840, mae: 0.165616, mean_q: 0.221579, mean_eps: 0.735368\n",
            "  294451/1750000: episode: 1176, duration: 5.279s, episode steps: 267, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001744, mae: 0.164297, mean_q: 0.216828, mean_eps: 0.735116\n",
            "  294752/1750000: episode: 1177, duration: 5.943s, episode steps: 301, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.001236, mae: 0.168661, mean_q: 0.224817, mean_eps: 0.734860\n",
            "  294929/1750000: episode: 1178, duration: 3.540s, episode steps: 177, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.153 [0.000, 3.000],  loss: 0.001670, mae: 0.173224, mean_q: 0.232156, mean_eps: 0.734644\n",
            "  295394/1750000: episode: 1179, duration: 9.110s, episode steps: 465, steps per second:  51, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.001361, mae: 0.169229, mean_q: 0.224277, mean_eps: 0.734354\n",
            "  295663/1750000: episode: 1180, duration: 5.292s, episode steps: 269, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001146, mae: 0.164265, mean_q: 0.218976, mean_eps: 0.734025\n",
            "  295849/1750000: episode: 1181, duration: 3.675s, episode steps: 186, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.731 [0.000, 3.000],  loss: 0.001340, mae: 0.168618, mean_q: 0.224279, mean_eps: 0.733820\n",
            "  296142/1750000: episode: 1182, duration: 5.794s, episode steps: 293, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.000849, mae: 0.169069, mean_q: 0.225211, mean_eps: 0.733604\n",
            "  296319/1750000: episode: 1183, duration: 3.502s, episode steps: 177, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.367 [0.000, 3.000],  loss: 0.001444, mae: 0.177128, mean_q: 0.236064, mean_eps: 0.733393\n",
            "  296591/1750000: episode: 1184, duration: 5.348s, episode steps: 272, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.001485, mae: 0.163076, mean_q: 0.218242, mean_eps: 0.733191\n",
            "  296784/1750000: episode: 1185, duration: 3.825s, episode steps: 193, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.741 [0.000, 3.000],  loss: 0.001073, mae: 0.166500, mean_q: 0.222192, mean_eps: 0.732983\n",
            "  297088/1750000: episode: 1186, duration: 6.027s, episode steps: 304, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.000955, mae: 0.159322, mean_q: 0.211461, mean_eps: 0.732759\n",
            "  297332/1750000: episode: 1187, duration: 4.873s, episode steps: 244, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.000596, mae: 0.159607, mean_q: 0.212054, mean_eps: 0.732513\n",
            "  297527/1750000: episode: 1188, duration: 3.889s, episode steps: 195, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.241 [0.000, 3.000],  loss: 0.001168, mae: 0.167246, mean_q: 0.223096, mean_eps: 0.732315\n",
            "  297842/1750000: episode: 1189, duration: 6.184s, episode steps: 315, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.273 [0.000, 3.000],  loss: 0.000757, mae: 0.164022, mean_q: 0.218341, mean_eps: 0.732084\n",
            "  298110/1750000: episode: 1190, duration: 5.291s, episode steps: 268, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001244, mae: 0.163696, mean_q: 0.216647, mean_eps: 0.731822\n",
            "  298359/1750000: episode: 1191, duration: 4.920s, episode steps: 249, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.000867, mae: 0.161251, mean_q: 0.213498, mean_eps: 0.731589\n",
            "  298599/1750000: episode: 1192, duration: 4.791s, episode steps: 240, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001091, mae: 0.162208, mean_q: 0.215101, mean_eps: 0.731370\n",
            "  298769/1750000: episode: 1193, duration: 3.389s, episode steps: 170, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001285, mae: 0.146130, mean_q: 0.195328, mean_eps: 0.731184\n",
            "  298947/1750000: episode: 1194, duration: 3.501s, episode steps: 178, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000518, mae: 0.157625, mean_q: 0.209514, mean_eps: 0.731028\n",
            "  299202/1750000: episode: 1195, duration: 5.073s, episode steps: 255, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.243 [0.000, 3.000],  loss: 0.001598, mae: 0.171067, mean_q: 0.228156, mean_eps: 0.730833\n",
            "  299409/1750000: episode: 1196, duration: 4.096s, episode steps: 207, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.001613, mae: 0.166556, mean_q: 0.220430, mean_eps: 0.730625\n",
            "  299632/1750000: episode: 1197, duration: 4.363s, episode steps: 223, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.000917, mae: 0.172276, mean_q: 0.229738, mean_eps: 0.730432\n",
            "  300006/1750000: episode: 1198, duration: 7.316s, episode steps: 374, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000911, mae: 0.161009, mean_q: 0.214731, mean_eps: 0.730164\n",
            "  300298/1750000: episode: 1199, duration: 5.681s, episode steps: 292, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.001692, mae: 0.162939, mean_q: 0.216256, mean_eps: 0.729863\n",
            "  300704/1750000: episode: 1200, duration: 8.008s, episode steps: 406, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002127, mae: 0.157562, mean_q: 0.207624, mean_eps: 0.729550\n",
            "  300880/1750000: episode: 1201, duration: 3.581s, episode steps: 176, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.648 [0.000, 3.000],  loss: 0.001873, mae: 0.156472, mean_q: 0.207099, mean_eps: 0.729289\n",
            "  301116/1750000: episode: 1202, duration: 4.714s, episode steps: 236, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001823, mae: 0.183691, mean_q: 0.243790, mean_eps: 0.729104\n",
            "  301535/1750000: episode: 1203, duration: 8.240s, episode steps: 419, steps per second:  51, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.001807, mae: 0.176117, mean_q: 0.234516, mean_eps: 0.728808\n",
            "  301756/1750000: episode: 1204, duration: 4.413s, episode steps: 221, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.002273, mae: 0.181252, mean_q: 0.239825, mean_eps: 0.728520\n",
            "  301988/1750000: episode: 1205, duration: 4.659s, episode steps: 232, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.002691, mae: 0.171282, mean_q: 0.227321, mean_eps: 0.728317\n",
            "  302216/1750000: episode: 1206, duration: 4.590s, episode steps: 228, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000999, mae: 0.171999, mean_q: 0.228747, mean_eps: 0.728110\n",
            "  302681/1750000: episode: 1207, duration: 9.315s, episode steps: 465, steps per second:  50, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001285, mae: 0.174397, mean_q: 0.232153, mean_eps: 0.727797\n",
            "  303044/1750000: episode: 1208, duration: 7.208s, episode steps: 363, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001123, mae: 0.170721, mean_q: 0.226615, mean_eps: 0.727424\n",
            "  303381/1750000: episode: 1209, duration: 6.681s, episode steps: 337, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000997, mae: 0.169393, mean_q: 0.225358, mean_eps: 0.727109\n",
            "  303560/1750000: episode: 1210, duration: 3.531s, episode steps: 179, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.760 [0.000, 3.000],  loss: 0.000853, mae: 0.179596, mean_q: 0.240394, mean_eps: 0.726877\n",
            "  303850/1750000: episode: 1211, duration: 5.751s, episode steps: 290, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: 0.001799, mae: 0.177496, mean_q: 0.238813, mean_eps: 0.726666\n",
            "  304130/1750000: episode: 1212, duration: 5.534s, episode steps: 280, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.001643, mae: 0.170554, mean_q: 0.227603, mean_eps: 0.726409\n",
            "  304400/1750000: episode: 1213, duration: 5.365s, episode steps: 270, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002007, mae: 0.178962, mean_q: 0.236794, mean_eps: 0.726162\n",
            "  304642/1750000: episode: 1214, duration: 4.823s, episode steps: 242, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.001747, mae: 0.177834, mean_q: 0.235951, mean_eps: 0.725932\n",
            "  304902/1750000: episode: 1215, duration: 5.146s, episode steps: 260, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.001578, mae: 0.169341, mean_q: 0.224478, mean_eps: 0.725705\n",
            "  305121/1750000: episode: 1216, duration: 4.371s, episode steps: 219, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.521 [0.000, 3.000],  loss: 0.001178, mae: 0.168525, mean_q: 0.224124, mean_eps: 0.725489\n",
            "  305291/1750000: episode: 1217, duration: 3.325s, episode steps: 170, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.506 [0.000, 3.000],  loss: 0.001970, mae: 0.180881, mean_q: 0.240573, mean_eps: 0.725315\n",
            "  305509/1750000: episode: 1218, duration: 4.353s, episode steps: 218, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.000618, mae: 0.163836, mean_q: 0.217923, mean_eps: 0.725140\n",
            "  305818/1750000: episode: 1219, duration: 6.161s, episode steps: 309, steps per second:  50, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.001067, mae: 0.164488, mean_q: 0.217827, mean_eps: 0.724902\n",
            "  306096/1750000: episode: 1220, duration: 5.544s, episode steps: 278, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001423, mae: 0.178215, mean_q: 0.236913, mean_eps: 0.724640\n",
            "  306364/1750000: episode: 1221, duration: 5.405s, episode steps: 268, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001207, mae: 0.176236, mean_q: 0.234344, mean_eps: 0.724395\n",
            "  306600/1750000: episode: 1222, duration: 4.780s, episode steps: 236, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.002096, mae: 0.168570, mean_q: 0.222086, mean_eps: 0.724168\n",
            "  306764/1750000: episode: 1223, duration: 3.309s, episode steps: 164, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.001197, mae: 0.178711, mean_q: 0.235994, mean_eps: 0.723988\n",
            "  306981/1750000: episode: 1224, duration: 4.351s, episode steps: 217, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001313, mae: 0.172896, mean_q: 0.228499, mean_eps: 0.723815\n",
            "  307337/1750000: episode: 1225, duration: 6.996s, episode steps: 356, steps per second:  51, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002001, mae: 0.188843, mean_q: 0.251052, mean_eps: 0.723556\n",
            "  307639/1750000: episode: 1226, duration: 5.943s, episode steps: 302, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002097, mae: 0.172955, mean_q: 0.229542, mean_eps: 0.723261\n",
            "  307959/1750000: episode: 1227, duration: 6.256s, episode steps: 320, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.869 [0.000, 3.000],  loss: 0.001789, mae: 0.181983, mean_q: 0.243537, mean_eps: 0.722982\n",
            "  308135/1750000: episode: 1228, duration: 3.494s, episode steps: 176, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000938, mae: 0.183015, mean_q: 0.243159, mean_eps: 0.722759\n",
            "  308414/1750000: episode: 1229, duration: 5.551s, episode steps: 279, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.001742, mae: 0.174014, mean_q: 0.230449, mean_eps: 0.722553\n",
            "  308684/1750000: episode: 1230, duration: 5.407s, episode steps: 270, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.000893, mae: 0.179817, mean_q: 0.239538, mean_eps: 0.722307\n",
            "  308858/1750000: episode: 1231, duration: 3.465s, episode steps: 174, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.001892, mae: 0.177864, mean_q: 0.234582, mean_eps: 0.722107\n",
            "  309176/1750000: episode: 1232, duration: 6.411s, episode steps: 318, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: 0.000890, mae: 0.178016, mean_q: 0.236388, mean_eps: 0.721886\n",
            "  309461/1750000: episode: 1233, duration: 5.698s, episode steps: 285, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001669, mae: 0.170833, mean_q: 0.228229, mean_eps: 0.721614\n",
            "  309795/1750000: episode: 1234, duration: 6.638s, episode steps: 334, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001681, mae: 0.176762, mean_q: 0.233125, mean_eps: 0.721335\n",
            "  310090/1750000: episode: 1235, duration: 5.874s, episode steps: 295, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002172, mae: 0.176668, mean_q: 0.234578, mean_eps: 0.721052\n",
            "  310379/1750000: episode: 1236, duration: 5.737s, episode steps: 289, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002225, mae: 0.189133, mean_q: 0.251895, mean_eps: 0.720789\n",
            "  310624/1750000: episode: 1237, duration: 4.951s, episode steps: 245, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.001915, mae: 0.187699, mean_q: 0.247272, mean_eps: 0.720550\n",
            "  310787/1750000: episode: 1238, duration: 3.309s, episode steps: 163, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: 0.001637, mae: 0.176245, mean_q: 0.233305, mean_eps: 0.720366\n",
            "  310966/1750000: episode: 1239, duration: 3.579s, episode steps: 179, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.788 [0.000, 3.000],  loss: 0.001532, mae: 0.182666, mean_q: 0.242833, mean_eps: 0.720212\n",
            "  311251/1750000: episode: 1240, duration: 5.637s, episode steps: 285, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002012, mae: 0.173986, mean_q: 0.229864, mean_eps: 0.720003\n",
            "  311421/1750000: episode: 1241, duration: 3.411s, episode steps: 170, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002258, mae: 0.179857, mean_q: 0.242514, mean_eps: 0.719798\n",
            "  311915/1750000: episode: 1242, duration: 9.737s, episode steps: 494, steps per second:  51, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001324, mae: 0.180347, mean_q: 0.239951, mean_eps: 0.719499\n",
            "  312180/1750000: episode: 1243, duration: 5.315s, episode steps: 265, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.002267, mae: 0.178458, mean_q: 0.235253, mean_eps: 0.719159\n",
            "  312447/1750000: episode: 1244, duration: 5.342s, episode steps: 267, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.001072, mae: 0.180664, mean_q: 0.239944, mean_eps: 0.718919\n",
            "  312680/1750000: episode: 1245, duration: 4.691s, episode steps: 233, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001847, mae: 0.184734, mean_q: 0.245282, mean_eps: 0.718694\n",
            "  313024/1750000: episode: 1246, duration: 6.872s, episode steps: 344, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002361, mae: 0.177852, mean_q: 0.235997, mean_eps: 0.718435\n",
            "  313208/1750000: episode: 1247, duration: 3.690s, episode steps: 184, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000971, mae: 0.186707, mean_q: 0.249264, mean_eps: 0.718197\n",
            "  313507/1750000: episode: 1248, duration: 5.920s, episode steps: 299, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.334 [0.000, 3.000],  loss: 0.001539, mae: 0.174043, mean_q: 0.230998, mean_eps: 0.717980\n",
            "  313723/1750000: episode: 1249, duration: 4.298s, episode steps: 216, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.001734, mae: 0.179669, mean_q: 0.238277, mean_eps: 0.717747\n",
            "  313999/1750000: episode: 1250, duration: 5.400s, episode steps: 276, steps per second:  51, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.001484, mae: 0.187220, mean_q: 0.248637, mean_eps: 0.717526\n",
            "  314261/1750000: episode: 1251, duration: 5.176s, episode steps: 262, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001208, mae: 0.182791, mean_q: 0.242799, mean_eps: 0.717283\n",
            "  314602/1750000: episode: 1252, duration: 6.803s, episode steps: 341, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001450, mae: 0.176049, mean_q: 0.234559, mean_eps: 0.717011\n",
            "  314865/1750000: episode: 1253, duration: 5.252s, episode steps: 263, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.338 [0.000, 3.000],  loss: 0.001583, mae: 0.195352, mean_q: 0.260108, mean_eps: 0.716739\n",
            "  315422/1750000: episode: 1254, duration: 10.996s, episode steps: 557, steps per second:  51, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.000821, mae: 0.176429, mean_q: 0.235471, mean_eps: 0.716370\n",
            "  315613/1750000: episode: 1255, duration: 3.824s, episode steps: 191, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.330 [0.000, 3.000],  loss: 0.000891, mae: 0.173192, mean_q: 0.229983, mean_eps: 0.716034\n",
            "  315812/1750000: episode: 1256, duration: 4.035s, episode steps: 199, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.271 [0.000, 3.000],  loss: 0.002254, mae: 0.181819, mean_q: 0.241868, mean_eps: 0.715859\n",
            "  316056/1750000: episode: 1257, duration: 4.923s, episode steps: 244, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.234 [0.000, 3.000],  loss: 0.001839, mae: 0.189885, mean_q: 0.253348, mean_eps: 0.715661\n",
            "  316310/1750000: episode: 1258, duration: 5.076s, episode steps: 254, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.000964, mae: 0.172115, mean_q: 0.229006, mean_eps: 0.715436\n",
            "  316479/1750000: episode: 1259, duration: 3.366s, episode steps: 169, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.002365, mae: 0.182836, mean_q: 0.240703, mean_eps: 0.715245\n",
            "  316821/1750000: episode: 1260, duration: 6.817s, episode steps: 342, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002141, mae: 0.174857, mean_q: 0.232420, mean_eps: 0.715015\n",
            "  317075/1750000: episode: 1261, duration: 5.046s, episode steps: 254, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.001547, mae: 0.179421, mean_q: 0.238349, mean_eps: 0.714747\n",
            "  317387/1750000: episode: 1262, duration: 6.233s, episode steps: 312, steps per second:  50, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001601, mae: 0.181115, mean_q: 0.240032, mean_eps: 0.714493\n",
            "  317724/1750000: episode: 1263, duration: 6.711s, episode steps: 337, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: 0.000882, mae: 0.177580, mean_q: 0.237265, mean_eps: 0.714201\n",
            "  318082/1750000: episode: 1264, duration: 7.132s, episode steps: 358, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.001586, mae: 0.178105, mean_q: 0.236085, mean_eps: 0.713888\n",
            "  318252/1750000: episode: 1265, duration: 3.444s, episode steps: 170, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.001268, mae: 0.171046, mean_q: 0.227289, mean_eps: 0.713651\n",
            "  318426/1750000: episode: 1266, duration: 3.547s, episode steps: 174, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001184, mae: 0.176238, mean_q: 0.234085, mean_eps: 0.713496\n",
            "  318599/1750000: episode: 1267, duration: 3.473s, episode steps: 173, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.393 [0.000, 3.000],  loss: 0.000901, mae: 0.174178, mean_q: 0.231717, mean_eps: 0.713339\n",
            "  318870/1750000: episode: 1268, duration: 5.461s, episode steps: 271, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.001824, mae: 0.183599, mean_q: 0.243471, mean_eps: 0.713139\n",
            "  319177/1750000: episode: 1269, duration: 6.169s, episode steps: 307, steps per second:  50, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001214, mae: 0.184131, mean_q: 0.245821, mean_eps: 0.712878\n",
            "  319427/1750000: episode: 1270, duration: 4.933s, episode steps: 250, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.888 [0.000, 3.000],  loss: 0.001455, mae: 0.168352, mean_q: 0.225097, mean_eps: 0.712628\n",
            "  319607/1750000: episode: 1271, duration: 3.608s, episode steps: 180, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.194 [0.000, 3.000],  loss: 0.000971, mae: 0.188112, mean_q: 0.252477, mean_eps: 0.712436\n",
            "  319869/1750000: episode: 1272, duration: 5.221s, episode steps: 262, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002219, mae: 0.173724, mean_q: 0.230214, mean_eps: 0.712236\n",
            "  320073/1750000: episode: 1273, duration: 4.106s, episode steps: 204, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002291, mae: 0.173122, mean_q: 0.229193, mean_eps: 0.712025\n",
            "  320248/1750000: episode: 1274, duration: 3.472s, episode steps: 175, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.989 [0.000, 3.000],  loss: 0.002122, mae: 0.174021, mean_q: 0.230557, mean_eps: 0.711856\n",
            "  320508/1750000: episode: 1275, duration: 5.224s, episode steps: 260, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001823, mae: 0.185003, mean_q: 0.245032, mean_eps: 0.711662\n",
            "  320947/1750000: episode: 1276, duration: 8.746s, episode steps: 439, steps per second:  50, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.001426, mae: 0.183734, mean_q: 0.243402, mean_eps: 0.711347\n",
            "  321171/1750000: episode: 1277, duration: 4.391s, episode steps: 224, steps per second:  51, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.272 [0.000, 3.000],  loss: 0.002125, mae: 0.180462, mean_q: 0.238983, mean_eps: 0.711048\n",
            "  321404/1750000: episode: 1278, duration: 4.656s, episode steps: 233, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002043, mae: 0.186311, mean_q: 0.245167, mean_eps: 0.710843\n",
            "  321619/1750000: episode: 1279, duration: 4.266s, episode steps: 215, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.002306, mae: 0.190235, mean_q: 0.252230, mean_eps: 0.710641\n",
            "  321813/1750000: episode: 1280, duration: 3.866s, episode steps: 194, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 0.001933, mae: 0.185074, mean_q: 0.245365, mean_eps: 0.710456\n",
            "  322165/1750000: episode: 1281, duration: 6.928s, episode steps: 352, steps per second:  51, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001349, mae: 0.186352, mean_q: 0.246600, mean_eps: 0.710209\n",
            "  322433/1750000: episode: 1282, duration: 5.370s, episode steps: 268, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.001327, mae: 0.184356, mean_q: 0.244519, mean_eps: 0.709930\n",
            "  322617/1750000: episode: 1283, duration: 3.694s, episode steps: 184, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001247, mae: 0.194466, mean_q: 0.261237, mean_eps: 0.709727\n",
            "  322844/1750000: episode: 1284, duration: 4.606s, episode steps: 227, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001629, mae: 0.187561, mean_q: 0.250333, mean_eps: 0.709543\n",
            "  323027/1750000: episode: 1285, duration: 3.700s, episode steps: 183, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 0.002092, mae: 0.192463, mean_q: 0.256514, mean_eps: 0.709359\n",
            "  323199/1750000: episode: 1286, duration: 3.471s, episode steps: 172, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.001970, mae: 0.189119, mean_q: 0.248463, mean_eps: 0.709199\n",
            "  323386/1750000: episode: 1287, duration: 3.744s, episode steps: 187, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.305 [0.000, 3.000],  loss: 0.002133, mae: 0.189146, mean_q: 0.252950, mean_eps: 0.709037\n",
            "  323621/1750000: episode: 1288, duration: 4.695s, episode steps: 235, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.001235, mae: 0.185647, mean_q: 0.247611, mean_eps: 0.708846\n",
            "  323784/1750000: episode: 1289, duration: 3.267s, episode steps: 163, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001667, mae: 0.193466, mean_q: 0.256026, mean_eps: 0.708668\n",
            "  324082/1750000: episode: 1290, duration: 5.990s, episode steps: 298, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.789 [0.000, 3.000],  loss: 0.001977, mae: 0.194711, mean_q: 0.258501, mean_eps: 0.708461\n",
            "  324303/1750000: episode: 1291, duration: 4.424s, episode steps: 221, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.001946, mae: 0.183903, mean_q: 0.242660, mean_eps: 0.708227\n",
            "  324490/1750000: episode: 1292, duration: 3.750s, episode steps: 187, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.021 [0.000, 3.000],  loss: 0.001419, mae: 0.180711, mean_q: 0.244145, mean_eps: 0.708044\n",
            "  324725/1750000: episode: 1293, duration: 4.734s, episode steps: 235, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001402, mae: 0.188919, mean_q: 0.249621, mean_eps: 0.707853\n",
            "  324913/1750000: episode: 1294, duration: 3.766s, episode steps: 188, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001093, mae: 0.185087, mean_q: 0.245804, mean_eps: 0.707662\n",
            "  325152/1750000: episode: 1295, duration: 4.831s, episode steps: 239, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.001663, mae: 0.198690, mean_q: 0.262537, mean_eps: 0.707471\n",
            "  325560/1750000: episode: 1296, duration: 8.190s, episode steps: 408, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.001798, mae: 0.192317, mean_q: 0.255057, mean_eps: 0.707181\n",
            "  325770/1750000: episode: 1297, duration: 4.281s, episode steps: 210, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.001706, mae: 0.201730, mean_q: 0.269267, mean_eps: 0.706902\n",
            "  326230/1750000: episode: 1298, duration: 9.155s, episode steps: 460, steps per second:  50, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.001746, mae: 0.183939, mean_q: 0.245209, mean_eps: 0.706600\n",
            "  326418/1750000: episode: 1299, duration: 3.753s, episode steps: 188, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.771 [0.000, 3.000],  loss: 0.002496, mae: 0.187598, mean_q: 0.248774, mean_eps: 0.706308\n",
            "  326583/1750000: episode: 1300, duration: 3.295s, episode steps: 165, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.000748, mae: 0.178330, mean_q: 0.236131, mean_eps: 0.706150\n",
            "  326988/1750000: episode: 1301, duration: 8.154s, episode steps: 405, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001612, mae: 0.185785, mean_q: 0.246794, mean_eps: 0.705894\n",
            "  327220/1750000: episode: 1302, duration: 4.710s, episode steps: 232, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.267 [0.000, 3.000],  loss: 0.001782, mae: 0.179983, mean_q: 0.239797, mean_eps: 0.705608\n",
            "  327550/1750000: episode: 1303, duration: 6.583s, episode steps: 330, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.001335, mae: 0.189092, mean_q: 0.251226, mean_eps: 0.705354\n",
            "  327791/1750000: episode: 1304, duration: 4.779s, episode steps: 241, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.001033, mae: 0.189035, mean_q: 0.250418, mean_eps: 0.705097\n",
            "  328067/1750000: episode: 1305, duration: 5.473s, episode steps: 276, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.225 [0.000, 3.000],  loss: 0.001150, mae: 0.183414, mean_q: 0.243574, mean_eps: 0.704865\n",
            "  328329/1750000: episode: 1306, duration: 5.267s, episode steps: 262, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.828 [0.000, 3.000],  loss: 0.001106, mae: 0.184951, mean_q: 0.244922, mean_eps: 0.704622\n",
            "  328573/1750000: episode: 1307, duration: 4.856s, episode steps: 244, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001300, mae: 0.188150, mean_q: 0.251236, mean_eps: 0.704393\n",
            "  328749/1750000: episode: 1308, duration: 3.567s, episode steps: 176, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.920 [0.000, 3.000],  loss: 0.001973, mae: 0.192246, mean_q: 0.254573, mean_eps: 0.704204\n",
            "  329024/1750000: episode: 1309, duration: 5.488s, episode steps: 275, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.236 [0.000, 3.000],  loss: 0.001727, mae: 0.186617, mean_q: 0.248232, mean_eps: 0.704003\n",
            "  329217/1750000: episode: 1310, duration: 3.854s, episode steps: 193, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.264 [0.000, 3.000],  loss: 0.000882, mae: 0.182014, mean_q: 0.241822, mean_eps: 0.703792\n",
            "  329388/1750000: episode: 1311, duration: 3.405s, episode steps: 171, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.608 [0.000, 3.000],  loss: 0.000959, mae: 0.183404, mean_q: 0.244396, mean_eps: 0.703628\n",
            "  329698/1750000: episode: 1312, duration: 6.126s, episode steps: 310, steps per second:  51, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: 0.000975, mae: 0.182895, mean_q: 0.243835, mean_eps: 0.703412\n",
            "  329916/1750000: episode: 1313, duration: 4.382s, episode steps: 218, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.001200, mae: 0.189046, mean_q: 0.251587, mean_eps: 0.703175\n",
            "  330091/1750000: episode: 1314, duration: 3.533s, episode steps: 175, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.001814, mae: 0.188588, mean_q: 0.248400, mean_eps: 0.702998\n",
            "  330404/1750000: episode: 1315, duration: 6.328s, episode steps: 313, steps per second:  49, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001638, mae: 0.189891, mean_q: 0.250416, mean_eps: 0.702779\n",
            "  330583/1750000: episode: 1316, duration: 3.626s, episode steps: 179, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002927, mae: 0.195033, mean_q: 0.261806, mean_eps: 0.702557\n",
            "  330774/1750000: episode: 1317, duration: 3.876s, episode steps: 191, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.785 [0.000, 3.000],  loss: 0.001930, mae: 0.184743, mean_q: 0.245025, mean_eps: 0.702390\n",
            "  331056/1750000: episode: 1318, duration: 5.682s, episode steps: 282, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.001929, mae: 0.205351, mean_q: 0.271731, mean_eps: 0.702177\n",
            "  331312/1750000: episode: 1319, duration: 5.180s, episode steps: 256, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001856, mae: 0.194403, mean_q: 0.256983, mean_eps: 0.701936\n",
            "  331567/1750000: episode: 1320, duration: 5.123s, episode steps: 255, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.001253, mae: 0.191870, mean_q: 0.255121, mean_eps: 0.701706\n",
            "  331926/1750000: episode: 1321, duration: 7.201s, episode steps: 359, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.001730, mae: 0.195604, mean_q: 0.260275, mean_eps: 0.701429\n",
            "  332147/1750000: episode: 1322, duration: 4.476s, episode steps: 221, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001855, mae: 0.194630, mean_q: 0.258097, mean_eps: 0.701168\n",
            "  332540/1750000: episode: 1323, duration: 7.870s, episode steps: 393, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001310, mae: 0.187058, mean_q: 0.249123, mean_eps: 0.700892\n",
            "  332721/1750000: episode: 1324, duration: 3.661s, episode steps: 181, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001758, mae: 0.186978, mean_q: 0.248417, mean_eps: 0.700633\n",
            "  333037/1750000: episode: 1325, duration: 6.330s, episode steps: 316, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001566, mae: 0.184661, mean_q: 0.244820, mean_eps: 0.700408\n",
            "  333315/1750000: episode: 1326, duration: 5.514s, episode steps: 278, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001382, mae: 0.191195, mean_q: 0.254324, mean_eps: 0.700142\n",
            "  333664/1750000: episode: 1327, duration: 7.020s, episode steps: 349, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001466, mae: 0.187871, mean_q: 0.251053, mean_eps: 0.699861\n",
            "  333907/1750000: episode: 1328, duration: 4.910s, episode steps: 243, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.346 [0.000, 3.000],  loss: 0.001649, mae: 0.196713, mean_q: 0.262017, mean_eps: 0.699594\n",
            "  334102/1750000: episode: 1329, duration: 3.917s, episode steps: 195, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002516, mae: 0.196428, mean_q: 0.258742, mean_eps: 0.699396\n",
            "  334415/1750000: episode: 1330, duration: 6.256s, episode steps: 313, steps per second:  50, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001497, mae: 0.193943, mean_q: 0.258040, mean_eps: 0.699168\n",
            "  334677/1750000: episode: 1331, duration: 5.329s, episode steps: 262, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.344 [0.000, 3.000],  loss: 0.001640, mae: 0.198487, mean_q: 0.262636, mean_eps: 0.698909\n",
            "  334915/1750000: episode: 1332, duration: 4.772s, episode steps: 238, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.374 [0.000, 3.000],  loss: 0.001205, mae: 0.191880, mean_q: 0.254316, mean_eps: 0.698684\n",
            "  335136/1750000: episode: 1333, duration: 4.531s, episode steps: 221, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.001116, mae: 0.190466, mean_q: 0.253160, mean_eps: 0.698478\n",
            "  335356/1750000: episode: 1334, duration: 4.452s, episode steps: 220, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.001317, mae: 0.187279, mean_q: 0.249258, mean_eps: 0.698280\n",
            "  335536/1750000: episode: 1335, duration: 3.694s, episode steps: 180, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.001421, mae: 0.193831, mean_q: 0.258404, mean_eps: 0.698100\n",
            "  335761/1750000: episode: 1336, duration: 4.555s, episode steps: 225, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.658 [0.000, 3.000],  loss: 0.001546, mae: 0.182581, mean_q: 0.241608, mean_eps: 0.697917\n",
            "  336008/1750000: episode: 1337, duration: 4.959s, episode steps: 247, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001912, mae: 0.204864, mean_q: 0.274012, mean_eps: 0.697704\n",
            "  336297/1750000: episode: 1338, duration: 5.828s, episode steps: 289, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001643, mae: 0.187842, mean_q: 0.250660, mean_eps: 0.697463\n",
            "  336566/1750000: episode: 1339, duration: 5.347s, episode steps: 269, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001671, mae: 0.192582, mean_q: 0.257439, mean_eps: 0.697211\n",
            "  336908/1750000: episode: 1340, duration: 6.897s, episode steps: 342, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.001577, mae: 0.187721, mean_q: 0.249959, mean_eps: 0.696938\n",
            "  337274/1750000: episode: 1341, duration: 7.382s, episode steps: 366, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001845, mae: 0.183235, mean_q: 0.243161, mean_eps: 0.696619\n",
            "  337530/1750000: episode: 1342, duration: 5.119s, episode steps: 256, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.001646, mae: 0.189197, mean_q: 0.250784, mean_eps: 0.696338\n",
            "  337764/1750000: episode: 1343, duration: 4.686s, episode steps: 234, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.001787, mae: 0.186874, mean_q: 0.247240, mean_eps: 0.696119\n",
            "  338151/1750000: episode: 1344, duration: 7.703s, episode steps: 387, steps per second:  50, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.001996, mae: 0.191149, mean_q: 0.253676, mean_eps: 0.695840\n",
            "  338464/1750000: episode: 1345, duration: 6.279s, episode steps: 313, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.001794, mae: 0.197011, mean_q: 0.260970, mean_eps: 0.695525\n",
            "  338706/1750000: episode: 1346, duration: 4.912s, episode steps: 242, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001608, mae: 0.189254, mean_q: 0.251858, mean_eps: 0.695274\n",
            "  338934/1750000: episode: 1347, duration: 4.630s, episode steps: 228, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.001371, mae: 0.191503, mean_q: 0.254491, mean_eps: 0.695062\n",
            "  339143/1750000: episode: 1348, duration: 4.218s, episode steps: 209, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.268 [0.000, 3.000],  loss: 0.000646, mae: 0.187539, mean_q: 0.249416, mean_eps: 0.694866\n",
            "  339456/1750000: episode: 1349, duration: 6.364s, episode steps: 313, steps per second:  49, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.307 [0.000, 3.000],  loss: 0.001541, mae: 0.194229, mean_q: 0.256700, mean_eps: 0.694632\n",
            "  339730/1750000: episode: 1350, duration: 5.559s, episode steps: 274, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.001561, mae: 0.189664, mean_q: 0.252080, mean_eps: 0.694367\n",
            "  339944/1750000: episode: 1351, duration: 4.330s, episode steps: 214, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.416 [0.000, 3.000],  loss: 0.001246, mae: 0.182016, mean_q: 0.242630, mean_eps: 0.694148\n",
            "  340192/1750000: episode: 1352, duration: 5.105s, episode steps: 248, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.002349, mae: 0.199584, mean_q: 0.261702, mean_eps: 0.693941\n",
            "  340414/1750000: episode: 1353, duration: 4.549s, episode steps: 222, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.815 [0.000, 3.000],  loss: 0.001901, mae: 0.208252, mean_q: 0.276510, mean_eps: 0.693728\n",
            "  340619/1750000: episode: 1354, duration: 4.143s, episode steps: 205, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002556, mae: 0.196095, mean_q: 0.259239, mean_eps: 0.693536\n",
            "  340792/1750000: episode: 1355, duration: 3.530s, episode steps: 173, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.173 [0.000, 3.000],  loss: 0.002526, mae: 0.207573, mean_q: 0.275784, mean_eps: 0.693366\n",
            "  341261/1750000: episode: 1356, duration: 9.475s, episode steps: 469, steps per second:  49, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.002540, mae: 0.195353, mean_q: 0.257172, mean_eps: 0.693077\n",
            "  341618/1750000: episode: 1357, duration: 7.142s, episode steps: 357, steps per second:  50, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001951, mae: 0.204374, mean_q: 0.270001, mean_eps: 0.692704\n",
            "  341806/1750000: episode: 1358, duration: 3.788s, episode steps: 188, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.372 [0.000, 3.000],  loss: 0.002619, mae: 0.204642, mean_q: 0.269570, mean_eps: 0.692459\n",
            "  342035/1750000: episode: 1359, duration: 4.606s, episode steps: 229, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002827, mae: 0.205911, mean_q: 0.272710, mean_eps: 0.692272\n",
            "  342257/1750000: episode: 1360, duration: 4.513s, episode steps: 222, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001915, mae: 0.199155, mean_q: 0.263631, mean_eps: 0.692069\n",
            "  342466/1750000: episode: 1361, duration: 4.187s, episode steps: 209, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001616, mae: 0.198577, mean_q: 0.263198, mean_eps: 0.691874\n",
            "  342670/1750000: episode: 1362, duration: 4.159s, episode steps: 204, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.926 [0.000, 3.000],  loss: 0.001542, mae: 0.202461, mean_q: 0.268242, mean_eps: 0.691689\n",
            "  342872/1750000: episode: 1363, duration: 4.158s, episode steps: 202, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001559, mae: 0.191063, mean_q: 0.253428, mean_eps: 0.691507\n",
            "  343046/1750000: episode: 1364, duration: 3.578s, episode steps: 174, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001858, mae: 0.205585, mean_q: 0.271295, mean_eps: 0.691338\n",
            "  343302/1750000: episode: 1365, duration: 5.155s, episode steps: 256, steps per second:  50, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.002762, mae: 0.203385, mean_q: 0.268780, mean_eps: 0.691143\n",
            "  343489/1750000: episode: 1366, duration: 3.809s, episode steps: 187, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.813 [0.000, 3.000],  loss: 0.001446, mae: 0.193893, mean_q: 0.257291, mean_eps: 0.690944\n",
            "  343917/1750000: episode: 1367, duration: 8.626s, episode steps: 428, steps per second:  50, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.001708, mae: 0.197767, mean_q: 0.261345, mean_eps: 0.690666\n",
            "  344187/1750000: episode: 1368, duration: 5.390s, episode steps: 270, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.001757, mae: 0.192697, mean_q: 0.254695, mean_eps: 0.690353\n",
            "  344412/1750000: episode: 1369, duration: 4.606s, episode steps: 225, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.302 [0.000, 3.000],  loss: 0.001800, mae: 0.201233, mean_q: 0.266108, mean_eps: 0.690132\n",
            "  344607/1750000: episode: 1370, duration: 3.942s, episode steps: 195, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002516, mae: 0.200743, mean_q: 0.265460, mean_eps: 0.689943\n",
            "  344883/1750000: episode: 1371, duration: 5.541s, episode steps: 276, steps per second:  50, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002410, mae: 0.189549, mean_q: 0.249766, mean_eps: 0.689730\n",
            "  345064/1750000: episode: 1372, duration: 3.654s, episode steps: 181, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.840 [0.000, 3.000],  loss: 0.001568, mae: 0.206141, mean_q: 0.273043, mean_eps: 0.689525\n",
            "  345255/1750000: episode: 1373, duration: 3.858s, episode steps: 191, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002315, mae: 0.206152, mean_q: 0.272886, mean_eps: 0.689358\n",
            "  345582/1750000: episode: 1374, duration: 6.628s, episode steps: 327, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002096, mae: 0.191311, mean_q: 0.252760, mean_eps: 0.689124\n",
            "  345769/1750000: episode: 1375, duration: 3.841s, episode steps: 187, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001606, mae: 0.199746, mean_q: 0.263978, mean_eps: 0.688892\n",
            "  345945/1750000: episode: 1376, duration: 3.555s, episode steps: 176, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.790 [0.000, 3.000],  loss: 0.001167, mae: 0.200142, mean_q: 0.265550, mean_eps: 0.688728\n",
            "  346196/1750000: episode: 1377, duration: 5.080s, episode steps: 251, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002136, mae: 0.208301, mean_q: 0.276810, mean_eps: 0.688537\n",
            "  346437/1750000: episode: 1378, duration: 4.924s, episode steps: 241, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001476, mae: 0.202716, mean_q: 0.268663, mean_eps: 0.688316\n",
            "  346893/1750000: episode: 1379, duration: 9.308s, episode steps: 456, steps per second:  49, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.002081, mae: 0.200054, mean_q: 0.265694, mean_eps: 0.688001\n",
            "  347117/1750000: episode: 1380, duration: 4.562s, episode steps: 224, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001673, mae: 0.198713, mean_q: 0.263190, mean_eps: 0.687695\n",
            "  347495/1750000: episode: 1381, duration: 7.547s, episode steps: 378, steps per second:  50, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.001396, mae: 0.198621, mean_q: 0.264120, mean_eps: 0.687425\n",
            "  347782/1750000: episode: 1382, duration: 5.803s, episode steps: 287, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001697, mae: 0.202818, mean_q: 0.269266, mean_eps: 0.687126\n",
            "  348088/1750000: episode: 1383, duration: 6.270s, episode steps: 306, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001560, mae: 0.186233, mean_q: 0.246975, mean_eps: 0.686859\n",
            "  348568/1750000: episode: 1384, duration: 9.775s, episode steps: 480, steps per second:  49, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.001731, mae: 0.194530, mean_q: 0.257728, mean_eps: 0.686507\n",
            "  348760/1750000: episode: 1385, duration: 3.939s, episode steps: 192, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.427 [0.000, 3.000],  loss: 0.002049, mae: 0.196108, mean_q: 0.258793, mean_eps: 0.686204\n",
            "  349036/1750000: episode: 1386, duration: 5.647s, episode steps: 276, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.001858, mae: 0.193053, mean_q: 0.254417, mean_eps: 0.685994\n",
            "  349402/1750000: episode: 1387, duration: 7.417s, episode steps: 366, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.762 [0.000, 3.000],  loss: 0.001709, mae: 0.204092, mean_q: 0.271214, mean_eps: 0.685704\n",
            "  349600/1750000: episode: 1388, duration: 4.020s, episode steps: 198, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.283 [0.000, 3.000],  loss: 0.002319, mae: 0.195401, mean_q: 0.258716, mean_eps: 0.685450\n",
            "  349867/1750000: episode: 1389, duration: 5.438s, episode steps: 267, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002207, mae: 0.204088, mean_q: 0.269569, mean_eps: 0.685241\n",
            "  350048/1750000: episode: 1390, duration: 3.740s, episode steps: 181, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002063, mae: 0.209291, mean_q: 0.275413, mean_eps: 0.685040\n",
            "  350467/1750000: episode: 1391, duration: 8.529s, episode steps: 419, steps per second:  49, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002183, mae: 0.203027, mean_q: 0.269675, mean_eps: 0.684770\n",
            "  350730/1750000: episode: 1392, duration: 5.385s, episode steps: 263, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001700, mae: 0.192348, mean_q: 0.254060, mean_eps: 0.684462\n",
            "  351024/1750000: episode: 1393, duration: 6.047s, episode steps: 294, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001487, mae: 0.195423, mean_q: 0.258858, mean_eps: 0.684212\n",
            "  351500/1750000: episode: 1394, duration: 9.675s, episode steps: 476, steps per second:  49, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001580, mae: 0.200602, mean_q: 0.267182, mean_eps: 0.683866\n",
            "  351720/1750000: episode: 1395, duration: 4.530s, episode steps: 220, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.909 [0.000, 3.000],  loss: 0.001684, mae: 0.193600, mean_q: 0.257530, mean_eps: 0.683553\n",
            "  352076/1750000: episode: 1396, duration: 7.331s, episode steps: 356, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.893 [0.000, 3.000],  loss: 0.001760, mae: 0.208897, mean_q: 0.279904, mean_eps: 0.683294\n",
            "  352303/1750000: episode: 1397, duration: 4.620s, episode steps: 227, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002484, mae: 0.199095, mean_q: 0.263311, mean_eps: 0.683031\n",
            "  352484/1750000: episode: 1398, duration: 3.718s, episode steps: 181, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000891, mae: 0.199864, mean_q: 0.266165, mean_eps: 0.682847\n",
            "  352664/1750000: episode: 1399, duration: 3.738s, episode steps: 180, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002855, mae: 0.201761, mean_q: 0.267787, mean_eps: 0.682685\n",
            "  352968/1750000: episode: 1400, duration: 6.279s, episode steps: 304, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.001992, mae: 0.200696, mean_q: 0.265997, mean_eps: 0.682467\n",
            "  353186/1750000: episode: 1401, duration: 4.407s, episode steps: 218, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001243, mae: 0.197761, mean_q: 0.261591, mean_eps: 0.682232\n",
            "  353439/1750000: episode: 1402, duration: 5.106s, episode steps: 253, steps per second:  50, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001789, mae: 0.197499, mean_q: 0.261480, mean_eps: 0.682019\n",
            "  353785/1750000: episode: 1403, duration: 6.943s, episode steps: 346, steps per second:  50, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.002282, mae: 0.193065, mean_q: 0.256360, mean_eps: 0.681749\n",
            "  354106/1750000: episode: 1404, duration: 6.371s, episode steps: 321, steps per second:  50, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002338, mae: 0.201658, mean_q: 0.267550, mean_eps: 0.681449\n",
            "  354322/1750000: episode: 1405, duration: 4.333s, episode steps: 216, steps per second:  50, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001363, mae: 0.190705, mean_q: 0.252691, mean_eps: 0.681207\n",
            "  354569/1750000: episode: 1406, duration: 5.041s, episode steps: 247, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.911 [0.000, 3.000],  loss: 0.001311, mae: 0.200494, mean_q: 0.267141, mean_eps: 0.680999\n",
            "  354807/1750000: episode: 1407, duration: 4.824s, episode steps: 238, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.001629, mae: 0.196261, mean_q: 0.260039, mean_eps: 0.680781\n",
            "  355001/1750000: episode: 1408, duration: 4.034s, episode steps: 194, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.727 [0.000, 3.000],  loss: 0.000939, mae: 0.196762, mean_q: 0.261808, mean_eps: 0.680586\n",
            "  355227/1750000: episode: 1409, duration: 4.588s, episode steps: 226, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.292 [0.000, 3.000],  loss: 0.001402, mae: 0.199672, mean_q: 0.265902, mean_eps: 0.680397\n",
            "  355677/1750000: episode: 1410, duration: 9.191s, episode steps: 450, steps per second:  49, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.002174, mae: 0.208743, mean_q: 0.278475, mean_eps: 0.680093\n",
            "  355860/1750000: episode: 1411, duration: 3.805s, episode steps: 183, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.339 [0.000, 3.000],  loss: 0.002461, mae: 0.189496, mean_q: 0.250204, mean_eps: 0.679809\n",
            "  356071/1750000: episode: 1412, duration: 4.304s, episode steps: 211, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.223 [0.000, 3.000],  loss: 0.001406, mae: 0.194677, mean_q: 0.258387, mean_eps: 0.679632\n",
            "  356250/1750000: episode: 1413, duration: 3.648s, episode steps: 179, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.001210, mae: 0.194807, mean_q: 0.257547, mean_eps: 0.679456\n",
            "  356512/1750000: episode: 1414, duration: 5.338s, episode steps: 262, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001935, mae: 0.191083, mean_q: 0.252780, mean_eps: 0.679258\n",
            "  356708/1750000: episode: 1415, duration: 4.056s, episode steps: 196, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.061 [0.000, 3.000],  loss: 0.001099, mae: 0.206161, mean_q: 0.274436, mean_eps: 0.679053\n",
            "  356949/1750000: episode: 1416, duration: 4.947s, episode steps: 241, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.001790, mae: 0.190842, mean_q: 0.253493, mean_eps: 0.678855\n",
            "  357287/1750000: episode: 1417, duration: 6.795s, episode steps: 338, steps per second:  50, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002119, mae: 0.195246, mean_q: 0.259136, mean_eps: 0.678594\n",
            "  357522/1750000: episode: 1418, duration: 4.794s, episode steps: 235, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001612, mae: 0.199920, mean_q: 0.265877, mean_eps: 0.678336\n",
            "  357760/1750000: episode: 1419, duration: 4.848s, episode steps: 238, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002218, mae: 0.196079, mean_q: 0.258940, mean_eps: 0.678124\n",
            "  358112/1750000: episode: 1420, duration: 7.220s, episode steps: 352, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.000960, mae: 0.192839, mean_q: 0.256733, mean_eps: 0.677859\n",
            "  358512/1750000: episode: 1421, duration: 8.272s, episode steps: 400, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.001894, mae: 0.202148, mean_q: 0.268407, mean_eps: 0.677521\n",
            "  358740/1750000: episode: 1422, duration: 4.780s, episode steps: 228, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.001805, mae: 0.198125, mean_q: 0.263224, mean_eps: 0.677238\n",
            "  359033/1750000: episode: 1423, duration: 6.028s, episode steps: 293, steps per second:  49, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001449, mae: 0.204840, mean_q: 0.272122, mean_eps: 0.677003\n",
            "  359247/1750000: episode: 1424, duration: 4.295s, episode steps: 214, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001666, mae: 0.200175, mean_q: 0.267380, mean_eps: 0.676774\n",
            "  359426/1750000: episode: 1425, duration: 3.695s, episode steps: 179, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002105, mae: 0.201699, mean_q: 0.266189, mean_eps: 0.676598\n",
            "  359779/1750000: episode: 1426, duration: 7.149s, episode steps: 353, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.001492, mae: 0.199782, mean_q: 0.265783, mean_eps: 0.676358\n",
            "  360055/1750000: episode: 1427, duration: 5.645s, episode steps: 276, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.001682, mae: 0.204358, mean_q: 0.270995, mean_eps: 0.676076\n",
            "  360293/1750000: episode: 1428, duration: 4.892s, episode steps: 238, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001456, mae: 0.198934, mean_q: 0.263916, mean_eps: 0.675843\n",
            "  360582/1750000: episode: 1429, duration: 5.861s, episode steps: 289, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001932, mae: 0.205717, mean_q: 0.271984, mean_eps: 0.675606\n",
            "  360859/1750000: episode: 1430, duration: 5.624s, episode steps: 277, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.874 [0.000, 3.000],  loss: 0.001594, mae: 0.201391, mean_q: 0.267609, mean_eps: 0.675352\n",
            "  361111/1750000: episode: 1431, duration: 5.127s, episode steps: 252, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.001895, mae: 0.212531, mean_q: 0.282554, mean_eps: 0.675114\n",
            "  361528/1750000: episode: 1432, duration: 8.551s, episode steps: 417, steps per second:  49, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.001707, mae: 0.216230, mean_q: 0.288040, mean_eps: 0.674814\n",
            "  361745/1750000: episode: 1433, duration: 4.511s, episode steps: 217, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.793 [0.000, 3.000],  loss: 0.002215, mae: 0.208613, mean_q: 0.278036, mean_eps: 0.674528\n",
            "  361967/1750000: episode: 1434, duration: 4.489s, episode steps: 222, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001323, mae: 0.216058, mean_q: 0.286297, mean_eps: 0.674330\n",
            "  362249/1750000: episode: 1435, duration: 5.764s, episode steps: 282, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.001261, mae: 0.211938, mean_q: 0.282402, mean_eps: 0.674103\n",
            "  362624/1750000: episode: 1436, duration: 7.725s, episode steps: 375, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001696, mae: 0.207243, mean_q: 0.276624, mean_eps: 0.673808\n",
            "  362842/1750000: episode: 1437, duration: 4.525s, episode steps: 218, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.001692, mae: 0.210509, mean_q: 0.282954, mean_eps: 0.673541\n",
            "  363255/1750000: episode: 1438, duration: 8.365s, episode steps: 413, steps per second:  49, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.001702, mae: 0.213449, mean_q: 0.283156, mean_eps: 0.673257\n",
            "  363447/1750000: episode: 1439, duration: 3.885s, episode steps: 192, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.198 [0.000, 3.000],  loss: 0.001456, mae: 0.207488, mean_q: 0.275817, mean_eps: 0.672985\n",
            "  363879/1750000: episode: 1440, duration: 8.788s, episode steps: 432, steps per second:  49, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.001629, mae: 0.211014, mean_q: 0.280659, mean_eps: 0.672704\n",
            "  364092/1750000: episode: 1441, duration: 4.396s, episode steps: 213, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001680, mae: 0.202200, mean_q: 0.269649, mean_eps: 0.672414\n",
            "  364367/1750000: episode: 1442, duration: 5.632s, episode steps: 275, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.001485, mae: 0.218753, mean_q: 0.292009, mean_eps: 0.672195\n",
            "  364640/1750000: episode: 1443, duration: 5.652s, episode steps: 273, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.001324, mae: 0.211038, mean_q: 0.281766, mean_eps: 0.671948\n",
            "  364850/1750000: episode: 1444, duration: 4.355s, episode steps: 210, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.971 [0.000, 3.000],  loss: 0.001298, mae: 0.207301, mean_q: 0.274998, mean_eps: 0.671730\n",
            "  365116/1750000: episode: 1445, duration: 5.426s, episode steps: 266, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001598, mae: 0.208202, mean_q: 0.275970, mean_eps: 0.671516\n",
            "  365434/1750000: episode: 1446, duration: 6.527s, episode steps: 318, steps per second:  49, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.001678, mae: 0.212474, mean_q: 0.283450, mean_eps: 0.671253\n",
            "  365631/1750000: episode: 1447, duration: 3.991s, episode steps: 197, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.259 [0.000, 3.000],  loss: 0.001023, mae: 0.218187, mean_q: 0.291224, mean_eps: 0.671021\n",
            "  365878/1750000: episode: 1448, duration: 5.153s, episode steps: 247, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001419, mae: 0.204598, mean_q: 0.272298, mean_eps: 0.670821\n",
            "  366060/1750000: episode: 1449, duration: 3.774s, episode steps: 182, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.857 [0.000, 3.000],  loss: 0.001988, mae: 0.212588, mean_q: 0.282970, mean_eps: 0.670629\n",
            "  366379/1750000: episode: 1450, duration: 6.600s, episode steps: 319, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001407, mae: 0.218392, mean_q: 0.290955, mean_eps: 0.670404\n",
            "  366747/1750000: episode: 1451, duration: 7.571s, episode steps: 368, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.894 [0.000, 3.000],  loss: 0.001825, mae: 0.212015, mean_q: 0.282011, mean_eps: 0.670094\n",
            "  367007/1750000: episode: 1452, duration: 5.355s, episode steps: 260, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001258, mae: 0.214134, mean_q: 0.286105, mean_eps: 0.669812\n",
            "  367408/1750000: episode: 1453, duration: 8.222s, episode steps: 401, steps per second:  49, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001566, mae: 0.205922, mean_q: 0.273427, mean_eps: 0.669515\n",
            "  367734/1750000: episode: 1454, duration: 6.749s, episode steps: 326, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001562, mae: 0.211748, mean_q: 0.281618, mean_eps: 0.669187\n",
            "  367911/1750000: episode: 1455, duration: 3.587s, episode steps: 177, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.949 [0.000, 3.000],  loss: 0.002036, mae: 0.217338, mean_q: 0.292065, mean_eps: 0.668960\n",
            "  368204/1750000: episode: 1456, duration: 6.062s, episode steps: 293, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001639, mae: 0.197109, mean_q: 0.261058, mean_eps: 0.668750\n",
            "  368409/1750000: episode: 1457, duration: 4.219s, episode steps: 205, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.351 [0.000, 3.000],  loss: 0.002091, mae: 0.229559, mean_q: 0.306821, mean_eps: 0.668525\n",
            "  368602/1750000: episode: 1458, duration: 3.928s, episode steps: 193, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.927 [0.000, 3.000],  loss: 0.001678, mae: 0.220777, mean_q: 0.294892, mean_eps: 0.668345\n",
            "  369067/1750000: episode: 1459, duration: 9.456s, episode steps: 465, steps per second:  49, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001563, mae: 0.220971, mean_q: 0.294206, mean_eps: 0.668049\n",
            "  369283/1750000: episode: 1460, duration: 4.454s, episode steps: 216, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.001366, mae: 0.208907, mean_q: 0.277596, mean_eps: 0.667743\n",
            "  369596/1750000: episode: 1461, duration: 6.496s, episode steps: 313, steps per second:  48, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001939, mae: 0.199656, mean_q: 0.264740, mean_eps: 0.667506\n",
            "  369906/1750000: episode: 1462, duration: 6.378s, episode steps: 310, steps per second:  49, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.001301, mae: 0.219718, mean_q: 0.293271, mean_eps: 0.667225\n",
            "  370112/1750000: episode: 1463, duration: 4.270s, episode steps: 206, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.002368, mae: 0.223001, mean_q: 0.294649, mean_eps: 0.666993\n",
            "  370386/1750000: episode: 1464, duration: 5.689s, episode steps: 274, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002199, mae: 0.229981, mean_q: 0.305806, mean_eps: 0.666777\n",
            "  370736/1750000: episode: 1465, duration: 7.310s, episode steps: 350, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002082, mae: 0.216727, mean_q: 0.286999, mean_eps: 0.666496\n",
            "  370937/1750000: episode: 1466, duration: 4.216s, episode steps: 201, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.323 [0.000, 3.000],  loss: 0.001828, mae: 0.216476, mean_q: 0.287509, mean_eps: 0.666248\n",
            "  371358/1750000: episode: 1467, duration: 8.569s, episode steps: 421, steps per second:  49, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.425 [0.000, 3.000],  loss: 0.001911, mae: 0.218014, mean_q: 0.290180, mean_eps: 0.665967\n",
            "  371547/1750000: episode: 1468, duration: 3.806s, episode steps: 189, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.598 [0.000, 3.000],  loss: 0.003045, mae: 0.209141, mean_q: 0.275979, mean_eps: 0.665693\n",
            "  371780/1750000: episode: 1469, duration: 4.780s, episode steps: 233, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.790 [0.000, 3.000],  loss: 0.002282, mae: 0.212468, mean_q: 0.284238, mean_eps: 0.665504\n",
            "  372055/1750000: episode: 1470, duration: 5.557s, episode steps: 275, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.002068, mae: 0.219288, mean_q: 0.291505, mean_eps: 0.665276\n",
            "  372325/1750000: episode: 1471, duration: 5.494s, episode steps: 270, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001675, mae: 0.212199, mean_q: 0.282421, mean_eps: 0.665029\n",
            "  372584/1750000: episode: 1472, duration: 5.293s, episode steps: 259, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.001679, mae: 0.207546, mean_q: 0.275953, mean_eps: 0.664791\n",
            "  372888/1750000: episode: 1473, duration: 6.278s, episode steps: 304, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.878 [0.000, 3.000],  loss: 0.002482, mae: 0.217871, mean_q: 0.290559, mean_eps: 0.664539\n",
            "  373105/1750000: episode: 1474, duration: 4.498s, episode steps: 217, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.002196, mae: 0.202298, mean_q: 0.268550, mean_eps: 0.664304\n",
            "  373390/1750000: episode: 1475, duration: 5.851s, episode steps: 285, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001899, mae: 0.215757, mean_q: 0.286619, mean_eps: 0.664077\n",
            "  373588/1750000: episode: 1476, duration: 4.087s, episode steps: 198, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.864 [0.000, 3.000],  loss: 0.001926, mae: 0.212087, mean_q: 0.283767, mean_eps: 0.663861\n",
            "  373808/1750000: episode: 1477, duration: 4.593s, episode steps: 220, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.001619, mae: 0.212070, mean_q: 0.283496, mean_eps: 0.663674\n",
            "  374231/1750000: episode: 1478, duration: 8.704s, episode steps: 423, steps per second:  49, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.797 [0.000, 3.000],  loss: 0.001939, mae: 0.219721, mean_q: 0.294259, mean_eps: 0.663384\n",
            "  374418/1750000: episode: 1479, duration: 3.925s, episode steps: 187, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.011 [0.000, 3.000],  loss: 0.001575, mae: 0.214486, mean_q: 0.285827, mean_eps: 0.663108\n",
            "  374755/1750000: episode: 1480, duration: 7.001s, episode steps: 337, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001788, mae: 0.210042, mean_q: 0.279577, mean_eps: 0.662873\n",
            "  375134/1750000: episode: 1481, duration: 7.855s, episode steps: 379, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.002161, mae: 0.218624, mean_q: 0.292395, mean_eps: 0.662550\n",
            "  375429/1750000: episode: 1482, duration: 6.085s, episode steps: 295, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.861 [0.000, 3.000],  loss: 0.002080, mae: 0.215543, mean_q: 0.287120, mean_eps: 0.662246\n",
            "  375846/1750000: episode: 1483, duration: 8.546s, episode steps: 417, steps per second:  49, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001609, mae: 0.212201, mean_q: 0.284359, mean_eps: 0.661926\n",
            "  376095/1750000: episode: 1484, duration: 5.168s, episode steps: 249, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001790, mae: 0.210610, mean_q: 0.280348, mean_eps: 0.661627\n",
            "  376297/1750000: episode: 1485, duration: 4.244s, episode steps: 202, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.743 [0.000, 3.000],  loss: 0.001706, mae: 0.218427, mean_q: 0.292111, mean_eps: 0.661424\n",
            "  376473/1750000: episode: 1486, duration: 3.612s, episode steps: 176, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.898 [0.000, 3.000],  loss: 0.001716, mae: 0.223752, mean_q: 0.300694, mean_eps: 0.661253\n",
            "  376781/1750000: episode: 1487, duration: 6.306s, episode steps: 308, steps per second:  49, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.001891, mae: 0.207144, mean_q: 0.276631, mean_eps: 0.661035\n",
            "  376999/1750000: episode: 1488, duration: 4.427s, episode steps: 218, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001793, mae: 0.219166, mean_q: 0.293318, mean_eps: 0.660799\n",
            "  377239/1750000: episode: 1489, duration: 4.944s, episode steps: 240, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001630, mae: 0.217746, mean_q: 0.289973, mean_eps: 0.660594\n",
            "  377553/1750000: episode: 1490, duration: 6.482s, episode steps: 314, steps per second:  48, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002162, mae: 0.217993, mean_q: 0.289997, mean_eps: 0.660344\n",
            "  377723/1750000: episode: 1491, duration: 3.507s, episode steps: 170, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.271 [0.000, 3.000],  loss: 0.001512, mae: 0.226735, mean_q: 0.301753, mean_eps: 0.660126\n",
            "  377896/1750000: episode: 1492, duration: 3.632s, episode steps: 173, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.653 [0.000, 3.000],  loss: 0.003090, mae: 0.228343, mean_q: 0.304385, mean_eps: 0.659973\n",
            "  378070/1750000: episode: 1493, duration: 3.620s, episode steps: 174, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.397 [0.000, 3.000],  loss: 0.001923, mae: 0.210277, mean_q: 0.279895, mean_eps: 0.659816\n",
            "  378241/1750000: episode: 1494, duration: 3.541s, episode steps: 171, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.002420, mae: 0.224320, mean_q: 0.298136, mean_eps: 0.659660\n",
            "  378472/1750000: episode: 1495, duration: 4.817s, episode steps: 231, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.381 [0.000, 3.000],  loss: 0.002532, mae: 0.217140, mean_q: 0.288364, mean_eps: 0.659480\n",
            "  378733/1750000: episode: 1496, duration: 5.436s, episode steps: 261, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001952, mae: 0.222042, mean_q: 0.294622, mean_eps: 0.659258\n",
            "  379011/1750000: episode: 1497, duration: 5.680s, episode steps: 278, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001900, mae: 0.209268, mean_q: 0.278015, mean_eps: 0.659015\n",
            "  379184/1750000: episode: 1498, duration: 3.692s, episode steps: 173, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001573, mae: 0.215236, mean_q: 0.287311, mean_eps: 0.658814\n",
            "  379441/1750000: episode: 1499, duration: 5.320s, episode steps: 257, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.346 [0.000, 3.000],  loss: 0.001717, mae: 0.222238, mean_q: 0.295937, mean_eps: 0.658619\n",
            "  379688/1750000: episode: 1500, duration: 5.083s, episode steps: 247, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: 0.002370, mae: 0.204903, mean_q: 0.272200, mean_eps: 0.658392\n",
            "  379906/1750000: episode: 1501, duration: 4.490s, episode steps: 218, steps per second:  49, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.001285, mae: 0.206111, mean_q: 0.275019, mean_eps: 0.658184\n",
            "  380103/1750000: episode: 1502, duration: 4.016s, episode steps: 197, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.330 [0.000, 3.000],  loss: 0.002372, mae: 0.219892, mean_q: 0.291633, mean_eps: 0.657996\n",
            "  380507/1750000: episode: 1503, duration: 8.199s, episode steps: 404, steps per second:  49, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.002345, mae: 0.204666, mean_q: 0.271318, mean_eps: 0.657726\n",
            "  380973/1750000: episode: 1504, duration: 9.556s, episode steps: 466, steps per second:  49, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001674, mae: 0.206221, mean_q: 0.274470, mean_eps: 0.657334\n",
            "  381214/1750000: episode: 1505, duration: 4.873s, episode steps: 241, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.876 [0.000, 3.000],  loss: 0.001744, mae: 0.213393, mean_q: 0.285061, mean_eps: 0.657015\n",
            "  381567/1750000: episode: 1506, duration: 7.199s, episode steps: 353, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001687, mae: 0.211235, mean_q: 0.280609, mean_eps: 0.656749\n",
            "  381878/1750000: episode: 1507, duration: 6.431s, episode steps: 311, steps per second:  48, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002104, mae: 0.225597, mean_q: 0.299833, mean_eps: 0.656450\n",
            "  382260/1750000: episode: 1508, duration: 7.993s, episode steps: 382, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001720, mae: 0.206430, mean_q: 0.275609, mean_eps: 0.656139\n",
            "  382529/1750000: episode: 1509, duration: 5.640s, episode steps: 269, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.001651, mae: 0.217832, mean_q: 0.290261, mean_eps: 0.655845\n",
            "  382787/1750000: episode: 1510, duration: 5.267s, episode steps: 258, steps per second:  49, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.837 [0.000, 3.000],  loss: 0.001538, mae: 0.205909, mean_q: 0.275440, mean_eps: 0.655608\n",
            "  383067/1750000: episode: 1511, duration: 5.762s, episode steps: 280, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.032 [0.000, 3.000],  loss: 0.002191, mae: 0.218640, mean_q: 0.291247, mean_eps: 0.655367\n",
            "  383259/1750000: episode: 1512, duration: 3.950s, episode steps: 192, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001596, mae: 0.198893, mean_q: 0.264343, mean_eps: 0.655154\n",
            "  383635/1750000: episode: 1513, duration: 7.743s, episode steps: 376, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002656, mae: 0.208156, mean_q: 0.276220, mean_eps: 0.654899\n",
            "  383834/1750000: episode: 1514, duration: 4.147s, episode steps: 199, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001918, mae: 0.202987, mean_q: 0.271480, mean_eps: 0.654639\n",
            "  384186/1750000: episode: 1515, duration: 7.265s, episode steps: 352, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.264 [0.000, 3.000],  loss: 0.001569, mae: 0.217169, mean_q: 0.288643, mean_eps: 0.654391\n",
            "  384432/1750000: episode: 1516, duration: 5.075s, episode steps: 246, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.001510, mae: 0.206214, mean_q: 0.273921, mean_eps: 0.654123\n",
            "  384642/1750000: episode: 1517, duration: 4.342s, episode steps: 210, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001699, mae: 0.205256, mean_q: 0.273087, mean_eps: 0.653918\n",
            "  385007/1750000: episode: 1518, duration: 7.541s, episode steps: 365, steps per second:  48, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001625, mae: 0.213429, mean_q: 0.283520, mean_eps: 0.653658\n",
            "  385369/1750000: episode: 1519, duration: 7.493s, episode steps: 362, steps per second:  48, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.001892, mae: 0.212661, mean_q: 0.281613, mean_eps: 0.653331\n",
            "  385767/1750000: episode: 1520, duration: 8.245s, episode steps: 398, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002580, mae: 0.207407, mean_q: 0.276821, mean_eps: 0.652989\n",
            "  386073/1750000: episode: 1521, duration: 6.405s, episode steps: 306, steps per second:  48, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002090, mae: 0.221138, mean_q: 0.294059, mean_eps: 0.652672\n",
            "  386306/1750000: episode: 1522, duration: 4.859s, episode steps: 233, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001850, mae: 0.206548, mean_q: 0.276173, mean_eps: 0.652429\n",
            "  386691/1750000: episode: 1523, duration: 7.968s, episode steps: 385, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.810 [0.000, 3.000],  loss: 0.001744, mae: 0.219084, mean_q: 0.294589, mean_eps: 0.652152\n",
            "  387013/1750000: episode: 1524, duration: 6.737s, episode steps: 322, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001886, mae: 0.213847, mean_q: 0.284624, mean_eps: 0.651833\n",
            "  387227/1750000: episode: 1525, duration: 4.446s, episode steps: 214, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.304 [0.000, 3.000],  loss: 0.001316, mae: 0.202666, mean_q: 0.269485, mean_eps: 0.651592\n",
            "  387583/1750000: episode: 1526, duration: 7.390s, episode steps: 356, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001819, mae: 0.202363, mean_q: 0.270046, mean_eps: 0.651336\n",
            "  387768/1750000: episode: 1527, duration: 3.882s, episode steps: 185, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.054 [0.000, 3.000],  loss: 0.001325, mae: 0.215366, mean_q: 0.287169, mean_eps: 0.651093\n",
            "  387956/1750000: episode: 1528, duration: 4.016s, episode steps: 188, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.729 [0.000, 3.000],  loss: 0.002096, mae: 0.199582, mean_q: 0.265669, mean_eps: 0.650926\n",
            "  388198/1750000: episode: 1529, duration: 5.048s, episode steps: 242, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.001212, mae: 0.213264, mean_q: 0.285283, mean_eps: 0.650732\n",
            "  388560/1750000: episode: 1530, duration: 7.493s, episode steps: 362, steps per second:  48, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.001815, mae: 0.208474, mean_q: 0.277939, mean_eps: 0.650460\n",
            "  388786/1750000: episode: 1531, duration: 4.724s, episode steps: 226, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001762, mae: 0.206889, mean_q: 0.275166, mean_eps: 0.650195\n",
            "  389147/1750000: episode: 1532, duration: 7.419s, episode steps: 361, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.319 [0.000, 3.000],  loss: 0.001339, mae: 0.209918, mean_q: 0.279945, mean_eps: 0.649931\n",
            "  389470/1750000: episode: 1533, duration: 6.673s, episode steps: 323, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.805 [0.000, 3.000],  loss: 0.001728, mae: 0.212374, mean_q: 0.283561, mean_eps: 0.649623\n",
            "  389698/1750000: episode: 1534, duration: 4.740s, episode steps: 228, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.001966, mae: 0.225201, mean_q: 0.301285, mean_eps: 0.649374\n",
            "  390078/1750000: episode: 1535, duration: 7.883s, episode steps: 380, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.001758, mae: 0.218937, mean_q: 0.293467, mean_eps: 0.649101\n",
            "  390331/1750000: episode: 1536, duration: 5.262s, episode steps: 253, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001785, mae: 0.212221, mean_q: 0.283035, mean_eps: 0.648816\n",
            "  390521/1750000: episode: 1537, duration: 4.014s, episode steps: 190, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.832 [0.000, 3.000],  loss: 0.002151, mae: 0.207031, mean_q: 0.275053, mean_eps: 0.648617\n",
            "  390729/1750000: episode: 1538, duration: 4.363s, episode steps: 208, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002097, mae: 0.210476, mean_q: 0.279016, mean_eps: 0.648437\n",
            "  391015/1750000: episode: 1539, duration: 5.890s, episode steps: 286, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.001794, mae: 0.209793, mean_q: 0.278474, mean_eps: 0.648215\n",
            "  391197/1750000: episode: 1540, duration: 3.755s, episode steps: 182, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.830 [0.000, 3.000],  loss: 0.002217, mae: 0.210079, mean_q: 0.281507, mean_eps: 0.648005\n",
            "  391368/1750000: episode: 1541, duration: 3.512s, episode steps: 171, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002348, mae: 0.219760, mean_q: 0.292693, mean_eps: 0.647846\n",
            "  391577/1750000: episode: 1542, duration: 4.312s, episode steps: 209, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.230 [0.000, 3.000],  loss: 0.001657, mae: 0.215529, mean_q: 0.286301, mean_eps: 0.647675\n",
            "  391764/1750000: episode: 1543, duration: 3.958s, episode steps: 187, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.877 [0.000, 3.000],  loss: 0.001987, mae: 0.211399, mean_q: 0.282342, mean_eps: 0.647497\n",
            "  392028/1750000: episode: 1544, duration: 5.570s, episode steps: 264, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.001452, mae: 0.210684, mean_q: 0.280499, mean_eps: 0.647295\n",
            "  392207/1750000: episode: 1545, duration: 3.744s, episode steps: 179, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.637 [0.000, 3.000],  loss: 0.001486, mae: 0.213005, mean_q: 0.283451, mean_eps: 0.647096\n",
            "  392432/1750000: episode: 1546, duration: 4.686s, episode steps: 225, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: 0.001392, mae: 0.198680, mean_q: 0.264636, mean_eps: 0.646914\n",
            "  392671/1750000: episode: 1547, duration: 4.948s, episode steps: 239, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.001838, mae: 0.206200, mean_q: 0.275568, mean_eps: 0.646705\n",
            "  393014/1750000: episode: 1548, duration: 7.151s, episode steps: 343, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001781, mae: 0.218636, mean_q: 0.291337, mean_eps: 0.646442\n",
            "  393191/1750000: episode: 1549, duration: 3.641s, episode steps: 177, steps per second:  49, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.847 [0.000, 3.000],  loss: 0.002316, mae: 0.223932, mean_q: 0.297817, mean_eps: 0.646208\n",
            "  393448/1750000: episode: 1550, duration: 5.368s, episode steps: 257, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.002042, mae: 0.211046, mean_q: 0.282761, mean_eps: 0.646014\n",
            "  393750/1750000: episode: 1551, duration: 6.418s, episode steps: 302, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.001868, mae: 0.209906, mean_q: 0.279793, mean_eps: 0.645762\n",
            "  394190/1750000: episode: 1552, duration: 9.215s, episode steps: 440, steps per second:  48, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.001358, mae: 0.209822, mean_q: 0.279938, mean_eps: 0.645427\n",
            "  394666/1750000: episode: 1553, duration: 9.830s, episode steps: 476, steps per second:  48, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001416, mae: 0.216325, mean_q: 0.288215, mean_eps: 0.645015\n",
            "  394971/1750000: episode: 1554, duration: 6.279s, episode steps: 305, steps per second:  49, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002041, mae: 0.212007, mean_q: 0.281542, mean_eps: 0.644664\n",
            "  395280/1750000: episode: 1555, duration: 6.535s, episode steps: 309, steps per second:  47, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.001667, mae: 0.218194, mean_q: 0.290812, mean_eps: 0.644388\n",
            "  395581/1750000: episode: 1556, duration: 6.323s, episode steps: 301, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.001870, mae: 0.205352, mean_q: 0.273459, mean_eps: 0.644113\n",
            "  395921/1750000: episode: 1557, duration: 7.016s, episode steps: 340, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.001758, mae: 0.209332, mean_q: 0.278514, mean_eps: 0.643823\n",
            "  396291/1750000: episode: 1558, duration: 7.610s, episode steps: 370, steps per second:  49, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001897, mae: 0.217938, mean_q: 0.290486, mean_eps: 0.643505\n",
            "  396632/1750000: episode: 1559, duration: 7.111s, episode steps: 341, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.001768, mae: 0.213273, mean_q: 0.285013, mean_eps: 0.643186\n",
            "  397150/1750000: episode: 1560, duration: 10.651s, episode steps: 518, steps per second:  49, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001703, mae: 0.208021, mean_q: 0.277243, mean_eps: 0.642799\n",
            "  397407/1750000: episode: 1561, duration: 5.236s, episode steps: 257, steps per second:  49, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.790 [0.000, 3.000],  loss: 0.002332, mae: 0.215123, mean_q: 0.288154, mean_eps: 0.642450\n",
            "  397627/1750000: episode: 1562, duration: 4.570s, episode steps: 220, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.001671, mae: 0.213976, mean_q: 0.286102, mean_eps: 0.642236\n",
            "  397948/1750000: episode: 1563, duration: 6.740s, episode steps: 321, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001624, mae: 0.212924, mean_q: 0.284069, mean_eps: 0.641993\n",
            "  398262/1750000: episode: 1564, duration: 6.682s, episode steps: 314, steps per second:  47, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001504, mae: 0.220232, mean_q: 0.293827, mean_eps: 0.641706\n",
            "  398681/1750000: episode: 1565, duration: 8.691s, episode steps: 419, steps per second:  48, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.001731, mae: 0.216461, mean_q: 0.290064, mean_eps: 0.641375\n",
            "  399042/1750000: episode: 1566, duration: 7.485s, episode steps: 361, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.002178, mae: 0.216353, mean_q: 0.288847, mean_eps: 0.641024\n",
            "  399393/1750000: episode: 1567, duration: 7.420s, episode steps: 351, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001796, mae: 0.215066, mean_q: 0.286690, mean_eps: 0.640704\n",
            "  399730/1750000: episode: 1568, duration: 7.015s, episode steps: 337, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.001843, mae: 0.216512, mean_q: 0.289292, mean_eps: 0.640394\n",
            "  400022/1750000: episode: 1569, duration: 6.101s, episode steps: 292, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.901 [0.000, 3.000],  loss: 0.001336, mae: 0.209907, mean_q: 0.280951, mean_eps: 0.640112\n",
            "  400284/1750000: episode: 1570, duration: 5.496s, episode steps: 262, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.382 [0.000, 3.000],  loss: 0.001778, mae: 0.231727, mean_q: 0.308370, mean_eps: 0.639863\n",
            "  400623/1750000: episode: 1571, duration: 7.143s, episode steps: 339, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002198, mae: 0.220888, mean_q: 0.294260, mean_eps: 0.639593\n",
            "  400942/1750000: episode: 1572, duration: 6.655s, episode steps: 319, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002342, mae: 0.235408, mean_q: 0.314081, mean_eps: 0.639296\n",
            "  401130/1750000: episode: 1573, duration: 3.938s, episode steps: 188, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.777 [0.000, 3.000],  loss: 0.001578, mae: 0.225365, mean_q: 0.300921, mean_eps: 0.639068\n",
            "  401508/1750000: episode: 1574, duration: 7.921s, episode steps: 378, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001691, mae: 0.222660, mean_q: 0.297262, mean_eps: 0.638814\n",
            "  401786/1750000: episode: 1575, duration: 5.872s, episode steps: 278, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.002323, mae: 0.226035, mean_q: 0.301011, mean_eps: 0.638519\n",
            "  401997/1750000: episode: 1576, duration: 4.440s, episode steps: 211, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.815 [0.000, 3.000],  loss: 0.002255, mae: 0.238376, mean_q: 0.318449, mean_eps: 0.638297\n",
            "  402400/1750000: episode: 1577, duration: 8.449s, episode steps: 403, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001919, mae: 0.222424, mean_q: 0.295273, mean_eps: 0.638022\n",
            "  402627/1750000: episode: 1578, duration: 4.747s, episode steps: 227, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002494, mae: 0.231477, mean_q: 0.310327, mean_eps: 0.637739\n",
            "  402876/1750000: episode: 1579, duration: 5.248s, episode steps: 249, steps per second:  47, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.001764, mae: 0.226625, mean_q: 0.301507, mean_eps: 0.637525\n",
            "  403071/1750000: episode: 1580, duration: 4.152s, episode steps: 195, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.313 [0.000, 3.000],  loss: 0.002087, mae: 0.210070, mean_q: 0.278539, mean_eps: 0.637325\n",
            "  403528/1750000: episode: 1581, duration: 9.432s, episode steps: 457, steps per second:  48, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.001915, mae: 0.213057, mean_q: 0.283734, mean_eps: 0.637032\n",
            "  403888/1750000: episode: 1582, duration: 7.517s, episode steps: 360, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: 0.001825, mae: 0.227423, mean_q: 0.303484, mean_eps: 0.636665\n",
            "  404197/1750000: episode: 1583, duration: 6.462s, episode steps: 309, steps per second:  48, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.252 [0.000, 3.000],  loss: 0.001933, mae: 0.220485, mean_q: 0.295411, mean_eps: 0.636362\n",
            "  404531/1750000: episode: 1584, duration: 6.922s, episode steps: 334, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002575, mae: 0.227327, mean_q: 0.303815, mean_eps: 0.636072\n",
            "  404849/1750000: episode: 1585, duration: 6.698s, episode steps: 318, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001832, mae: 0.237386, mean_q: 0.316901, mean_eps: 0.635779\n",
            "  405075/1750000: episode: 1586, duration: 4.708s, episode steps: 226, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.001861, mae: 0.220633, mean_q: 0.293137, mean_eps: 0.635534\n",
            "  405273/1750000: episode: 1587, duration: 4.217s, episode steps: 198, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.929 [0.000, 3.000],  loss: 0.001916, mae: 0.211202, mean_q: 0.282058, mean_eps: 0.635343\n",
            "  405532/1750000: episode: 1588, duration: 5.409s, episode steps: 259, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.001817, mae: 0.232077, mean_q: 0.309162, mean_eps: 0.635138\n",
            "  405777/1750000: episode: 1589, duration: 5.216s, episode steps: 245, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001769, mae: 0.215010, mean_q: 0.286284, mean_eps: 0.634911\n",
            "  406070/1750000: episode: 1590, duration: 6.202s, episode steps: 293, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002020, mae: 0.222583, mean_q: 0.296732, mean_eps: 0.634668\n",
            "  406372/1750000: episode: 1591, duration: 6.335s, episode steps: 302, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002248, mae: 0.222660, mean_q: 0.297562, mean_eps: 0.634402\n",
            "  406834/1750000: episode: 1592, duration: 9.618s, episode steps: 462, steps per second:  48, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001617, mae: 0.216715, mean_q: 0.290332, mean_eps: 0.634058\n",
            "  407060/1750000: episode: 1593, duration: 4.742s, episode steps: 226, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.001418, mae: 0.224752, mean_q: 0.298970, mean_eps: 0.633749\n",
            "  407381/1750000: episode: 1594, duration: 6.762s, episode steps: 321, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.002225, mae: 0.221275, mean_q: 0.294315, mean_eps: 0.633502\n",
            "  407637/1750000: episode: 1595, duration: 5.333s, episode steps: 256, steps per second:  48, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.001094, mae: 0.218267, mean_q: 0.292210, mean_eps: 0.633241\n",
            "  407808/1750000: episode: 1596, duration: 3.645s, episode steps: 171, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.304 [0.000, 3.000],  loss: 0.001692, mae: 0.231848, mean_q: 0.309245, mean_eps: 0.633050\n",
            "  408215/1750000: episode: 1597, duration: 8.533s, episode steps: 407, steps per second:  48, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.001581, mae: 0.218805, mean_q: 0.291326, mean_eps: 0.632791\n",
            "  408493/1750000: episode: 1598, duration: 5.830s, episode steps: 278, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001679, mae: 0.221886, mean_q: 0.297182, mean_eps: 0.632481\n",
            "  408709/1750000: episode: 1599, duration: 4.515s, episode steps: 216, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.894 [0.000, 3.000],  loss: 0.001942, mae: 0.223883, mean_q: 0.298691, mean_eps: 0.632258\n",
            "  409152/1750000: episode: 1600, duration: 9.270s, episode steps: 443, steps per second:  48, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.002370, mae: 0.231349, mean_q: 0.308271, mean_eps: 0.631963\n",
            "  409507/1750000: episode: 1601, duration: 7.544s, episode steps: 355, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.001817, mae: 0.230235, mean_q: 0.307747, mean_eps: 0.631605\n",
            "  409773/1750000: episode: 1602, duration: 5.671s, episode steps: 266, steps per second:  47, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.003042, mae: 0.228887, mean_q: 0.303250, mean_eps: 0.631324\n",
            "  409967/1750000: episode: 1603, duration: 4.008s, episode steps: 194, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.196 [0.000, 3.000],  loss: 0.001142, mae: 0.214474, mean_q: 0.287364, mean_eps: 0.631117\n",
            "  410297/1750000: episode: 1604, duration: 6.948s, episode steps: 330, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002302, mae: 0.241607, mean_q: 0.321189, mean_eps: 0.630881\n",
            "  410714/1750000: episode: 1605, duration: 8.608s, episode steps: 417, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002763, mae: 0.241071, mean_q: 0.320665, mean_eps: 0.630545\n",
            "  411028/1750000: episode: 1606, duration: 6.582s, episode steps: 314, steps per second:  48, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001810, mae: 0.237101, mean_q: 0.315460, mean_eps: 0.630217\n",
            "  411441/1750000: episode: 1607, duration: 8.688s, episode steps: 413, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: 0.002005, mae: 0.228755, mean_q: 0.304188, mean_eps: 0.629889\n",
            "  411640/1750000: episode: 1608, duration: 4.202s, episode steps: 199, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.794 [0.000, 3.000],  loss: 0.002777, mae: 0.241635, mean_q: 0.323144, mean_eps: 0.629614\n",
            "  412117/1750000: episode: 1609, duration: 10.001s, episode steps: 477, steps per second:  48, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001860, mae: 0.235788, mean_q: 0.314785, mean_eps: 0.629310\n",
            "  412347/1750000: episode: 1610, duration: 4.768s, episode steps: 230, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.001653, mae: 0.244286, mean_q: 0.327110, mean_eps: 0.628991\n",
            "  412833/1750000: episode: 1611, duration: 10.246s, episode steps: 486, steps per second:  47, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002454, mae: 0.241127, mean_q: 0.321129, mean_eps: 0.628669\n",
            "  413011/1750000: episode: 1612, duration: 3.709s, episode steps: 178, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.831 [0.000, 3.000],  loss: 0.002577, mae: 0.248904, mean_q: 0.331254, mean_eps: 0.628370\n",
            "  413363/1750000: episode: 1613, duration: 7.437s, episode steps: 352, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002689, mae: 0.244175, mean_q: 0.324820, mean_eps: 0.628133\n",
            "  413665/1750000: episode: 1614, duration: 6.406s, episode steps: 302, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002599, mae: 0.246832, mean_q: 0.329710, mean_eps: 0.627837\n",
            "  414022/1750000: episode: 1615, duration: 7.517s, episode steps: 357, steps per second:  47, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.002045, mae: 0.236276, mean_q: 0.314755, mean_eps: 0.627540\n",
            "  414370/1750000: episode: 1616, duration: 7.286s, episode steps: 348, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.001484, mae: 0.243355, mean_q: 0.326514, mean_eps: 0.627224\n",
            "  414591/1750000: episode: 1617, duration: 4.617s, episode steps: 221, steps per second:  48, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.002179, mae: 0.239840, mean_q: 0.321832, mean_eps: 0.626968\n",
            "  415121/1750000: episode: 1618, duration: 11.163s, episode steps: 530, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.002015, mae: 0.235652, mean_q: 0.314403, mean_eps: 0.626630\n",
            "  415503/1750000: episode: 1619, duration: 8.024s, episode steps: 382, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002960, mae: 0.241561, mean_q: 0.321465, mean_eps: 0.626219\n",
            "  415730/1750000: episode: 1620, duration: 4.840s, episode steps: 227, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.872 [0.000, 3.000],  loss: 0.002286, mae: 0.239716, mean_q: 0.320982, mean_eps: 0.625946\n",
            "  415956/1750000: episode: 1621, duration: 4.817s, episode steps: 226, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002382, mae: 0.246135, mean_q: 0.328061, mean_eps: 0.625742\n",
            "  416190/1750000: episode: 1622, duration: 4.999s, episode steps: 234, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.002097, mae: 0.246116, mean_q: 0.329471, mean_eps: 0.625535\n",
            "  416409/1750000: episode: 1623, duration: 4.615s, episode steps: 219, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001966, mae: 0.248551, mean_q: 0.331359, mean_eps: 0.625330\n",
            "  416676/1750000: episode: 1624, duration: 5.627s, episode steps: 267, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001491, mae: 0.237936, mean_q: 0.319390, mean_eps: 0.625112\n",
            "  416909/1750000: episode: 1625, duration: 4.912s, episode steps: 233, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002382, mae: 0.246701, mean_q: 0.329673, mean_eps: 0.624887\n",
            "  417133/1750000: episode: 1626, duration: 4.732s, episode steps: 224, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.317 [0.000, 3.000],  loss: 0.002716, mae: 0.242211, mean_q: 0.322188, mean_eps: 0.624680\n",
            "  417312/1750000: episode: 1627, duration: 3.797s, episode steps: 179, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.827 [0.000, 3.000],  loss: 0.001778, mae: 0.232853, mean_q: 0.313237, mean_eps: 0.624500\n",
            "  417603/1750000: episode: 1628, duration: 6.147s, episode steps: 291, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.002272, mae: 0.236291, mean_q: 0.316526, mean_eps: 0.624290\n",
            "  417924/1750000: episode: 1629, duration: 6.812s, episode steps: 321, steps per second:  47, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.001813, mae: 0.227938, mean_q: 0.305303, mean_eps: 0.624014\n",
            "  418236/1750000: episode: 1630, duration: 6.544s, episode steps: 312, steps per second:  48, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001999, mae: 0.235763, mean_q: 0.313937, mean_eps: 0.623730\n",
            "  418531/1750000: episode: 1631, duration: 6.161s, episode steps: 295, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002204, mae: 0.240176, mean_q: 0.322037, mean_eps: 0.623456\n",
            "  418834/1750000: episode: 1632, duration: 6.422s, episode steps: 303, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.002227, mae: 0.235262, mean_q: 0.314475, mean_eps: 0.623186\n",
            "  419074/1750000: episode: 1633, duration: 5.022s, episode steps: 240, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.779 [0.000, 3.000],  loss: 0.002096, mae: 0.241296, mean_q: 0.321939, mean_eps: 0.622941\n",
            "  419251/1750000: episode: 1634, duration: 3.704s, episode steps: 177, steps per second:  48, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.294 [0.000, 3.000],  loss: 0.002121, mae: 0.237961, mean_q: 0.319063, mean_eps: 0.622754\n",
            "  419716/1750000: episode: 1635, duration: 9.855s, episode steps: 465, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.817 [0.000, 3.000],  loss: 0.001866, mae: 0.244462, mean_q: 0.327668, mean_eps: 0.622466\n",
            "  420333/1750000: episode: 1636, duration: 13.032s, episode steps: 617, steps per second:  47, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002293, mae: 0.239290, mean_q: 0.319017, mean_eps: 0.621978\n",
            "  420657/1750000: episode: 1637, duration: 6.804s, episode steps: 324, steps per second:  48, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002081, mae: 0.241940, mean_q: 0.322000, mean_eps: 0.621554\n",
            "  421015/1750000: episode: 1638, duration: 7.534s, episode steps: 358, steps per second:  48, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002577, mae: 0.239035, mean_q: 0.317132, mean_eps: 0.621248\n",
            "  421375/1750000: episode: 1639, duration: 7.668s, episode steps: 360, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001982, mae: 0.243084, mean_q: 0.323648, mean_eps: 0.620925\n",
            "  421842/1750000: episode: 1640, duration: 9.863s, episode steps: 467, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002432, mae: 0.238798, mean_q: 0.317580, mean_eps: 0.620553\n",
            "  422154/1750000: episode: 1641, duration: 6.581s, episode steps: 312, steps per second:  47, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.001776, mae: 0.239223, mean_q: 0.319792, mean_eps: 0.620202\n",
            "  422492/1750000: episode: 1642, duration: 7.127s, episode steps: 338, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.944 [0.000, 3.000],  loss: 0.001852, mae: 0.243747, mean_q: 0.328183, mean_eps: 0.619910\n",
            "  422816/1750000: episode: 1643, duration: 6.844s, episode steps: 324, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001884, mae: 0.244654, mean_q: 0.326503, mean_eps: 0.619613\n",
            "  423039/1750000: episode: 1644, duration: 4.700s, episode steps: 223, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.002608, mae: 0.240611, mean_q: 0.320390, mean_eps: 0.619367\n",
            "  423389/1750000: episode: 1645, duration: 7.407s, episode steps: 350, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.829 [0.000, 3.000],  loss: 0.002771, mae: 0.247494, mean_q: 0.331678, mean_eps: 0.619107\n",
            "  423658/1750000: episode: 1646, duration: 5.594s, episode steps: 269, steps per second:  48, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.833 [0.000, 3.000],  loss: 0.002618, mae: 0.251930, mean_q: 0.336700, mean_eps: 0.618828\n",
            "  424077/1750000: episode: 1647, duration: 8.660s, episode steps: 419, steps per second:  48, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.002441, mae: 0.248745, mean_q: 0.331931, mean_eps: 0.618519\n",
            "  424532/1750000: episode: 1648, duration: 9.520s, episode steps: 455, steps per second:  48, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.002326, mae: 0.252270, mean_q: 0.337750, mean_eps: 0.618126\n",
            "  424853/1750000: episode: 1649, duration: 6.860s, episode steps: 321, steps per second:  47, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002306, mae: 0.250540, mean_q: 0.334516, mean_eps: 0.617777\n",
            "  425119/1750000: episode: 1650, duration: 5.625s, episode steps: 266, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.891 [0.000, 3.000],  loss: 0.001885, mae: 0.253166, mean_q: 0.338769, mean_eps: 0.617513\n",
            "  425392/1750000: episode: 1651, duration: 5.777s, episode steps: 273, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.001939, mae: 0.250761, mean_q: 0.335304, mean_eps: 0.617271\n",
            "  425670/1750000: episode: 1652, duration: 5.905s, episode steps: 278, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.001775, mae: 0.232044, mean_q: 0.310718, mean_eps: 0.617023\n",
            "  426044/1750000: episode: 1653, duration: 7.921s, episode steps: 374, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.001994, mae: 0.248042, mean_q: 0.331506, mean_eps: 0.616730\n",
            "  426445/1750000: episode: 1654, duration: 8.485s, episode steps: 401, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.001889, mae: 0.258953, mean_q: 0.347619, mean_eps: 0.616380\n",
            "  426854/1750000: episode: 1655, duration: 8.670s, episode steps: 409, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002304, mae: 0.254148, mean_q: 0.339807, mean_eps: 0.616015\n",
            "  427249/1750000: episode: 1656, duration: 8.353s, episode steps: 395, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.001908, mae: 0.252024, mean_q: 0.336781, mean_eps: 0.615653\n",
            "  427430/1750000: episode: 1657, duration: 3.826s, episode steps: 181, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.663 [0.000, 3.000],  loss: 0.002060, mae: 0.244240, mean_q: 0.326115, mean_eps: 0.615394\n",
            "  427729/1750000: episode: 1658, duration: 6.361s, episode steps: 299, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.001637, mae: 0.252351, mean_q: 0.337264, mean_eps: 0.615178\n",
            "  428006/1750000: episode: 1659, duration: 5.882s, episode steps: 277, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.001428, mae: 0.242920, mean_q: 0.324607, mean_eps: 0.614919\n",
            "  428326/1750000: episode: 1660, duration: 6.778s, episode steps: 320, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001737, mae: 0.249141, mean_q: 0.335604, mean_eps: 0.614651\n",
            "  428707/1750000: episode: 1661, duration: 8.054s, episode steps: 381, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.002173, mae: 0.244964, mean_q: 0.328690, mean_eps: 0.614336\n",
            "  429127/1750000: episode: 1662, duration: 8.927s, episode steps: 420, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.001731, mae: 0.248353, mean_q: 0.331834, mean_eps: 0.613976\n",
            "  429328/1750000: episode: 1663, duration: 4.290s, episode steps: 201, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001765, mae: 0.240348, mean_q: 0.322076, mean_eps: 0.613697\n",
            "  429696/1750000: episode: 1664, duration: 7.885s, episode steps: 368, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001745, mae: 0.245983, mean_q: 0.329469, mean_eps: 0.613441\n",
            "  429946/1750000: episode: 1665, duration: 5.297s, episode steps: 250, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.232 [0.000, 3.000],  loss: 0.002632, mae: 0.253446, mean_q: 0.337915, mean_eps: 0.613162\n",
            "  430241/1750000: episode: 1666, duration: 6.228s, episode steps: 295, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.002668, mae: 0.247065, mean_q: 0.328352, mean_eps: 0.612915\n",
            "  430413/1750000: episode: 1667, duration: 3.688s, episode steps: 172, steps per second:  47, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002411, mae: 0.253737, mean_q: 0.338817, mean_eps: 0.612705\n",
            "  430669/1750000: episode: 1668, duration: 5.386s, episode steps: 256, steps per second:  48, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002178, mae: 0.243931, mean_q: 0.324913, mean_eps: 0.612512\n",
            "  431117/1750000: episode: 1669, duration: 9.526s, episode steps: 448, steps per second:  47, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002223, mae: 0.246361, mean_q: 0.328909, mean_eps: 0.612195\n",
            "  431444/1750000: episode: 1670, duration: 6.975s, episode steps: 327, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.001794, mae: 0.242090, mean_q: 0.324360, mean_eps: 0.611848\n",
            "  431866/1750000: episode: 1671, duration: 8.916s, episode steps: 422, steps per second:  47, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.002174, mae: 0.242393, mean_q: 0.325080, mean_eps: 0.611511\n",
            "  432240/1750000: episode: 1672, duration: 7.929s, episode steps: 374, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.002050, mae: 0.247610, mean_q: 0.331129, mean_eps: 0.611153\n",
            "  432498/1750000: episode: 1673, duration: 5.572s, episode steps: 258, steps per second:  46, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.973 [0.000, 3.000],  loss: 0.002177, mae: 0.248787, mean_q: 0.334351, mean_eps: 0.610869\n",
            "  432766/1750000: episode: 1674, duration: 5.738s, episode steps: 268, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.836 [0.000, 3.000],  loss: 0.002183, mae: 0.249639, mean_q: 0.335908, mean_eps: 0.610631\n",
            "  433182/1750000: episode: 1675, duration: 8.854s, episode steps: 416, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002153, mae: 0.251243, mean_q: 0.338029, mean_eps: 0.610323\n",
            "  433423/1750000: episode: 1676, duration: 5.110s, episode steps: 241, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.001852, mae: 0.253672, mean_q: 0.341929, mean_eps: 0.610028\n",
            "  433687/1750000: episode: 1677, duration: 5.625s, episode steps: 264, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002470, mae: 0.250449, mean_q: 0.335491, mean_eps: 0.609801\n",
            "  434065/1750000: episode: 1678, duration: 8.008s, episode steps: 378, steps per second:  47, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002123, mae: 0.251736, mean_q: 0.337592, mean_eps: 0.609512\n",
            "  434491/1750000: episode: 1679, duration: 9.053s, episode steps: 426, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.001963, mae: 0.251268, mean_q: 0.337168, mean_eps: 0.609150\n",
            "  434743/1750000: episode: 1680, duration: 5.350s, episode steps: 252, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001569, mae: 0.246412, mean_q: 0.330981, mean_eps: 0.608846\n",
            "  435075/1750000: episode: 1681, duration: 6.992s, episode steps: 332, steps per second:  47, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002081, mae: 0.238406, mean_q: 0.319059, mean_eps: 0.608583\n",
            "  435457/1750000: episode: 1682, duration: 8.132s, episode steps: 382, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.001838, mae: 0.247654, mean_q: 0.333244, mean_eps: 0.608261\n",
            "  436004/1750000: episode: 1683, duration: 11.656s, episode steps: 547, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002094, mae: 0.247500, mean_q: 0.332433, mean_eps: 0.607843\n",
            "  436366/1750000: episode: 1684, duration: 7.688s, episode steps: 362, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001778, mae: 0.243522, mean_q: 0.325818, mean_eps: 0.607434\n",
            "  436745/1750000: episode: 1685, duration: 8.127s, episode steps: 379, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002137, mae: 0.247321, mean_q: 0.332306, mean_eps: 0.607100\n",
            "  437357/1750000: episode: 1686, duration: 12.884s, episode steps: 612, steps per second:  48, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001920, mae: 0.247839, mean_q: 0.332849, mean_eps: 0.606653\n",
            "  437698/1750000: episode: 1687, duration: 7.273s, episode steps: 341, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.002632, mae: 0.252428, mean_q: 0.338361, mean_eps: 0.606225\n",
            "  438018/1750000: episode: 1688, duration: 6.768s, episode steps: 320, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.001940, mae: 0.241705, mean_q: 0.325718, mean_eps: 0.605928\n",
            "  438420/1750000: episode: 1689, duration: 8.534s, episode steps: 402, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.001882, mae: 0.250243, mean_q: 0.335240, mean_eps: 0.605604\n",
            "  438810/1750000: episode: 1690, duration: 8.328s, episode steps: 390, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002013, mae: 0.243448, mean_q: 0.326478, mean_eps: 0.605247\n",
            "  439101/1750000: episode: 1691, duration: 6.281s, episode steps: 291, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001831, mae: 0.240775, mean_q: 0.323449, mean_eps: 0.604940\n",
            "  439627/1750000: episode: 1692, duration: 11.155s, episode steps: 526, steps per second:  47, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001859, mae: 0.245232, mean_q: 0.328387, mean_eps: 0.604572\n",
            "  439875/1750000: episode: 1693, duration: 5.263s, episode steps: 248, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001709, mae: 0.258195, mean_q: 0.347206, mean_eps: 0.604225\n",
            "  440247/1750000: episode: 1694, duration: 7.932s, episode steps: 372, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002860, mae: 0.250295, mean_q: 0.333343, mean_eps: 0.603946\n",
            "  440730/1750000: episode: 1695, duration: 10.381s, episode steps: 483, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002023, mae: 0.251664, mean_q: 0.336726, mean_eps: 0.603561\n",
            "  441214/1750000: episode: 1696, duration: 10.248s, episode steps: 484, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001701, mae: 0.249727, mean_q: 0.334770, mean_eps: 0.603125\n",
            "  441657/1750000: episode: 1697, duration: 9.388s, episode steps: 443, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.862 [0.000, 3.000],  loss: 0.002157, mae: 0.270303, mean_q: 0.361879, mean_eps: 0.602708\n",
            "  442116/1750000: episode: 1698, duration: 9.863s, episode steps: 459, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002814, mae: 0.255229, mean_q: 0.343601, mean_eps: 0.602303\n",
            "  442343/1750000: episode: 1699, duration: 4.827s, episode steps: 227, steps per second:  47, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.002007, mae: 0.247398, mean_q: 0.329954, mean_eps: 0.601995\n",
            "  442669/1750000: episode: 1700, duration: 6.966s, episode steps: 326, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.002086, mae: 0.255302, mean_q: 0.341571, mean_eps: 0.601745\n",
            "  443145/1750000: episode: 1701, duration: 10.126s, episode steps: 476, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.779 [0.000, 3.000],  loss: 0.001755, mae: 0.245641, mean_q: 0.331058, mean_eps: 0.601383\n",
            "  443439/1750000: episode: 1702, duration: 6.236s, episode steps: 294, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001856, mae: 0.253200, mean_q: 0.341187, mean_eps: 0.601037\n",
            "  443708/1750000: episode: 1703, duration: 5.759s, episode steps: 269, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.833 [0.000, 3.000],  loss: 0.001760, mae: 0.257181, mean_q: 0.348116, mean_eps: 0.600785\n",
            "  444004/1750000: episode: 1704, duration: 6.443s, episode steps: 296, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.001996, mae: 0.245456, mean_q: 0.329626, mean_eps: 0.600531\n",
            "  444224/1750000: episode: 1705, duration: 4.853s, episode steps: 220, steps per second:  45, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002070, mae: 0.256694, mean_q: 0.345087, mean_eps: 0.600299\n",
            "  444457/1750000: episode: 1706, duration: 5.070s, episode steps: 233, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.219 [0.000, 3.000],  loss: 0.002731, mae: 0.257196, mean_q: 0.344271, mean_eps: 0.600094\n",
            "  444824/1750000: episode: 1707, duration: 7.919s, episode steps: 367, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.801 [0.000, 3.000],  loss: 0.002213, mae: 0.263615, mean_q: 0.354727, mean_eps: 0.599824\n",
            "  445114/1750000: episode: 1708, duration: 6.287s, episode steps: 290, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002575, mae: 0.260146, mean_q: 0.351236, mean_eps: 0.599529\n",
            "  445597/1750000: episode: 1709, duration: 10.307s, episode steps: 483, steps per second:  47, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.001730, mae: 0.247287, mean_q: 0.331884, mean_eps: 0.599180\n",
            "  445919/1750000: episode: 1710, duration: 6.842s, episode steps: 322, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001777, mae: 0.254011, mean_q: 0.342137, mean_eps: 0.598818\n",
            "  446375/1750000: episode: 1711, duration: 9.741s, episode steps: 456, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.002104, mae: 0.262059, mean_q: 0.353854, mean_eps: 0.598469\n",
            "  446906/1750000: episode: 1712, duration: 11.415s, episode steps: 531, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002308, mae: 0.248006, mean_q: 0.332792, mean_eps: 0.598024\n",
            "  447126/1750000: episode: 1713, duration: 4.716s, episode steps: 220, steps per second:  47, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002268, mae: 0.256238, mean_q: 0.344705, mean_eps: 0.597686\n",
            "  447420/1750000: episode: 1714, duration: 6.322s, episode steps: 294, steps per second:  47, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001834, mae: 0.247968, mean_q: 0.332920, mean_eps: 0.597455\n",
            "  447753/1750000: episode: 1715, duration: 7.252s, episode steps: 333, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002282, mae: 0.249308, mean_q: 0.335729, mean_eps: 0.597173\n",
            "  448134/1750000: episode: 1716, duration: 8.131s, episode steps: 381, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002093, mae: 0.252766, mean_q: 0.339687, mean_eps: 0.596850\n",
            "  448585/1750000: episode: 1717, duration: 9.706s, episode steps: 451, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001811, mae: 0.250223, mean_q: 0.335712, mean_eps: 0.596476\n",
            "  449001/1750000: episode: 1718, duration: 8.875s, episode steps: 416, steps per second:  47, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001956, mae: 0.256530, mean_q: 0.345160, mean_eps: 0.596085\n",
            "  449520/1750000: episode: 1719, duration: 11.101s, episode steps: 519, steps per second:  47, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.002003, mae: 0.249130, mean_q: 0.335839, mean_eps: 0.595666\n",
            "  449838/1750000: episode: 1720, duration: 6.927s, episode steps: 318, steps per second:  46, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001571, mae: 0.249084, mean_q: 0.333809, mean_eps: 0.595290\n",
            "  450202/1750000: episode: 1721, duration: 7.737s, episode steps: 364, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002346, mae: 0.255552, mean_q: 0.343050, mean_eps: 0.594982\n",
            "  450476/1750000: episode: 1722, duration: 5.887s, episode steps: 274, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.350 [0.000, 3.000],  loss: 0.002048, mae: 0.260399, mean_q: 0.349074, mean_eps: 0.594696\n",
            "  450796/1750000: episode: 1723, duration: 6.812s, episode steps: 320, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002214, mae: 0.266940, mean_q: 0.359177, mean_eps: 0.594429\n",
            "  451096/1750000: episode: 1724, duration: 6.428s, episode steps: 300, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.002257, mae: 0.261322, mean_q: 0.350841, mean_eps: 0.594150\n",
            "  451298/1750000: episode: 1725, duration: 4.421s, episode steps: 202, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.847 [0.000, 3.000],  loss: 0.001890, mae: 0.256140, mean_q: 0.344647, mean_eps: 0.593924\n",
            "  452039/1750000: episode: 1726, duration: 15.921s, episode steps: 741, steps per second:  47, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002521, mae: 0.264895, mean_q: 0.355463, mean_eps: 0.593499\n",
            "  452337/1750000: episode: 1727, duration: 6.497s, episode steps: 298, steps per second:  46, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.826 [0.000, 3.000],  loss: 0.001869, mae: 0.266033, mean_q: 0.359149, mean_eps: 0.593031\n",
            "  452879/1750000: episode: 1728, duration: 11.602s, episode steps: 542, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.001924, mae: 0.260711, mean_q: 0.350806, mean_eps: 0.592653\n",
            "  453220/1750000: episode: 1729, duration: 7.404s, episode steps: 341, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.001752, mae: 0.263626, mean_q: 0.354012, mean_eps: 0.592257\n",
            "  453597/1750000: episode: 1730, duration: 8.208s, episode steps: 377, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.002110, mae: 0.262902, mean_q: 0.351504, mean_eps: 0.591933\n",
            "  453986/1750000: episode: 1731, duration: 8.334s, episode steps: 389, steps per second:  47, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.002116, mae: 0.259097, mean_q: 0.348473, mean_eps: 0.591587\n",
            "  454291/1750000: episode: 1732, duration: 6.496s, episode steps: 305, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.001868, mae: 0.258310, mean_q: 0.346326, mean_eps: 0.591276\n",
            "  454737/1750000: episode: 1733, duration: 9.687s, episode steps: 446, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.002003, mae: 0.266725, mean_q: 0.358582, mean_eps: 0.590937\n",
            "  455110/1750000: episode: 1734, duration: 8.023s, episode steps: 373, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002153, mae: 0.265610, mean_q: 0.357034, mean_eps: 0.590568\n",
            "  455484/1750000: episode: 1735, duration: 8.141s, episode steps: 374, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.002271, mae: 0.266771, mean_q: 0.359043, mean_eps: 0.590234\n",
            "  456061/1750000: episode: 1736, duration: 12.461s, episode steps: 577, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001731, mae: 0.257000, mean_q: 0.345338, mean_eps: 0.589805\n",
            "  456229/1750000: episode: 1737, duration: 3.637s, episode steps: 168, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.696 [0.000, 3.000],  loss: 0.001639, mae: 0.256897, mean_q: 0.345982, mean_eps: 0.589469\n",
            "  456804/1750000: episode: 1738, duration: 12.360s, episode steps: 575, steps per second:  47, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002131, mae: 0.258897, mean_q: 0.350318, mean_eps: 0.589136\n",
            "  457265/1750000: episode: 1739, duration: 9.940s, episode steps: 461, steps per second:  46, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.809 [0.000, 3.000],  loss: 0.002113, mae: 0.267305, mean_q: 0.362159, mean_eps: 0.588669\n",
            "  457719/1750000: episode: 1740, duration: 9.589s, episode steps: 454, steps per second:  47, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002111, mae: 0.254222, mean_q: 0.342294, mean_eps: 0.588257\n",
            "  458138/1750000: episode: 1741, duration: 8.886s, episode steps: 419, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002096, mae: 0.259168, mean_q: 0.347720, mean_eps: 0.587865\n",
            "  458548/1750000: episode: 1742, duration: 8.830s, episode steps: 410, steps per second:  46, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.793 [0.000, 3.000],  loss: 0.001959, mae: 0.253766, mean_q: 0.342363, mean_eps: 0.587492\n",
            "  458999/1750000: episode: 1743, duration: 9.813s, episode steps: 451, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001816, mae: 0.253196, mean_q: 0.342204, mean_eps: 0.587105\n",
            "  459289/1750000: episode: 1744, duration: 6.404s, episode steps: 290, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001550, mae: 0.262158, mean_q: 0.353513, mean_eps: 0.586770\n",
            "  459701/1750000: episode: 1745, duration: 8.896s, episode steps: 412, steps per second:  46, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002421, mae: 0.262913, mean_q: 0.354966, mean_eps: 0.586454\n",
            "  460163/1750000: episode: 1746, duration: 9.921s, episode steps: 462, steps per second:  47, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002197, mae: 0.261539, mean_q: 0.352775, mean_eps: 0.586061\n",
            "  460469/1750000: episode: 1747, duration: 6.638s, episode steps: 306, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002234, mae: 0.285213, mean_q: 0.385584, mean_eps: 0.585716\n",
            "  460869/1750000: episode: 1748, duration: 8.650s, episode steps: 400, steps per second:  46, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.001942, mae: 0.283619, mean_q: 0.383831, mean_eps: 0.585397\n",
            "  461138/1750000: episode: 1749, duration: 5.766s, episode steps: 269, steps per second:  47, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.967 [0.000, 3.000],  loss: 0.001745, mae: 0.277443, mean_q: 0.375289, mean_eps: 0.585096\n",
            "  461574/1750000: episode: 1750, duration: 9.433s, episode steps: 436, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002224, mae: 0.280818, mean_q: 0.377774, mean_eps: 0.584780\n",
            "  461948/1750000: episode: 1751, duration: 8.109s, episode steps: 374, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001984, mae: 0.285662, mean_q: 0.384484, mean_eps: 0.584416\n",
            "  462267/1750000: episode: 1752, duration: 6.930s, episode steps: 319, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002175, mae: 0.285880, mean_q: 0.385551, mean_eps: 0.584105\n",
            "  462852/1750000: episode: 1753, duration: 12.829s, episode steps: 585, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002163, mae: 0.282218, mean_q: 0.382231, mean_eps: 0.583698\n",
            "  463214/1750000: episode: 1754, duration: 7.920s, episode steps: 362, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.001931, mae: 0.276718, mean_q: 0.372458, mean_eps: 0.583271\n",
            "  463688/1750000: episode: 1755, duration: 10.319s, episode steps: 474, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001857, mae: 0.277520, mean_q: 0.374397, mean_eps: 0.582895\n",
            "  464391/1750000: episode: 1756, duration: 15.078s, episode steps: 703, steps per second:  47, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001850, mae: 0.281247, mean_q: 0.379794, mean_eps: 0.582366\n",
            "  464757/1750000: episode: 1757, duration: 7.922s, episode steps: 366, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002022, mae: 0.291718, mean_q: 0.394095, mean_eps: 0.581883\n",
            "  465311/1750000: episode: 1758, duration: 11.980s, episode steps: 554, steps per second:  46, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002255, mae: 0.290518, mean_q: 0.390860, mean_eps: 0.581469\n",
            "  465720/1750000: episode: 1759, duration: 8.912s, episode steps: 409, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.002443, mae: 0.291625, mean_q: 0.393288, mean_eps: 0.581037\n",
            "  466030/1750000: episode: 1760, duration: 6.730s, episode steps: 310, steps per second:  46, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.829 [0.000, 3.000],  loss: 0.002219, mae: 0.281762, mean_q: 0.382057, mean_eps: 0.580713\n",
            "  466283/1750000: episode: 1761, duration: 5.475s, episode steps: 253, steps per second:  46, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001882, mae: 0.273695, mean_q: 0.369942, mean_eps: 0.580460\n",
            "  466712/1750000: episode: 1762, duration: 9.461s, episode steps: 429, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.783 [0.000, 3.000],  loss: 0.001902, mae: 0.282126, mean_q: 0.380505, mean_eps: 0.580154\n",
            "  467026/1750000: episode: 1763, duration: 6.816s, episode steps: 314, steps per second:  46, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.879 [0.000, 3.000],  loss: 0.002100, mae: 0.289758, mean_q: 0.391103, mean_eps: 0.579819\n",
            "  467232/1750000: episode: 1764, duration: 4.531s, episode steps: 206, steps per second:  45, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002507, mae: 0.286459, mean_q: 0.385560, mean_eps: 0.579585\n",
            "  467619/1750000: episode: 1765, duration: 8.455s, episode steps: 387, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001992, mae: 0.286805, mean_q: 0.386733, mean_eps: 0.579318\n",
            "  467983/1750000: episode: 1766, duration: 7.872s, episode steps: 364, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.001408, mae: 0.287877, mean_q: 0.388132, mean_eps: 0.578980\n",
            "  468310/1750000: episode: 1767, duration: 7.157s, episode steps: 327, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001706, mae: 0.289438, mean_q: 0.390010, mean_eps: 0.578669\n",
            "  468647/1750000: episode: 1768, duration: 7.261s, episode steps: 337, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.001891, mae: 0.283012, mean_q: 0.383011, mean_eps: 0.578370\n",
            "  469015/1750000: episode: 1769, duration: 7.976s, episode steps: 368, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.826 [0.000, 3.000],  loss: 0.001868, mae: 0.282853, mean_q: 0.381692, mean_eps: 0.578053\n",
            "  469378/1750000: episode: 1770, duration: 7.911s, episode steps: 363, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.868 [0.000, 3.000],  loss: 0.002032, mae: 0.286066, mean_q: 0.387079, mean_eps: 0.577724\n",
            "  469785/1750000: episode: 1771, duration: 8.890s, episode steps: 407, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.801 [0.000, 3.000],  loss: 0.001602, mae: 0.272223, mean_q: 0.368473, mean_eps: 0.577376\n",
            "  470221/1750000: episode: 1772, duration: 9.561s, episode steps: 436, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002250, mae: 0.285992, mean_q: 0.385810, mean_eps: 0.576996\n",
            "  470651/1750000: episode: 1773, duration: 9.334s, episode steps: 430, steps per second:  46, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.002109, mae: 0.291784, mean_q: 0.394587, mean_eps: 0.576608\n",
            "  470881/1750000: episode: 1774, duration: 5.018s, episode steps: 230, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.002051, mae: 0.296148, mean_q: 0.399984, mean_eps: 0.576311\n",
            "  471245/1750000: episode: 1775, duration: 7.923s, episode steps: 364, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001823, mae: 0.301820, mean_q: 0.408626, mean_eps: 0.576042\n",
            "  471712/1750000: episode: 1776, duration: 9.956s, episode steps: 467, steps per second:  47, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.001917, mae: 0.297952, mean_q: 0.402297, mean_eps: 0.575670\n",
            "  472034/1750000: episode: 1777, duration: 6.897s, episode steps: 322, steps per second:  47, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.002055, mae: 0.294257, mean_q: 0.395983, mean_eps: 0.575315\n",
            "  472573/1750000: episode: 1778, duration: 11.568s, episode steps: 539, steps per second:  47, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.002075, mae: 0.303317, mean_q: 0.410096, mean_eps: 0.574926\n",
            "  473255/1750000: episode: 1779, duration: 14.822s, episode steps: 682, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.001670, mae: 0.295671, mean_q: 0.399064, mean_eps: 0.574377\n",
            "  473896/1750000: episode: 1780, duration: 14.144s, episode steps: 641, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.001901, mae: 0.302783, mean_q: 0.408888, mean_eps: 0.573783\n",
            "  474261/1750000: episode: 1781, duration: 8.125s, episode steps: 365, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001740, mae: 0.301376, mean_q: 0.408205, mean_eps: 0.573330\n",
            "  474614/1750000: episode: 1782, duration: 7.636s, episode steps: 353, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002000, mae: 0.294762, mean_q: 0.397710, mean_eps: 0.573006\n",
            "  474890/1750000: episode: 1783, duration: 6.009s, episode steps: 276, steps per second:  46, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002181, mae: 0.305374, mean_q: 0.411941, mean_eps: 0.572723\n",
            "  475301/1750000: episode: 1784, duration: 8.918s, episode steps: 411, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.001834, mae: 0.294107, mean_q: 0.398114, mean_eps: 0.572414\n",
            "  475605/1750000: episode: 1785, duration: 6.669s, episode steps: 304, steps per second:  46, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002028, mae: 0.292116, mean_q: 0.393431, mean_eps: 0.572091\n",
            "  476071/1750000: episode: 1786, duration: 10.129s, episode steps: 466, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001875, mae: 0.296156, mean_q: 0.400804, mean_eps: 0.571746\n",
            "  476469/1750000: episode: 1787, duration: 8.657s, episode steps: 398, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001596, mae: 0.284970, mean_q: 0.386871, mean_eps: 0.571357\n",
            "  476865/1750000: episode: 1788, duration: 8.619s, episode steps: 396, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001832, mae: 0.302747, mean_q: 0.409069, mean_eps: 0.570999\n",
            "  477185/1750000: episode: 1789, duration: 6.999s, episode steps: 320, steps per second:  46, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002032, mae: 0.291632, mean_q: 0.392114, mean_eps: 0.570677\n",
            "  477650/1750000: episode: 1790, duration: 10.156s, episode steps: 465, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001933, mae: 0.299247, mean_q: 0.403537, mean_eps: 0.570324\n",
            "  477956/1750000: episode: 1791, duration: 6.789s, episode steps: 306, steps per second:  45, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002499, mae: 0.308696, mean_q: 0.418756, mean_eps: 0.569978\n",
            "  478274/1750000: episode: 1792, duration: 6.994s, episode steps: 318, steps per second:  45, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002046, mae: 0.301703, mean_q: 0.409045, mean_eps: 0.569697\n",
            "  478694/1750000: episode: 1793, duration: 9.250s, episode steps: 420, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001953, mae: 0.293157, mean_q: 0.395497, mean_eps: 0.569364\n",
            "  479402/1750000: episode: 1794, duration: 15.310s, episode steps: 708, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.801 [0.000, 3.000],  loss: 0.001992, mae: 0.289500, mean_q: 0.393243, mean_eps: 0.568857\n",
            "  479632/1750000: episode: 1795, duration: 4.962s, episode steps: 230, steps per second:  46, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002219, mae: 0.297607, mean_q: 0.403948, mean_eps: 0.568436\n",
            "  480167/1750000: episode: 1796, duration: 11.598s, episode steps: 535, steps per second:  46, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.001856, mae: 0.305899, mean_q: 0.412417, mean_eps: 0.568092\n",
            "  480630/1750000: episode: 1797, duration: 10.191s, episode steps: 463, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002442, mae: 0.316792, mean_q: 0.426118, mean_eps: 0.567642\n",
            "  481097/1750000: episode: 1798, duration: 10.300s, episode steps: 467, steps per second:  45, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002248, mae: 0.311858, mean_q: 0.421353, mean_eps: 0.567222\n",
            "  481485/1750000: episode: 1799, duration: 8.610s, episode steps: 388, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002133, mae: 0.315912, mean_q: 0.426298, mean_eps: 0.566837\n",
            "  481975/1750000: episode: 1800, duration: 10.717s, episode steps: 490, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002059, mae: 0.317597, mean_q: 0.428708, mean_eps: 0.566443\n",
            "  482537/1750000: episode: 1801, duration: 12.307s, episode steps: 562, steps per second:  46, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002081, mae: 0.324273, mean_q: 0.438872, mean_eps: 0.565970\n",
            "  482993/1750000: episode: 1802, duration: 9.989s, episode steps: 456, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002356, mae: 0.311560, mean_q: 0.421268, mean_eps: 0.565511\n",
            "  483454/1750000: episode: 1803, duration: 10.097s, episode steps: 461, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002163, mae: 0.308585, mean_q: 0.418402, mean_eps: 0.565098\n",
            "  483889/1750000: episode: 1804, duration: 9.558s, episode steps: 435, steps per second:  46, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002105, mae: 0.314626, mean_q: 0.427060, mean_eps: 0.564695\n",
            "  484267/1750000: episode: 1805, duration: 8.212s, episode steps: 378, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002247, mae: 0.307797, mean_q: 0.417902, mean_eps: 0.564330\n",
            "  484769/1750000: episode: 1806, duration: 11.027s, episode steps: 502, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.809 [0.000, 3.000],  loss: 0.002167, mae: 0.311948, mean_q: 0.423478, mean_eps: 0.563934\n",
            "  485378/1750000: episode: 1807, duration: 13.413s, episode steps: 609, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001893, mae: 0.312426, mean_q: 0.422714, mean_eps: 0.563433\n",
            "  485975/1750000: episode: 1808, duration: 13.048s, episode steps: 597, steps per second:  46, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002191, mae: 0.320922, mean_q: 0.433756, mean_eps: 0.562892\n",
            "  486398/1750000: episode: 1809, duration: 9.330s, episode steps: 423, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002580, mae: 0.319126, mean_q: 0.432533, mean_eps: 0.562433\n",
            "  486803/1750000: episode: 1810, duration: 8.818s, episode steps: 405, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001596, mae: 0.319794, mean_q: 0.433215, mean_eps: 0.562060\n",
            "  487352/1750000: episode: 1811, duration: 12.042s, episode steps: 549, steps per second:  46, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.001791, mae: 0.316391, mean_q: 0.428165, mean_eps: 0.561632\n",
            "  487862/1750000: episode: 1812, duration: 11.287s, episode steps: 510, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001875, mae: 0.317761, mean_q: 0.432703, mean_eps: 0.561155\n",
            "  488413/1750000: episode: 1813, duration: 12.015s, episode steps: 551, steps per second:  46, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.849 [0.000, 3.000],  loss: 0.002035, mae: 0.317893, mean_q: 0.432802, mean_eps: 0.560676\n",
            "  488774/1750000: episode: 1814, duration: 7.988s, episode steps: 361, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.900 [0.000, 3.000],  loss: 0.001729, mae: 0.320501, mean_q: 0.436257, mean_eps: 0.560265\n",
            "  489189/1750000: episode: 1815, duration: 9.243s, episode steps: 415, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.793 [0.000, 3.000],  loss: 0.002217, mae: 0.319094, mean_q: 0.433279, mean_eps: 0.559916\n",
            "  489542/1750000: episode: 1816, duration: 7.780s, episode steps: 353, steps per second:  45, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001560, mae: 0.316907, mean_q: 0.430338, mean_eps: 0.559571\n",
            "  489833/1750000: episode: 1817, duration: 6.418s, episode steps: 291, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.893 [0.000, 3.000],  loss: 0.001737, mae: 0.315061, mean_q: 0.427922, mean_eps: 0.559281\n",
            "  490465/1750000: episode: 1818, duration: 13.808s, episode steps: 632, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002608, mae: 0.326225, mean_q: 0.441042, mean_eps: 0.558865\n",
            "  490813/1750000: episode: 1819, duration: 7.652s, episode steps: 348, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.002357, mae: 0.329560, mean_q: 0.444292, mean_eps: 0.558424\n",
            "  491275/1750000: episode: 1820, duration: 10.141s, episode steps: 462, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002383, mae: 0.323555, mean_q: 0.438020, mean_eps: 0.558060\n",
            "  491808/1750000: episode: 1821, duration: 11.705s, episode steps: 533, steps per second:  46, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002321, mae: 0.331916, mean_q: 0.449690, mean_eps: 0.557614\n",
            "  492228/1750000: episode: 1822, duration: 9.305s, episode steps: 420, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002718, mae: 0.325577, mean_q: 0.440437, mean_eps: 0.557186\n",
            "  492500/1750000: episode: 1823, duration: 6.126s, episode steps: 272, steps per second:  44, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002237, mae: 0.327098, mean_q: 0.442083, mean_eps: 0.556874\n",
            "  492899/1750000: episode: 1824, duration: 8.791s, episode steps: 399, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002717, mae: 0.325430, mean_q: 0.439877, mean_eps: 0.556572\n",
            "  493204/1750000: episode: 1825, duration: 6.825s, episode steps: 305, steps per second:  45, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002378, mae: 0.327732, mean_q: 0.443967, mean_eps: 0.556255\n",
            "  493471/1750000: episode: 1826, duration: 5.980s, episode steps: 267, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002241, mae: 0.321592, mean_q: 0.436182, mean_eps: 0.555998\n",
            "  493781/1750000: episode: 1827, duration: 6.938s, episode steps: 310, steps per second:  45, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002786, mae: 0.341516, mean_q: 0.462079, mean_eps: 0.555737\n",
            "  494604/1750000: episode: 1828, duration: 18.109s, episode steps: 823, steps per second:  45, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002407, mae: 0.328862, mean_q: 0.445562, mean_eps: 0.555227\n",
            "  495000/1750000: episode: 1829, duration: 8.774s, episode steps: 396, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002306, mae: 0.325150, mean_q: 0.440040, mean_eps: 0.554680\n",
            "  495371/1750000: episode: 1830, duration: 8.262s, episode steps: 371, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002117, mae: 0.330921, mean_q: 0.449135, mean_eps: 0.554334\n",
            "  495708/1750000: episode: 1831, duration: 7.386s, episode steps: 337, steps per second:  46, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001941, mae: 0.322563, mean_q: 0.437557, mean_eps: 0.554016\n",
            "  496194/1750000: episode: 1832, duration: 10.669s, episode steps: 486, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002220, mae: 0.332491, mean_q: 0.450504, mean_eps: 0.553645\n",
            "  496488/1750000: episode: 1833, duration: 6.480s, episode steps: 294, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.002407, mae: 0.337255, mean_q: 0.458079, mean_eps: 0.553294\n",
            "  496961/1750000: episode: 1834, duration: 10.505s, episode steps: 473, steps per second:  45, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002071, mae: 0.326730, mean_q: 0.442888, mean_eps: 0.552948\n",
            "  497390/1750000: episode: 1835, duration: 9.517s, episode steps: 429, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.002160, mae: 0.329987, mean_q: 0.448642, mean_eps: 0.552542\n",
            "  497944/1750000: episode: 1836, duration: 12.215s, episode steps: 554, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002075, mae: 0.326863, mean_q: 0.443945, mean_eps: 0.552101\n",
            "  498325/1750000: episode: 1837, duration: 8.486s, episode steps: 381, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002427, mae: 0.334570, mean_q: 0.454492, mean_eps: 0.551679\n",
            "  498812/1750000: episode: 1838, duration: 10.796s, episode steps: 487, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002332, mae: 0.331584, mean_q: 0.448764, mean_eps: 0.551289\n",
            "  499344/1750000: episode: 1839, duration: 11.840s, episode steps: 532, steps per second:  45, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.002244, mae: 0.327088, mean_q: 0.443862, mean_eps: 0.550832\n",
            "  499789/1750000: episode: 1840, duration: 10.039s, episode steps: 445, steps per second:  44, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.002159, mae: 0.322308, mean_q: 0.437781, mean_eps: 0.550391\n",
            "  500156/1750000: episode: 1841, duration: 8.163s, episode steps: 367, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002277, mae: 0.339403, mean_q: 0.459333, mean_eps: 0.550025\n",
            "  500839/1750000: episode: 1842, duration: 15.075s, episode steps: 683, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.002459, mae: 0.346138, mean_q: 0.469281, mean_eps: 0.549554\n",
            "  501272/1750000: episode: 1843, duration: 9.670s, episode steps: 433, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002769, mae: 0.350392, mean_q: 0.475742, mean_eps: 0.549051\n",
            "  501609/1750000: episode: 1844, duration: 7.599s, episode steps: 337, steps per second:  44, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002642, mae: 0.347863, mean_q: 0.472542, mean_eps: 0.548704\n",
            "  502312/1750000: episode: 1845, duration: 15.457s, episode steps: 703, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002755, mae: 0.347282, mean_q: 0.469884, mean_eps: 0.548236\n",
            "  502678/1750000: episode: 1846, duration: 8.205s, episode steps: 366, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002391, mae: 0.348281, mean_q: 0.473211, mean_eps: 0.547755\n",
            "  503364/1750000: episode: 1847, duration: 15.356s, episode steps: 686, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002436, mae: 0.348873, mean_q: 0.474345, mean_eps: 0.547282\n",
            "  503743/1750000: episode: 1848, duration: 8.445s, episode steps: 379, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.002657, mae: 0.349174, mean_q: 0.474699, mean_eps: 0.546803\n",
            "  504114/1750000: episode: 1849, duration: 8.282s, episode steps: 371, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002430, mae: 0.344568, mean_q: 0.468600, mean_eps: 0.546465\n",
            "  504580/1750000: episode: 1850, duration: 10.396s, episode steps: 466, steps per second:  45, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002151, mae: 0.344901, mean_q: 0.467689, mean_eps: 0.546089\n",
            "  504973/1750000: episode: 1851, duration: 8.742s, episode steps: 393, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002404, mae: 0.351499, mean_q: 0.477767, mean_eps: 0.545702\n",
            "  505294/1750000: episode: 1852, duration: 7.119s, episode steps: 321, steps per second:  45, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002813, mae: 0.346664, mean_q: 0.470651, mean_eps: 0.545379\n",
            "  505701/1750000: episode: 1853, duration: 9.068s, episode steps: 407, steps per second:  45, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002355, mae: 0.350586, mean_q: 0.476286, mean_eps: 0.545052\n",
            "  506157/1750000: episode: 1854, duration: 10.135s, episode steps: 456, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002525, mae: 0.355307, mean_q: 0.481612, mean_eps: 0.544663\n",
            "  506566/1750000: episode: 1855, duration: 8.953s, episode steps: 409, steps per second:  46, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002494, mae: 0.362983, mean_q: 0.493069, mean_eps: 0.544274\n",
            "  507054/1750000: episode: 1856, duration: 10.951s, episode steps: 488, steps per second:  45, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.002270, mae: 0.340831, mean_q: 0.462948, mean_eps: 0.543871\n",
            "  507362/1750000: episode: 1857, duration: 6.887s, episode steps: 308, steps per second:  45, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.003029, mae: 0.348267, mean_q: 0.472223, mean_eps: 0.543513\n",
            "  507774/1750000: episode: 1858, duration: 9.075s, episode steps: 412, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002572, mae: 0.351466, mean_q: 0.476614, mean_eps: 0.543189\n",
            "  508244/1750000: episode: 1859, duration: 10.509s, episode steps: 470, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002160, mae: 0.349281, mean_q: 0.474868, mean_eps: 0.542793\n",
            "  508658/1750000: episode: 1860, duration: 9.272s, episode steps: 414, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.002486, mae: 0.352678, mean_q: 0.476972, mean_eps: 0.542395\n",
            "  508942/1750000: episode: 1861, duration: 6.350s, episode steps: 284, steps per second:  45, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001919, mae: 0.342211, mean_q: 0.464393, mean_eps: 0.542080\n",
            "  509459/1750000: episode: 1862, duration: 11.478s, episode steps: 517, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002282, mae: 0.350392, mean_q: 0.475163, mean_eps: 0.541720\n",
            "  509738/1750000: episode: 1863, duration: 6.203s, episode steps: 279, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002766, mae: 0.354529, mean_q: 0.481799, mean_eps: 0.541362\n",
            "  510333/1750000: episode: 1864, duration: 13.245s, episode steps: 595, steps per second:  45, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.002679, mae: 0.350900, mean_q: 0.475141, mean_eps: 0.540968\n",
            "  510913/1750000: episode: 1865, duration: 12.992s, episode steps: 580, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002822, mae: 0.362364, mean_q: 0.493010, mean_eps: 0.540438\n",
            "  511542/1750000: episode: 1866, duration: 13.905s, episode steps: 629, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.002984, mae: 0.361986, mean_q: 0.490739, mean_eps: 0.539895\n",
            "  511958/1750000: episode: 1867, duration: 9.234s, episode steps: 416, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002435, mae: 0.365825, mean_q: 0.495171, mean_eps: 0.539425\n",
            "  512317/1750000: episode: 1868, duration: 8.049s, episode steps: 359, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.003359, mae: 0.365167, mean_q: 0.496763, mean_eps: 0.539076\n",
            "  512839/1750000: episode: 1869, duration: 11.561s, episode steps: 522, steps per second:  45, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.003196, mae: 0.364555, mean_q: 0.497220, mean_eps: 0.538680\n",
            "  513286/1750000: episode: 1870, duration: 10.021s, episode steps: 447, steps per second:  45, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.003355, mae: 0.363955, mean_q: 0.494961, mean_eps: 0.538244\n",
            "  513553/1750000: episode: 1871, duration: 5.998s, episode steps: 267, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.004003, mae: 0.364676, mean_q: 0.496163, mean_eps: 0.537922\n",
            "  513991/1750000: episode: 1872, duration: 9.599s, episode steps: 438, steps per second:  46, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.002535, mae: 0.359409, mean_q: 0.490677, mean_eps: 0.537605\n",
            "  514475/1750000: episode: 1873, duration: 10.761s, episode steps: 484, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002667, mae: 0.362716, mean_q: 0.492646, mean_eps: 0.537191\n",
            "  514988/1750000: episode: 1874, duration: 11.473s, episode steps: 513, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002672, mae: 0.355711, mean_q: 0.485151, mean_eps: 0.536743\n",
            "  515426/1750000: episode: 1875, duration: 9.844s, episode steps: 438, steps per second:  44, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.003223, mae: 0.367426, mean_q: 0.499777, mean_eps: 0.536315\n",
            "  516132/1750000: episode: 1876, duration: 15.972s, episode steps: 706, steps per second:  44, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.002577, mae: 0.359909, mean_q: 0.490684, mean_eps: 0.535800\n",
            "  516508/1750000: episode: 1877, duration: 8.497s, episode steps: 376, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.003293, mae: 0.360905, mean_q: 0.491941, mean_eps: 0.535314\n",
            "  516933/1750000: episode: 1878, duration: 9.566s, episode steps: 425, steps per second:  44, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002818, mae: 0.364342, mean_q: 0.496806, mean_eps: 0.534952\n",
            "  517546/1750000: episode: 1879, duration: 13.723s, episode steps: 613, steps per second:  45, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002562, mae: 0.362150, mean_q: 0.492790, mean_eps: 0.534484\n",
            "  518033/1750000: episode: 1880, duration: 11.010s, episode steps: 487, steps per second:  44, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002804, mae: 0.364330, mean_q: 0.495114, mean_eps: 0.533989\n",
            "  518546/1750000: episode: 1881, duration: 11.464s, episode steps: 513, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002820, mae: 0.355631, mean_q: 0.484266, mean_eps: 0.533539\n",
            "  518771/1750000: episode: 1882, duration: 4.994s, episode steps: 225, steps per second:  45, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 0.002945, mae: 0.362228, mean_q: 0.492416, mean_eps: 0.533208\n",
            "  519312/1750000: episode: 1883, duration: 12.299s, episode steps: 541, steps per second:  44, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002666, mae: 0.358706, mean_q: 0.488872, mean_eps: 0.532864\n",
            "  519657/1750000: episode: 1884, duration: 7.736s, episode steps: 345, steps per second:  45, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.003045, mae: 0.367856, mean_q: 0.501854, mean_eps: 0.532464\n",
            "  520009/1750000: episode: 1885, duration: 7.920s, episode steps: 352, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.002620, mae: 0.357360, mean_q: 0.486393, mean_eps: 0.532149\n",
            "  520419/1750000: episode: 1886, duration: 9.141s, episode steps: 410, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.003402, mae: 0.381636, mean_q: 0.516369, mean_eps: 0.531807\n",
            "  520686/1750000: episode: 1887, duration: 5.992s, episode steps: 267, steps per second:  45, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.003738, mae: 0.366000, mean_q: 0.496599, mean_eps: 0.531503\n",
            "  521275/1750000: episode: 1888, duration: 13.256s, episode steps: 589, steps per second:  44, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.002797, mae: 0.370305, mean_q: 0.504223, mean_eps: 0.531118\n",
            "  521607/1750000: episode: 1889, duration: 7.473s, episode steps: 332, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.002680, mae: 0.384690, mean_q: 0.523603, mean_eps: 0.530704\n",
            "  522015/1750000: episode: 1890, duration: 9.242s, episode steps: 408, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.003103, mae: 0.372939, mean_q: 0.505202, mean_eps: 0.530371\n",
            "  522317/1750000: episode: 1891, duration: 6.694s, episode steps: 302, steps per second:  45, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002680, mae: 0.382674, mean_q: 0.520614, mean_eps: 0.530051\n",
            "  522687/1750000: episode: 1892, duration: 8.156s, episode steps: 370, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.003876, mae: 0.373713, mean_q: 0.506617, mean_eps: 0.529748\n",
            "  523260/1750000: episode: 1893, duration: 12.865s, episode steps: 573, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.003047, mae: 0.372544, mean_q: 0.508422, mean_eps: 0.529325\n",
            "  523765/1750000: episode: 1894, duration: 11.386s, episode steps: 505, steps per second:  44, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002634, mae: 0.377190, mean_q: 0.514481, mean_eps: 0.528839\n",
            "  524136/1750000: episode: 1895, duration: 8.330s, episode steps: 371, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.003142, mae: 0.376548, mean_q: 0.512974, mean_eps: 0.528445\n",
            "  524504/1750000: episode: 1896, duration: 8.437s, episode steps: 368, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.003646, mae: 0.377920, mean_q: 0.514428, mean_eps: 0.528114\n",
            "  524812/1750000: episode: 1897, duration: 7.040s, episode steps: 308, steps per second:  44, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.003404, mae: 0.383251, mean_q: 0.524726, mean_eps: 0.527810\n",
            "  525139/1750000: episode: 1898, duration: 7.509s, episode steps: 327, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002988, mae: 0.364027, mean_q: 0.494915, mean_eps: 0.527523\n",
            "  525520/1750000: episode: 1899, duration: 8.715s, episode steps: 381, steps per second:  44, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.003222, mae: 0.376869, mean_q: 0.512718, mean_eps: 0.527205\n",
            "  525948/1750000: episode: 1900, duration: 9.682s, episode steps: 428, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002519, mae: 0.374511, mean_q: 0.509312, mean_eps: 0.526841\n",
            "  526380/1750000: episode: 1901, duration: 9.881s, episode steps: 432, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.003223, mae: 0.374542, mean_q: 0.507555, mean_eps: 0.526454\n",
            "  526917/1750000: episode: 1902, duration: 12.073s, episode steps: 537, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.003090, mae: 0.381458, mean_q: 0.518944, mean_eps: 0.526017\n",
            "  527519/1750000: episode: 1903, duration: 13.595s, episode steps: 602, steps per second:  44, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002554, mae: 0.377795, mean_q: 0.513716, mean_eps: 0.525504\n",
            "  527886/1750000: episode: 1904, duration: 8.338s, episode steps: 367, steps per second:  44, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.003024, mae: 0.374138, mean_q: 0.508200, mean_eps: 0.525068\n",
            "  528412/1750000: episode: 1905, duration: 11.882s, episode steps: 526, steps per second:  44, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.002948, mae: 0.368802, mean_q: 0.503284, mean_eps: 0.524667\n",
            "  529081/1750000: episode: 1906, duration: 15.114s, episode steps: 669, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.003091, mae: 0.370693, mean_q: 0.504794, mean_eps: 0.524129\n",
            "  529433/1750000: episode: 1907, duration: 7.933s, episode steps: 352, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.003020, mae: 0.374694, mean_q: 0.511916, mean_eps: 0.523668\n",
            "  529939/1750000: episode: 1908, duration: 11.316s, episode steps: 506, steps per second:  45, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002949, mae: 0.378030, mean_q: 0.515253, mean_eps: 0.523283\n",
            "  530181/1750000: episode: 1909, duration: 5.579s, episode steps: 242, steps per second:  43, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.003007, mae: 0.388386, mean_q: 0.529159, mean_eps: 0.522946\n",
            "  530652/1750000: episode: 1910, duration: 10.686s, episode steps: 471, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002677, mae: 0.389679, mean_q: 0.529079, mean_eps: 0.522626\n",
            "  531363/1750000: episode: 1911, duration: 16.064s, episode steps: 711, steps per second:  44, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.003327, mae: 0.388029, mean_q: 0.528041, mean_eps: 0.522095\n",
            "  531943/1750000: episode: 1912, duration: 13.023s, episode steps: 580, steps per second:  45, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.003004, mae: 0.391245, mean_q: 0.532116, mean_eps: 0.521513\n",
            "  532299/1750000: episode: 1913, duration: 8.099s, episode steps: 356, steps per second:  44, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.002654, mae: 0.392009, mean_q: 0.533246, mean_eps: 0.521092\n",
            "  532819/1750000: episode: 1914, duration: 11.769s, episode steps: 520, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.002821, mae: 0.391170, mean_q: 0.532087, mean_eps: 0.520698\n",
            "  533287/1750000: episode: 1915, duration: 10.494s, episode steps: 468, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.003387, mae: 0.392422, mean_q: 0.533204, mean_eps: 0.520253\n",
            "  533727/1750000: episode: 1916, duration: 10.065s, episode steps: 440, steps per second:  44, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.003021, mae: 0.390231, mean_q: 0.534003, mean_eps: 0.519845\n",
            "  534153/1750000: episode: 1917, duration: 9.683s, episode steps: 426, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.002857, mae: 0.392901, mean_q: 0.535182, mean_eps: 0.519454\n",
            "  534565/1750000: episode: 1918, duration: 9.252s, episode steps: 412, steps per second:  45, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002923, mae: 0.392155, mean_q: 0.534113, mean_eps: 0.519076\n",
            "  535352/1750000: episode: 1919, duration: 17.855s, episode steps: 787, steps per second:  44, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002663, mae: 0.391487, mean_q: 0.533143, mean_eps: 0.518538\n",
            "  535814/1750000: episode: 1920, duration: 10.492s, episode steps: 462, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002818, mae: 0.386794, mean_q: 0.527515, mean_eps: 0.517976\n",
            "  536428/1750000: episode: 1921, duration: 13.975s, episode steps: 614, steps per second:  44, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002961, mae: 0.388019, mean_q: 0.527499, mean_eps: 0.517492\n",
            "  536841/1750000: episode: 1922, duration: 9.501s, episode steps: 413, steps per second:  43, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.003137, mae: 0.388450, mean_q: 0.528187, mean_eps: 0.517029\n",
            "  537372/1750000: episode: 1923, duration: 11.958s, episode steps: 531, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.002526, mae: 0.384414, mean_q: 0.522552, mean_eps: 0.516605\n",
            "  537910/1750000: episode: 1924, duration: 12.152s, episode steps: 538, steps per second:  44, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002380, mae: 0.382066, mean_q: 0.518716, mean_eps: 0.516124\n",
            "  538241/1750000: episode: 1925, duration: 7.580s, episode steps: 331, steps per second:  44, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.003057, mae: 0.381823, mean_q: 0.518954, mean_eps: 0.515732\n",
            "  538800/1750000: episode: 1926, duration: 12.640s, episode steps: 559, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.003188, mae: 0.390143, mean_q: 0.529392, mean_eps: 0.515332\n",
            "  539278/1750000: episode: 1927, duration: 10.912s, episode steps: 478, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.002381, mae: 0.390800, mean_q: 0.531984, mean_eps: 0.514866\n",
            "  539838/1750000: episode: 1928, duration: 12.808s, episode steps: 560, steps per second:  44, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.003158, mae: 0.385951, mean_q: 0.526919, mean_eps: 0.514398\n",
            "  540470/1750000: episode: 1929, duration: 14.389s, episode steps: 632, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.003004, mae: 0.406666, mean_q: 0.553068, mean_eps: 0.513861\n",
            "  541020/1750000: episode: 1930, duration: 12.574s, episode steps: 550, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.003234, mae: 0.422556, mean_q: 0.573777, mean_eps: 0.513330\n",
            "  541321/1750000: episode: 1931, duration: 6.933s, episode steps: 301, steps per second:  43, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.003626, mae: 0.429110, mean_q: 0.580573, mean_eps: 0.512947\n",
            "  542108/1750000: episode: 1932, duration: 17.935s, episode steps: 787, steps per second:  44, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.003360, mae: 0.424902, mean_q: 0.575818, mean_eps: 0.512457\n",
            "  542477/1750000: episode: 1933, duration: 8.496s, episode steps: 369, steps per second:  43, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.003178, mae: 0.419806, mean_q: 0.572131, mean_eps: 0.511937\n",
            "  542721/1750000: episode: 1934, duration: 5.647s, episode steps: 244, steps per second:  43, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.003230, mae: 0.422755, mean_q: 0.574583, mean_eps: 0.511660\n",
            "  543283/1750000: episode: 1935, duration: 12.987s, episode steps: 562, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002995, mae: 0.421019, mean_q: 0.572879, mean_eps: 0.511298\n",
            "  543764/1750000: episode: 1936, duration: 11.113s, episode steps: 481, steps per second:  43, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002935, mae: 0.425100, mean_q: 0.578811, mean_eps: 0.510830\n",
            "  544276/1750000: episode: 1937, duration: 11.521s, episode steps: 512, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002887, mae: 0.428908, mean_q: 0.581666, mean_eps: 0.510384\n",
            "  544655/1750000: episode: 1938, duration: 8.527s, episode steps: 379, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.002849, mae: 0.415504, mean_q: 0.564767, mean_eps: 0.509982\n",
            "  545170/1750000: episode: 1939, duration: 11.697s, episode steps: 515, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.002578, mae: 0.415971, mean_q: 0.565300, mean_eps: 0.509579\n",
            "  545574/1750000: episode: 1940, duration: 9.273s, episode steps: 404, steps per second:  44, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.003406, mae: 0.426939, mean_q: 0.580831, mean_eps: 0.509165\n",
            "  546186/1750000: episode: 1941, duration: 13.855s, episode steps: 612, steps per second:  44, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.002955, mae: 0.416427, mean_q: 0.566874, mean_eps: 0.508708\n",
            "  546756/1750000: episode: 1942, duration: 13.130s, episode steps: 570, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002754, mae: 0.426430, mean_q: 0.580169, mean_eps: 0.508177\n",
            "  547455/1750000: episode: 1943, duration: 15.889s, episode steps: 699, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.003295, mae: 0.423495, mean_q: 0.575720, mean_eps: 0.507606\n",
            "  548163/1750000: episode: 1944, duration: 16.159s, episode steps: 708, steps per second:  44, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.003362, mae: 0.421968, mean_q: 0.573295, mean_eps: 0.506973\n",
            "  548613/1750000: episode: 1945, duration: 10.228s, episode steps: 450, steps per second:  44, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.003408, mae: 0.424139, mean_q: 0.576143, mean_eps: 0.506451\n",
            "  549010/1750000: episode: 1946, duration: 8.989s, episode steps: 397, steps per second:  44, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002445, mae: 0.424285, mean_q: 0.577216, mean_eps: 0.506069\n",
            "  549383/1750000: episode: 1947, duration: 8.563s, episode steps: 373, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002916, mae: 0.424937, mean_q: 0.578313, mean_eps: 0.505724\n",
            "  549863/1750000: episode: 1948, duration: 11.001s, episode steps: 480, steps per second:  44, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002830, mae: 0.416071, mean_q: 0.565707, mean_eps: 0.505340\n",
            "  550633/1750000: episode: 1949, duration: 17.606s, episode steps: 770, steps per second:  44, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.003132, mae: 0.435984, mean_q: 0.590367, mean_eps: 0.504777\n",
            "  551249/1750000: episode: 1950, duration: 13.907s, episode steps: 616, steps per second:  44, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.003019, mae: 0.440090, mean_q: 0.594840, mean_eps: 0.504152\n",
            "  551619/1750000: episode: 1951, duration: 8.294s, episode steps: 370, steps per second:  45, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.003193, mae: 0.438807, mean_q: 0.593982, mean_eps: 0.503709\n",
            "  552305/1750000: episode: 1952, duration: 15.666s, episode steps: 686, steps per second:  44, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.002831, mae: 0.435796, mean_q: 0.591908, mean_eps: 0.503234\n",
            "  552942/1750000: episode: 1953, duration: 14.602s, episode steps: 637, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.002792, mae: 0.431738, mean_q: 0.586508, mean_eps: 0.502638\n",
            "  553643/1750000: episode: 1954, duration: 16.107s, episode steps: 701, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002692, mae: 0.438993, mean_q: 0.594745, mean_eps: 0.502037\n",
            "  553912/1750000: episode: 1955, duration: 6.324s, episode steps: 269, steps per second:  43, episode reward:  2.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002499, mae: 0.430263, mean_q: 0.582239, mean_eps: 0.501602\n",
            "  554434/1750000: episode: 1956, duration: 11.888s, episode steps: 522, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.002408, mae: 0.434510, mean_q: 0.589814, mean_eps: 0.501245\n",
            "  555039/1750000: episode: 1957, duration: 13.771s, episode steps: 605, steps per second:  44, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.003180, mae: 0.434029, mean_q: 0.587229, mean_eps: 0.500738\n",
            "  555598/1750000: episode: 1958, duration: 12.852s, episode steps: 559, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.002772, mae: 0.436525, mean_q: 0.592141, mean_eps: 0.500214\n",
            "  556105/1750000: episode: 1959, duration: 11.660s, episode steps: 507, steps per second:  43, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002974, mae: 0.432703, mean_q: 0.587016, mean_eps: 0.499733\n",
            "  556388/1750000: episode: 1960, duration: 6.532s, episode steps: 283, steps per second:  43, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002869, mae: 0.439283, mean_q: 0.595629, mean_eps: 0.499379\n",
            "  556783/1750000: episode: 1961, duration: 9.183s, episode steps: 395, steps per second:  43, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.003227, mae: 0.438104, mean_q: 0.594453, mean_eps: 0.499074\n",
            "  557334/1750000: episode: 1962, duration: 12.839s, episode steps: 551, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002932, mae: 0.434235, mean_q: 0.589716, mean_eps: 0.498648\n",
            "  557710/1750000: episode: 1963, duration: 8.675s, episode steps: 376, steps per second:  43, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.002511, mae: 0.428722, mean_q: 0.582483, mean_eps: 0.498230\n",
            "  558367/1750000: episode: 1964, duration: 15.060s, episode steps: 657, steps per second:  44, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.002718, mae: 0.432646, mean_q: 0.587674, mean_eps: 0.497766\n",
            "  558741/1750000: episode: 1965, duration: 8.584s, episode steps: 374, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002576, mae: 0.438621, mean_q: 0.595594, mean_eps: 0.497301\n",
            "  559265/1750000: episode: 1966, duration: 11.843s, episode steps: 524, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002645, mae: 0.437545, mean_q: 0.594430, mean_eps: 0.496896\n",
            "  559847/1750000: episode: 1967, duration: 13.228s, episode steps: 582, steps per second:  44, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.881 [0.000, 3.000],  loss: 0.002490, mae: 0.430177, mean_q: 0.583235, mean_eps: 0.496400\n",
            "  560434/1750000: episode: 1968, duration: 13.458s, episode steps: 587, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.002853, mae: 0.442396, mean_q: 0.598797, mean_eps: 0.495874\n",
            "  560992/1750000: episode: 1969, duration: 12.992s, episode steps: 558, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.003073, mae: 0.444679, mean_q: 0.603239, mean_eps: 0.495359\n",
            "  561514/1750000: episode: 1970, duration: 12.126s, episode steps: 522, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002802, mae: 0.445437, mean_q: 0.604334, mean_eps: 0.494873\n",
            "  562181/1750000: episode: 1971, duration: 15.334s, episode steps: 667, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002980, mae: 0.446373, mean_q: 0.607346, mean_eps: 0.494337\n",
            "  562751/1750000: episode: 1972, duration: 13.146s, episode steps: 570, steps per second:  43, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002977, mae: 0.444478, mean_q: 0.603606, mean_eps: 0.493781\n",
            "  563311/1750000: episode: 1973, duration: 12.778s, episode steps: 560, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002954, mae: 0.441786, mean_q: 0.599528, mean_eps: 0.493273\n",
            "  564030/1750000: episode: 1974, duration: 16.650s, episode steps: 719, steps per second:  43, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002898, mae: 0.447836, mean_q: 0.607694, mean_eps: 0.492697\n",
            "  564764/1750000: episode: 1975, duration: 16.951s, episode steps: 734, steps per second:  43, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002872, mae: 0.448300, mean_q: 0.608472, mean_eps: 0.492044\n",
            "  565224/1750000: episode: 1976, duration: 10.678s, episode steps: 460, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.003793, mae: 0.453630, mean_q: 0.614109, mean_eps: 0.491507\n",
            "  565716/1750000: episode: 1977, duration: 11.415s, episode steps: 492, steps per second:  43, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002593, mae: 0.450501, mean_q: 0.611831, mean_eps: 0.491079\n",
            "  566321/1750000: episode: 1978, duration: 13.945s, episode steps: 605, steps per second:  43, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.003081, mae: 0.450540, mean_q: 0.611042, mean_eps: 0.490584\n",
            "  566777/1750000: episode: 1979, duration: 10.451s, episode steps: 456, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.003026, mae: 0.449804, mean_q: 0.611639, mean_eps: 0.490105\n",
            "  567520/1750000: episode: 1980, duration: 17.144s, episode steps: 743, steps per second:  43, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.002797, mae: 0.446748, mean_q: 0.606169, mean_eps: 0.489567\n",
            "  568072/1750000: episode: 1981, duration: 12.895s, episode steps: 552, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.003137, mae: 0.449952, mean_q: 0.609727, mean_eps: 0.488985\n",
            "  568483/1750000: episode: 1982, duration: 9.578s, episode steps: 411, steps per second:  43, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.002940, mae: 0.454026, mean_q: 0.616571, mean_eps: 0.488552\n",
            "  569298/1750000: episode: 1983, duration: 18.734s, episode steps: 815, steps per second:  44, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.912 [0.000, 3.000],  loss: 0.003197, mae: 0.445628, mean_q: 0.604298, mean_eps: 0.487999\n",
            "  570120/1750000: episode: 1984, duration: 19.111s, episode steps: 822, steps per second:  43, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002811, mae: 0.444974, mean_q: 0.604220, mean_eps: 0.487263\n",
            "  570830/1750000: episode: 1985, duration: 16.519s, episode steps: 710, steps per second:  43, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.003247, mae: 0.460084, mean_q: 0.625091, mean_eps: 0.486573\n",
            "  571310/1750000: episode: 1986, duration: 11.189s, episode steps: 480, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.842 [0.000, 3.000],  loss: 0.002535, mae: 0.466566, mean_q: 0.632957, mean_eps: 0.486037\n",
            "  571936/1750000: episode: 1987, duration: 14.361s, episode steps: 626, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002673, mae: 0.457838, mean_q: 0.620518, mean_eps: 0.485540\n",
            "  572411/1750000: episode: 1988, duration: 10.763s, episode steps: 475, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.002939, mae: 0.458729, mean_q: 0.623170, mean_eps: 0.485045\n",
            "  572952/1750000: episode: 1989, duration: 12.560s, episode steps: 541, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.002949, mae: 0.469562, mean_q: 0.635911, mean_eps: 0.484588\n",
            "  573551/1750000: episode: 1990, duration: 13.875s, episode steps: 599, steps per second:  43, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002872, mae: 0.463212, mean_q: 0.629680, mean_eps: 0.484075\n",
            "  574010/1750000: episode: 1991, duration: 10.638s, episode steps: 459, steps per second:  43, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.002370, mae: 0.453034, mean_q: 0.615722, mean_eps: 0.483598\n",
            "  574745/1750000: episode: 1992, duration: 17.097s, episode steps: 735, steps per second:  43, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002592, mae: 0.455449, mean_q: 0.619980, mean_eps: 0.483060\n",
            "  575375/1750000: episode: 1993, duration: 14.598s, episode steps: 630, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002733, mae: 0.457251, mean_q: 0.621602, mean_eps: 0.482446\n",
            "  576018/1750000: episode: 1994, duration: 15.011s, episode steps: 643, steps per second:  43, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002998, mae: 0.458899, mean_q: 0.623519, mean_eps: 0.481874\n",
            "  576529/1750000: episode: 1995, duration: 11.785s, episode steps: 511, steps per second:  43, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.002539, mae: 0.455829, mean_q: 0.619718, mean_eps: 0.481353\n",
            "  577049/1750000: episode: 1996, duration: 12.086s, episode steps: 520, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.002389, mae: 0.466284, mean_q: 0.632301, mean_eps: 0.480889\n",
            "  577530/1750000: episode: 1997, duration: 11.167s, episode steps: 481, steps per second:  43, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.003332, mae: 0.470172, mean_q: 0.636922, mean_eps: 0.480439\n",
            "  578020/1750000: episode: 1998, duration: 11.295s, episode steps: 490, steps per second:  43, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.003400, mae: 0.468791, mean_q: 0.639373, mean_eps: 0.480003\n",
            "  578521/1750000: episode: 1999, duration: 11.750s, episode steps: 501, steps per second:  43, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.002371, mae: 0.461683, mean_q: 0.629119, mean_eps: 0.479557\n",
            "  579161/1750000: episode: 2000, duration: 14.750s, episode steps: 640, steps per second:  43, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002385, mae: 0.454469, mean_q: 0.618964, mean_eps: 0.479042\n",
            "  579789/1750000: episode: 2001, duration: 14.657s, episode steps: 628, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002976, mae: 0.458849, mean_q: 0.624399, mean_eps: 0.478472\n",
            "  580368/1750000: episode: 2002, duration: 13.430s, episode steps: 579, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002859, mae: 0.472317, mean_q: 0.641728, mean_eps: 0.477930\n",
            "  580835/1750000: episode: 2003, duration: 10.813s, episode steps: 467, steps per second:  43, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.002711, mae: 0.476834, mean_q: 0.647221, mean_eps: 0.477460\n",
            "  581494/1750000: episode: 2004, duration: 15.250s, episode steps: 659, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.003136, mae: 0.483704, mean_q: 0.656548, mean_eps: 0.476952\n",
            "  582042/1750000: episode: 2005, duration: 12.880s, episode steps: 548, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.002863, mae: 0.483108, mean_q: 0.655676, mean_eps: 0.476409\n",
            "  582796/1750000: episode: 2006, duration: 17.532s, episode steps: 754, steps per second:  43, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.002486, mae: 0.485375, mean_q: 0.659079, mean_eps: 0.475824\n",
            "  583545/1750000: episode: 2007, duration: 17.505s, episode steps: 749, steps per second:  43, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002556, mae: 0.486572, mean_q: 0.662006, mean_eps: 0.475147\n",
            "  584007/1750000: episode: 2008, duration: 10.788s, episode steps: 462, steps per second:  43, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002660, mae: 0.483660, mean_q: 0.658909, mean_eps: 0.474602\n",
            "  584849/1750000: episode: 2009, duration: 19.641s, episode steps: 842, steps per second:  43, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002467, mae: 0.480607, mean_q: 0.652903, mean_eps: 0.474015\n",
            "  585462/1750000: episode: 2010, duration: 14.370s, episode steps: 613, steps per second:  43, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002715, mae: 0.480694, mean_q: 0.651791, mean_eps: 0.473360\n",
            "  585856/1750000: episode: 2011, duration: 9.292s, episode steps: 394, steps per second:  42, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002763, mae: 0.481339, mean_q: 0.653519, mean_eps: 0.472908\n",
            "  586342/1750000: episode: 2012, duration: 11.502s, episode steps: 486, steps per second:  42, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002855, mae: 0.483005, mean_q: 0.657717, mean_eps: 0.472512\n",
            "  586886/1750000: episode: 2013, duration: 12.700s, episode steps: 544, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.003005, mae: 0.484851, mean_q: 0.657426, mean_eps: 0.472047\n",
            "  587494/1750000: episode: 2014, duration: 14.326s, episode steps: 608, steps per second:  42, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002966, mae: 0.475872, mean_q: 0.645769, mean_eps: 0.471529\n",
            "  588042/1750000: episode: 2015, duration: 12.893s, episode steps: 548, steps per second:  43, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002638, mae: 0.482307, mean_q: 0.653815, mean_eps: 0.471009\n",
            "  588593/1750000: episode: 2016, duration: 13.052s, episode steps: 551, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.003150, mae: 0.487595, mean_q: 0.661078, mean_eps: 0.470514\n",
            "  589134/1750000: episode: 2017, duration: 12.724s, episode steps: 541, steps per second:  43, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002992, mae: 0.487308, mean_q: 0.661862, mean_eps: 0.470022\n",
            "  589795/1750000: episode: 2018, duration: 15.446s, episode steps: 661, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002918, mae: 0.482637, mean_q: 0.654575, mean_eps: 0.469482\n",
            "  590342/1750000: episode: 2019, duration: 12.797s, episode steps: 547, steps per second:  43, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.002663, mae: 0.496242, mean_q: 0.671175, mean_eps: 0.468939\n",
            "  590880/1750000: episode: 2020, duration: 12.789s, episode steps: 538, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.003170, mae: 0.499903, mean_q: 0.676375, mean_eps: 0.468451\n",
            "  591369/1750000: episode: 2021, duration: 11.410s, episode steps: 489, steps per second:  43, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.002624, mae: 0.504601, mean_q: 0.683502, mean_eps: 0.467988\n",
            "  591812/1750000: episode: 2022, duration: 10.486s, episode steps: 443, steps per second:  42, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002738, mae: 0.501103, mean_q: 0.679378, mean_eps: 0.467569\n",
            "  592668/1750000: episode: 2023, duration: 20.142s, episode steps: 856, steps per second:  42, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002826, mae: 0.500424, mean_q: 0.679819, mean_eps: 0.466986\n",
            "  593152/1750000: episode: 2024, duration: 11.357s, episode steps: 484, steps per second:  43, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.003147, mae: 0.497749, mean_q: 0.675634, mean_eps: 0.466383\n",
            "  593959/1750000: episode: 2025, duration: 18.945s, episode steps: 807, steps per second:  43, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002891, mae: 0.501964, mean_q: 0.680940, mean_eps: 0.465801\n",
            "  594364/1750000: episode: 2026, duration: 9.489s, episode steps: 405, steps per second:  43, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.002367, mae: 0.500663, mean_q: 0.680397, mean_eps: 0.465256\n",
            "  594907/1750000: episode: 2027, duration: 12.896s, episode steps: 543, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002461, mae: 0.495667, mean_q: 0.672938, mean_eps: 0.464829\n",
            "  595383/1750000: episode: 2028, duration: 11.203s, episode steps: 476, steps per second:  42, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.002773, mae: 0.494053, mean_q: 0.670940, mean_eps: 0.464370\n",
            "  596035/1750000: episode: 2029, duration: 15.317s, episode steps: 652, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002199, mae: 0.504786, mean_q: 0.685707, mean_eps: 0.463863\n",
            "  596790/1750000: episode: 2030, duration: 17.717s, episode steps: 755, steps per second:  43, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.002877, mae: 0.498873, mean_q: 0.676094, mean_eps: 0.463229\n",
            "  597371/1750000: episode: 2031, duration: 13.688s, episode steps: 581, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002804, mae: 0.496640, mean_q: 0.674394, mean_eps: 0.462628\n",
            "  597848/1750000: episode: 2032, duration: 11.228s, episode steps: 477, steps per second:  42, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002852, mae: 0.503772, mean_q: 0.686171, mean_eps: 0.462153\n",
            "  598561/1750000: episode: 2033, duration: 16.743s, episode steps: 713, steps per second:  43, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002454, mae: 0.496844, mean_q: 0.674758, mean_eps: 0.461616\n",
            "  599128/1750000: episode: 2034, duration: 13.287s, episode steps: 567, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002703, mae: 0.501942, mean_q: 0.679401, mean_eps: 0.461040\n",
            "  599694/1750000: episode: 2035, duration: 13.322s, episode steps: 566, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.002319, mae: 0.505711, mean_q: 0.685138, mean_eps: 0.460531\n",
            "  600322/1750000: episode: 2036, duration: 14.722s, episode steps: 628, steps per second:  43, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.868 [0.000, 3.000],  loss: 0.002269, mae: 0.505138, mean_q: 0.684007, mean_eps: 0.459993\n",
            "  600985/1750000: episode: 2037, duration: 15.583s, episode steps: 663, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.790 [0.000, 3.000],  loss: 0.002612, mae: 0.514054, mean_q: 0.696335, mean_eps: 0.459411\n",
            "  601290/1750000: episode: 2038, duration: 7.118s, episode steps: 305, steps per second:  43, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002632, mae: 0.513582, mean_q: 0.697099, mean_eps: 0.458976\n",
            "  601863/1750000: episode: 2039, duration: 13.289s, episode steps: 573, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.002810, mae: 0.510761, mean_q: 0.693514, mean_eps: 0.458582\n",
            "  602264/1750000: episode: 2040, duration: 9.473s, episode steps: 401, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002532, mae: 0.506202, mean_q: 0.686881, mean_eps: 0.458144\n",
            "  602827/1750000: episode: 2041, duration: 13.367s, episode steps: 563, steps per second:  42, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002685, mae: 0.517221, mean_q: 0.700973, mean_eps: 0.457710\n",
            "  603579/1750000: episode: 2042, duration: 17.593s, episode steps: 752, steps per second:  43, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002663, mae: 0.516473, mean_q: 0.700496, mean_eps: 0.457118\n",
            "  604045/1750000: episode: 2043, duration: 11.046s, episode steps: 466, steps per second:  42, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002330, mae: 0.513057, mean_q: 0.696542, mean_eps: 0.456569\n",
            "  604440/1750000: episode: 2044, duration: 9.352s, episode steps: 395, steps per second:  42, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.808 [0.000, 3.000],  loss: 0.002365, mae: 0.506120, mean_q: 0.688373, mean_eps: 0.456182\n",
            "  604991/1750000: episode: 2045, duration: 13.007s, episode steps: 551, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002578, mae: 0.513500, mean_q: 0.694126, mean_eps: 0.455757\n",
            "  605678/1750000: episode: 2046, duration: 16.156s, episode steps: 687, steps per second:  43, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.002637, mae: 0.511087, mean_q: 0.693035, mean_eps: 0.455199\n",
            "  606261/1750000: episode: 2047, duration: 13.826s, episode steps: 583, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002757, mae: 0.512907, mean_q: 0.695716, mean_eps: 0.454627\n",
            "  606720/1750000: episode: 2048, duration: 10.779s, episode steps: 459, steps per second:  43, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002464, mae: 0.508157, mean_q: 0.690165, mean_eps: 0.454159\n",
            "  607572/1750000: episode: 2049, duration: 20.068s, episode steps: 852, steps per second:  42, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.002870, mae: 0.512039, mean_q: 0.695190, mean_eps: 0.453570\n",
            "  608380/1750000: episode: 2050, duration: 18.892s, episode steps: 808, steps per second:  43, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002554, mae: 0.510549, mean_q: 0.693288, mean_eps: 0.452823\n",
            "  609050/1750000: episode: 2051, duration: 15.706s, episode steps: 670, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002599, mae: 0.510002, mean_q: 0.692395, mean_eps: 0.452157\n",
            "  609610/1750000: episode: 2052, duration: 13.314s, episode steps: 560, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.002635, mae: 0.516529, mean_q: 0.700841, mean_eps: 0.451603\n",
            "  610339/1750000: episode: 2053, duration: 17.241s, episode steps: 729, steps per second:  42, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002263, mae: 0.516039, mean_q: 0.699653, mean_eps: 0.451023\n",
            "  610872/1750000: episode: 2054, duration: 12.672s, episode steps: 533, steps per second:  42, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002423, mae: 0.531128, mean_q: 0.719031, mean_eps: 0.450456\n",
            "  611257/1750000: episode: 2055, duration: 9.128s, episode steps: 385, steps per second:  42, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002128, mae: 0.527782, mean_q: 0.713325, mean_eps: 0.450042\n",
            "  611929/1750000: episode: 2056, duration: 15.737s, episode steps: 672, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002603, mae: 0.530613, mean_q: 0.718248, mean_eps: 0.449565\n",
            "  612618/1750000: episode: 2057, duration: 16.191s, episode steps: 689, steps per second:  43, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002889, mae: 0.536271, mean_q: 0.725792, mean_eps: 0.448953\n",
            "  613143/1750000: episode: 2058, duration: 12.432s, episode steps: 525, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002932, mae: 0.525194, mean_q: 0.711346, mean_eps: 0.448408\n",
            "  613793/1750000: episode: 2059, duration: 15.448s, episode steps: 650, steps per second:  42, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002692, mae: 0.532590, mean_q: 0.720437, mean_eps: 0.447879\n",
            "  614437/1750000: episode: 2060, duration: 15.090s, episode steps: 644, steps per second:  43, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002307, mae: 0.532558, mean_q: 0.722703, mean_eps: 0.447296\n",
            "  615126/1750000: episode: 2061, duration: 16.559s, episode steps: 689, steps per second:  42, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002501, mae: 0.533708, mean_q: 0.724261, mean_eps: 0.446696\n",
            "  615701/1750000: episode: 2062, duration: 13.580s, episode steps: 575, steps per second:  42, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.002674, mae: 0.527013, mean_q: 0.714272, mean_eps: 0.446127\n",
            "  616208/1750000: episode: 2063, duration: 12.075s, episode steps: 507, steps per second:  42, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002235, mae: 0.532149, mean_q: 0.719878, mean_eps: 0.445641\n",
            "  617028/1750000: episode: 2064, duration: 19.376s, episode steps: 820, steps per second:  42, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.002406, mae: 0.528161, mean_q: 0.716099, mean_eps: 0.445046\n",
            "  617611/1750000: episode: 2065, duration: 13.679s, episode steps: 583, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002365, mae: 0.531358, mean_q: 0.721152, mean_eps: 0.444414\n",
            "  618044/1750000: episode: 2066, duration: 10.470s, episode steps: 433, steps per second:  41, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.002477, mae: 0.529221, mean_q: 0.716953, mean_eps: 0.443957\n",
            "  618457/1750000: episode: 2067, duration: 9.855s, episode steps: 413, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.002468, mae: 0.537402, mean_q: 0.727008, mean_eps: 0.443575\n",
            "  619300/1750000: episode: 2068, duration: 20.092s, episode steps: 843, steps per second:  42, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002261, mae: 0.529999, mean_q: 0.717006, mean_eps: 0.443010\n",
            "  619870/1750000: episode: 2069, duration: 13.589s, episode steps: 570, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002485, mae: 0.525737, mean_q: 0.712741, mean_eps: 0.442374\n",
            "  620437/1750000: episode: 2070, duration: 13.494s, episode steps: 567, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002609, mae: 0.544295, mean_q: 0.736478, mean_eps: 0.441861\n",
            "  620920/1750000: episode: 2071, duration: 11.510s, episode steps: 483, steps per second:  42, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.002331, mae: 0.541469, mean_q: 0.732879, mean_eps: 0.441390\n",
            "  621466/1750000: episode: 2072, duration: 12.980s, episode steps: 546, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002668, mae: 0.546620, mean_q: 0.738529, mean_eps: 0.440927\n",
            "  621827/1750000: episode: 2073, duration: 8.535s, episode steps: 361, steps per second:  42, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002417, mae: 0.551721, mean_q: 0.747642, mean_eps: 0.440519\n",
            "  622511/1750000: episode: 2074, duration: 16.173s, episode steps: 684, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.845 [0.000, 3.000],  loss: 0.002082, mae: 0.542424, mean_q: 0.734857, mean_eps: 0.440049\n",
            "  623169/1750000: episode: 2075, duration: 15.887s, episode steps: 658, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002508, mae: 0.550545, mean_q: 0.744557, mean_eps: 0.439444\n",
            "  623725/1750000: episode: 2076, duration: 13.074s, episode steps: 556, steps per second:  43, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002297, mae: 0.539938, mean_q: 0.732622, mean_eps: 0.438897\n",
            "  624186/1750000: episode: 2077, duration: 10.976s, episode steps: 461, steps per second:  42, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002519, mae: 0.549351, mean_q: 0.744009, mean_eps: 0.438440\n",
            "  624694/1750000: episode: 2078, duration: 11.990s, episode steps: 508, steps per second:  42, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002484, mae: 0.539242, mean_q: 0.729390, mean_eps: 0.438004\n",
            "  625312/1750000: episode: 2079, duration: 14.538s, episode steps: 618, steps per second:  43, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002372, mae: 0.537761, mean_q: 0.727488, mean_eps: 0.437498\n",
            "  626160/1750000: episode: 2080, duration: 20.167s, episode steps: 848, steps per second:  42, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002605, mae: 0.549991, mean_q: 0.745711, mean_eps: 0.436839\n",
            "  626640/1750000: episode: 2081, duration: 11.645s, episode steps: 480, steps per second:  41, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.002349, mae: 0.547244, mean_q: 0.740892, mean_eps: 0.436242\n",
            "  627306/1750000: episode: 2082, duration: 15.830s, episode steps: 666, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.002419, mae: 0.546793, mean_q: 0.739911, mean_eps: 0.435725\n",
            "  627991/1750000: episode: 2083, duration: 16.472s, episode steps: 685, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.002239, mae: 0.547194, mean_q: 0.740846, mean_eps: 0.435117\n",
            "  628682/1750000: episode: 2084, duration: 16.494s, episode steps: 691, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002504, mae: 0.542802, mean_q: 0.735077, mean_eps: 0.434498\n",
            "  629208/1750000: episode: 2085, duration: 12.647s, episode steps: 526, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002483, mae: 0.546081, mean_q: 0.739358, mean_eps: 0.433950\n",
            "  629654/1750000: episode: 2086, duration: 10.679s, episode steps: 446, steps per second:  42, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.002483, mae: 0.541230, mean_q: 0.732573, mean_eps: 0.433513\n",
            "  630164/1750000: episode: 2087, duration: 12.187s, episode steps: 510, steps per second:  42, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.002191, mae: 0.553947, mean_q: 0.748406, mean_eps: 0.433083\n",
            "  630832/1750000: episode: 2088, duration: 15.844s, episode steps: 668, steps per second:  42, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002371, mae: 0.563984, mean_q: 0.762540, mean_eps: 0.432554\n",
            "  631518/1750000: episode: 2089, duration: 16.388s, episode steps: 686, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002418, mae: 0.569180, mean_q: 0.768359, mean_eps: 0.431943\n",
            "  632111/1750000: episode: 2090, duration: 14.161s, episode steps: 593, steps per second:  42, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002186, mae: 0.560338, mean_q: 0.756420, mean_eps: 0.431367\n",
            "  632855/1750000: episode: 2091, duration: 17.772s, episode steps: 744, steps per second:  42, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002128, mae: 0.564101, mean_q: 0.762379, mean_eps: 0.430766\n",
            "  633292/1750000: episode: 2092, duration: 10.556s, episode steps: 437, steps per second:  41, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.780 [0.000, 3.000],  loss: 0.002769, mae: 0.567765, mean_q: 0.767039, mean_eps: 0.430235\n",
            "  634012/1750000: episode: 2093, duration: 17.256s, episode steps: 720, steps per second:  42, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002170, mae: 0.567912, mean_q: 0.766547, mean_eps: 0.429715\n",
            "  634419/1750000: episode: 2094, duration: 9.861s, episode steps: 407, steps per second:  41, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002066, mae: 0.565342, mean_q: 0.762056, mean_eps: 0.429207\n",
            "  634825/1750000: episode: 2095, duration: 9.783s, episode steps: 406, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.002101, mae: 0.565274, mean_q: 0.762882, mean_eps: 0.428840\n",
            "  635205/1750000: episode: 2096, duration: 9.121s, episode steps: 380, steps per second:  42, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002146, mae: 0.565310, mean_q: 0.763318, mean_eps: 0.428486\n",
            "  635808/1750000: episode: 2097, duration: 14.510s, episode steps: 603, steps per second:  42, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002056, mae: 0.558461, mean_q: 0.754922, mean_eps: 0.428045\n",
            "  636217/1750000: episode: 2098, duration: 9.894s, episode steps: 409, steps per second:  41, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.001989, mae: 0.566668, mean_q: 0.766746, mean_eps: 0.427589\n",
            "  636658/1750000: episode: 2099, duration: 10.592s, episode steps: 441, steps per second:  42, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.001981, mae: 0.562217, mean_q: 0.760144, mean_eps: 0.427206\n",
            "  636893/1750000: episode: 2100, duration: 5.763s, episode steps: 235, steps per second:  41, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.002515, mae: 0.560797, mean_q: 0.757633, mean_eps: 0.426902\n",
            "  637542/1750000: episode: 2101, duration: 15.469s, episode steps: 649, steps per second:  42, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002351, mae: 0.563238, mean_q: 0.761127, mean_eps: 0.426504\n",
            "  637990/1750000: episode: 2102, duration: 10.799s, episode steps: 448, steps per second:  41, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.002057, mae: 0.560293, mean_q: 0.756653, mean_eps: 0.426011\n",
            "  638527/1750000: episode: 2103, duration: 12.875s, episode steps: 537, steps per second:  42, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002221, mae: 0.564266, mean_q: 0.760801, mean_eps: 0.425568\n",
            "  638941/1750000: episode: 2104, duration: 9.964s, episode steps: 414, steps per second:  42, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002189, mae: 0.565089, mean_q: 0.762592, mean_eps: 0.425139\n",
            "  639437/1750000: episode: 2105, duration: 11.862s, episode steps: 496, steps per second:  42, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002303, mae: 0.563873, mean_q: 0.760853, mean_eps: 0.424729\n",
            "  640093/1750000: episode: 2106, duration: 15.832s, episode steps: 656, steps per second:  41, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001965, mae: 0.565249, mean_q: 0.763378, mean_eps: 0.424211\n",
            "  640707/1750000: episode: 2107, duration: 14.808s, episode steps: 614, steps per second:  41, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002338, mae: 0.569539, mean_q: 0.768839, mean_eps: 0.423640\n",
            "  641302/1750000: episode: 2108, duration: 14.278s, episode steps: 595, steps per second:  42, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002236, mae: 0.565471, mean_q: 0.763970, mean_eps: 0.423096\n",
            "  641944/1750000: episode: 2109, duration: 15.416s, episode steps: 642, steps per second:  42, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.002218, mae: 0.571226, mean_q: 0.769905, mean_eps: 0.422540\n",
            "  642534/1750000: episode: 2110, duration: 14.212s, episode steps: 590, steps per second:  42, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002074, mae: 0.571888, mean_q: 0.772134, mean_eps: 0.421986\n",
            "  643330/1750000: episode: 2111, duration: 19.200s, episode steps: 796, steps per second:  41, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002263, mae: 0.574731, mean_q: 0.777097, mean_eps: 0.421361\n",
            "  644034/1750000: episode: 2112, duration: 16.917s, episode steps: 704, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002363, mae: 0.568225, mean_q: 0.767959, mean_eps: 0.420686\n",
            "  644619/1750000: episode: 2113, duration: 13.817s, episode steps: 585, steps per second:  42, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002009, mae: 0.563750, mean_q: 0.761482, mean_eps: 0.420107\n",
            "  645309/1750000: episode: 2114, duration: 16.662s, episode steps: 690, steps per second:  41, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.002307, mae: 0.571745, mean_q: 0.771196, mean_eps: 0.419532\n",
            "  645915/1750000: episode: 2115, duration: 14.526s, episode steps: 606, steps per second:  42, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002078, mae: 0.570612, mean_q: 0.770366, mean_eps: 0.418949\n",
            "  646480/1750000: episode: 2116, duration: 13.742s, episode steps: 565, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002112, mae: 0.568885, mean_q: 0.766789, mean_eps: 0.418424\n",
            "  647176/1750000: episode: 2117, duration: 16.848s, episode steps: 696, steps per second:  41, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002106, mae: 0.568594, mean_q: 0.767826, mean_eps: 0.417857\n",
            "  647762/1750000: episode: 2118, duration: 14.124s, episode steps: 586, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.002200, mae: 0.569273, mean_q: 0.769655, mean_eps: 0.417279\n",
            "  648475/1750000: episode: 2119, duration: 17.166s, episode steps: 713, steps per second:  42, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002319, mae: 0.571225, mean_q: 0.771036, mean_eps: 0.416694\n",
            "  649390/1750000: episode: 2120, duration: 22.226s, episode steps: 915, steps per second:  41, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002383, mae: 0.569099, mean_q: 0.767756, mean_eps: 0.415961\n",
            "  650106/1750000: episode: 2121, duration: 17.320s, episode steps: 716, steps per second:  41, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.001827, mae: 0.577267, mean_q: 0.778289, mean_eps: 0.415227\n",
            "  650783/1750000: episode: 2122, duration: 16.245s, episode steps: 677, steps per second:  42, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002125, mae: 0.585059, mean_q: 0.788247, mean_eps: 0.414600\n",
            "  651496/1750000: episode: 2123, duration: 17.104s, episode steps: 713, steps per second:  42, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.002222, mae: 0.577962, mean_q: 0.779979, mean_eps: 0.413976\n",
            "  652131/1750000: episode: 2124, duration: 15.345s, episode steps: 635, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.002189, mae: 0.581763, mean_q: 0.785730, mean_eps: 0.413369\n",
            "  652702/1750000: episode: 2125, duration: 13.841s, episode steps: 571, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002239, mae: 0.581121, mean_q: 0.784785, mean_eps: 0.412826\n",
            "  653070/1750000: episode: 2126, duration: 8.875s, episode steps: 368, steps per second:  41, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002063, mae: 0.584461, mean_q: 0.788778, mean_eps: 0.412403\n",
            "  653527/1750000: episode: 2127, duration: 11.144s, episode steps: 457, steps per second:  41, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001970, mae: 0.580224, mean_q: 0.784712, mean_eps: 0.412032\n",
            "  654162/1750000: episode: 2128, duration: 15.363s, episode steps: 635, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002277, mae: 0.578827, mean_q: 0.781138, mean_eps: 0.411540\n",
            "  654724/1750000: episode: 2129, duration: 13.615s, episode steps: 562, steps per second:  41, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.845 [0.000, 3.000],  loss: 0.001916, mae: 0.585848, mean_q: 0.791508, mean_eps: 0.411002\n",
            "  655292/1750000: episode: 2130, duration: 13.890s, episode steps: 568, steps per second:  41, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001882, mae: 0.580289, mean_q: 0.783375, mean_eps: 0.410495\n",
            "  655966/1750000: episode: 2131, duration: 16.313s, episode steps: 674, steps per second:  41, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.002068, mae: 0.578764, mean_q: 0.780520, mean_eps: 0.409935\n",
            "  656767/1750000: episode: 2132, duration: 19.526s, episode steps: 801, steps per second:  41, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002049, mae: 0.574344, mean_q: 0.775155, mean_eps: 0.409271\n",
            "  657328/1750000: episode: 2133, duration: 13.732s, episode steps: 561, steps per second:  41, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002312, mae: 0.578492, mean_q: 0.780105, mean_eps: 0.408659\n",
            "  658180/1750000: episode: 2134, duration: 20.474s, episode steps: 852, steps per second:  42, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.001956, mae: 0.577064, mean_q: 0.778585, mean_eps: 0.408023\n",
            "  658816/1750000: episode: 2135, duration: 15.245s, episode steps: 636, steps per second:  42, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001938, mae: 0.576384, mean_q: 0.777052, mean_eps: 0.407354\n",
            "  659540/1750000: episode: 2136, duration: 17.761s, episode steps: 724, steps per second:  41, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002119, mae: 0.581218, mean_q: 0.785343, mean_eps: 0.406742\n",
            "  660186/1750000: episode: 2137, duration: 15.763s, episode steps: 646, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.002131, mae: 0.579884, mean_q: 0.782152, mean_eps: 0.406124\n",
            "  660622/1750000: episode: 2138, duration: 10.618s, episode steps: 436, steps per second:  41, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.002443, mae: 0.586261, mean_q: 0.789863, mean_eps: 0.405636\n",
            "  661205/1750000: episode: 2139, duration: 14.218s, episode steps: 583, steps per second:  41, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002369, mae: 0.589466, mean_q: 0.796762, mean_eps: 0.405177\n",
            "  662044/1750000: episode: 2140, duration: 20.507s, episode steps: 839, steps per second:  41, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.002260, mae: 0.589195, mean_q: 0.796136, mean_eps: 0.404538\n",
            "  662683/1750000: episode: 2141, duration: 15.548s, episode steps: 639, steps per second:  41, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.001880, mae: 0.581866, mean_q: 0.787047, mean_eps: 0.403874\n",
            "  663320/1750000: episode: 2142, duration: 15.608s, episode steps: 637, steps per second:  41, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.001936, mae: 0.587697, mean_q: 0.793439, mean_eps: 0.403300\n",
            "  664111/1750000: episode: 2143, duration: 19.289s, episode steps: 791, steps per second:  41, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.001984, mae: 0.586563, mean_q: 0.792720, mean_eps: 0.402657\n",
            "  664811/1750000: episode: 2144, duration: 16.832s, episode steps: 700, steps per second:  42, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002132, mae: 0.590141, mean_q: 0.796302, mean_eps: 0.401986\n",
            "  665458/1750000: episode: 2145, duration: 15.676s, episode steps: 647, steps per second:  41, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001810, mae: 0.587761, mean_q: 0.793391, mean_eps: 0.401379\n",
            "  666146/1750000: episode: 2146, duration: 16.782s, episode steps: 688, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002294, mae: 0.586271, mean_q: 0.791819, mean_eps: 0.400778\n",
            "  666663/1750000: episode: 2147, duration: 12.559s, episode steps: 517, steps per second:  41, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002083, mae: 0.591725, mean_q: 0.799408, mean_eps: 0.400236\n",
            "  667357/1750000: episode: 2148, duration: 17.074s, episode steps: 694, steps per second:  41, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002283, mae: 0.588635, mean_q: 0.795155, mean_eps: 0.399691\n",
            "  668086/1750000: episode: 2149, duration: 17.758s, episode steps: 729, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002198, mae: 0.586094, mean_q: 0.790973, mean_eps: 0.399050\n",
            "  668741/1750000: episode: 2150, duration: 16.086s, episode steps: 655, steps per second:  41, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002456, mae: 0.589425, mean_q: 0.795563, mean_eps: 0.398427\n",
            "  669426/1750000: episode: 2151, duration: 16.652s, episode steps: 685, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002527, mae: 0.582272, mean_q: 0.787764, mean_eps: 0.397824\n",
            "  670074/1750000: episode: 2152, duration: 15.995s, episode steps: 648, steps per second:  41, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001897, mae: 0.591293, mean_q: 0.798644, mean_eps: 0.397225\n",
            "  670893/1750000: episode: 2153, duration: 20.044s, episode steps: 819, steps per second:  41, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002288, mae: 0.604077, mean_q: 0.815123, mean_eps: 0.396564\n",
            "  671644/1750000: episode: 2154, duration: 18.258s, episode steps: 751, steps per second:  41, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002272, mae: 0.601598, mean_q: 0.811553, mean_eps: 0.395859\n",
            "  672212/1750000: episode: 2155, duration: 13.957s, episode steps: 568, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002415, mae: 0.593753, mean_q: 0.799997, mean_eps: 0.395267\n",
            "  672863/1750000: episode: 2156, duration: 15.996s, episode steps: 651, steps per second:  41, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002216, mae: 0.603071, mean_q: 0.812973, mean_eps: 0.394718\n",
            "  673654/1750000: episode: 2157, duration: 19.408s, episode steps: 791, steps per second:  41, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002092, mae: 0.600862, mean_q: 0.810776, mean_eps: 0.394068\n",
            "  674689/1750000: episode: 2158, duration: 25.388s, episode steps: 1035, steps per second:  41, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002028, mae: 0.599711, mean_q: 0.808975, mean_eps: 0.393245\n",
            "  675188/1750000: episode: 2159, duration: 12.355s, episode steps: 499, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002009, mae: 0.601823, mean_q: 0.814434, mean_eps: 0.392556\n",
            "  675655/1750000: episode: 2160, duration: 11.407s, episode steps: 467, steps per second:  41, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001994, mae: 0.595260, mean_q: 0.803306, mean_eps: 0.392122\n",
            "  676392/1750000: episode: 2161, duration: 18.043s, episode steps: 737, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.002200, mae: 0.600248, mean_q: 0.809834, mean_eps: 0.391580\n",
            "  676896/1750000: episode: 2162, duration: 12.503s, episode steps: 504, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001690, mae: 0.597530, mean_q: 0.806691, mean_eps: 0.391022\n",
            "  677278/1750000: episode: 2163, duration: 9.446s, episode steps: 382, steps per second:  40, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002161, mae: 0.603350, mean_q: 0.813700, mean_eps: 0.390623\n",
            "  677735/1750000: episode: 2164, duration: 11.026s, episode steps: 457, steps per second:  41, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002196, mae: 0.599090, mean_q: 0.808412, mean_eps: 0.390245\n",
            "  678406/1750000: episode: 2165, duration: 16.481s, episode steps: 671, steps per second:  41, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001801, mae: 0.599301, mean_q: 0.809991, mean_eps: 0.389737\n",
            "  678970/1750000: episode: 2166, duration: 13.859s, episode steps: 564, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.002166, mae: 0.599546, mean_q: 0.810692, mean_eps: 0.389181\n",
            "  679502/1750000: episode: 2167, duration: 13.012s, episode steps: 532, steps per second:  41, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.001634, mae: 0.599384, mean_q: 0.809018, mean_eps: 0.388688\n",
            "  680059/1750000: episode: 2168, duration: 13.801s, episode steps: 557, steps per second:  40, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.001980, mae: 0.601044, mean_q: 0.810281, mean_eps: 0.388198\n",
            "  680804/1750000: episode: 2169, duration: 18.438s, episode steps: 745, steps per second:  40, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.001880, mae: 0.608027, mean_q: 0.820269, mean_eps: 0.387613\n",
            "  681354/1750000: episode: 2170, duration: 13.511s, episode steps: 550, steps per second:  41, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.002268, mae: 0.607459, mean_q: 0.820091, mean_eps: 0.387030\n",
            "  681828/1750000: episode: 2171, duration: 11.643s, episode steps: 474, steps per second:  41, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001962, mae: 0.612988, mean_q: 0.827989, mean_eps: 0.386569\n",
            "  682629/1750000: episode: 2172, duration: 19.894s, episode steps: 801, steps per second:  40, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002028, mae: 0.608481, mean_q: 0.820928, mean_eps: 0.385995\n",
            "  683203/1750000: episode: 2173, duration: 14.237s, episode steps: 574, steps per second:  40, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.001674, mae: 0.611303, mean_q: 0.825566, mean_eps: 0.385376\n",
            "  683712/1750000: episode: 2174, duration: 12.653s, episode steps: 509, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002006, mae: 0.612074, mean_q: 0.826623, mean_eps: 0.384890\n",
            "  684080/1750000: episode: 2175, duration: 9.158s, episode steps: 368, steps per second:  40, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.002227, mae: 0.605866, mean_q: 0.816504, mean_eps: 0.384495\n",
            "  684671/1750000: episode: 2176, duration: 14.478s, episode steps: 591, steps per second:  41, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.001766, mae: 0.607746, mean_q: 0.820899, mean_eps: 0.384063\n",
            "  685440/1750000: episode: 2177, duration: 18.745s, episode steps: 769, steps per second:  41, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002198, mae: 0.606138, mean_q: 0.818791, mean_eps: 0.383451\n",
            "  685966/1750000: episode: 2178, duration: 13.094s, episode steps: 526, steps per second:  40, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002188, mae: 0.607015, mean_q: 0.819895, mean_eps: 0.382868\n",
            "  686867/1750000: episode: 2179, duration: 22.451s, episode steps: 901, steps per second:  40, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.002060, mae: 0.607835, mean_q: 0.820044, mean_eps: 0.382226\n",
            "  687518/1750000: episode: 2180, duration: 16.357s, episode steps: 651, steps per second:  40, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002201, mae: 0.608070, mean_q: 0.819781, mean_eps: 0.381527\n",
            "  687990/1750000: episode: 2181, duration: 11.675s, episode steps: 472, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002107, mae: 0.608127, mean_q: 0.821668, mean_eps: 0.381021\n",
            "  688597/1750000: episode: 2182, duration: 15.267s, episode steps: 607, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.001767, mae: 0.602985, mean_q: 0.814448, mean_eps: 0.380535\n",
            "  689122/1750000: episode: 2183, duration: 13.073s, episode steps: 525, steps per second:  40, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002098, mae: 0.602942, mean_q: 0.813500, mean_eps: 0.380026\n",
            "  689657/1750000: episode: 2184, duration: 13.337s, episode steps: 535, steps per second:  40, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.002139, mae: 0.611725, mean_q: 0.824683, mean_eps: 0.379549\n",
            "  690216/1750000: episode: 2185, duration: 13.713s, episode steps: 559, steps per second:  41, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001916, mae: 0.617171, mean_q: 0.832483, mean_eps: 0.379058\n",
            "  690842/1750000: episode: 2186, duration: 15.515s, episode steps: 626, steps per second:  40, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002156, mae: 0.626203, mean_q: 0.843187, mean_eps: 0.378525\n",
            "  691616/1750000: episode: 2187, duration: 19.349s, episode steps: 774, steps per second:  40, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002023, mae: 0.628522, mean_q: 0.847501, mean_eps: 0.377895\n",
            "  692128/1750000: episode: 2188, duration: 12.953s, episode steps: 512, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.001708, mae: 0.618815, mean_q: 0.834466, mean_eps: 0.377317\n",
            "  692642/1750000: episode: 2189, duration: 12.849s, episode steps: 514, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001855, mae: 0.628257, mean_q: 0.845874, mean_eps: 0.376854\n",
            "  693107/1750000: episode: 2190, duration: 11.500s, episode steps: 465, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002217, mae: 0.628501, mean_q: 0.848234, mean_eps: 0.376413\n",
            "  693667/1750000: episode: 2191, duration: 13.985s, episode steps: 560, steps per second:  40, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.002122, mae: 0.624730, mean_q: 0.842879, mean_eps: 0.375953\n",
            "  694219/1750000: episode: 2192, duration: 13.710s, episode steps: 552, steps per second:  40, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.001949, mae: 0.621482, mean_q: 0.839115, mean_eps: 0.375452\n",
            "  694646/1750000: episode: 2193, duration: 10.701s, episode steps: 427, steps per second:  40, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.001654, mae: 0.620009, mean_q: 0.836863, mean_eps: 0.375011\n",
            "  695229/1750000: episode: 2194, duration: 14.659s, episode steps: 583, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001987, mae: 0.626247, mean_q: 0.843690, mean_eps: 0.374556\n",
            "  695674/1750000: episode: 2195, duration: 11.058s, episode steps: 445, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.002666, mae: 0.628585, mean_q: 0.846843, mean_eps: 0.374093\n",
            "  696312/1750000: episode: 2196, duration: 15.876s, episode steps: 638, steps per second:  40, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002279, mae: 0.627192, mean_q: 0.846160, mean_eps: 0.373607\n",
            "  696921/1750000: episode: 2197, duration: 15.177s, episode steps: 609, steps per second:  40, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.783 [0.000, 3.000],  loss: 0.001957, mae: 0.625173, mean_q: 0.844614, mean_eps: 0.373046\n",
            "  697374/1750000: episode: 2198, duration: 11.220s, episode steps: 453, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002061, mae: 0.626703, mean_q: 0.846485, mean_eps: 0.372567\n",
            "  698004/1750000: episode: 2199, duration: 15.734s, episode steps: 630, steps per second:  40, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.002122, mae: 0.623206, mean_q: 0.839544, mean_eps: 0.372081\n",
            "  698816/1750000: episode: 2200, duration: 20.399s, episode steps: 812, steps per second:  40, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002059, mae: 0.624801, mean_q: 0.843475, mean_eps: 0.371433\n",
            "  699430/1750000: episode: 2201, duration: 15.277s, episode steps: 614, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.001918, mae: 0.623399, mean_q: 0.841588, mean_eps: 0.370790\n",
            "  699791/1750000: episode: 2202, duration: 8.959s, episode steps: 361, steps per second:  40, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.001418, mae: 0.623831, mean_q: 0.842422, mean_eps: 0.370351\n",
            "  700096/1750000: episode: 2203, duration: 7.696s, episode steps: 305, steps per second:  40, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.797 [0.000, 3.000],  loss: 0.002408, mae: 0.623288, mean_q: 0.841175, mean_eps: 0.370052\n",
            "  700679/1750000: episode: 2204, duration: 14.758s, episode steps: 583, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001561, mae: 0.635088, mean_q: 0.855376, mean_eps: 0.369653\n",
            "  701149/1750000: episode: 2205, duration: 11.690s, episode steps: 470, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.002294, mae: 0.633016, mean_q: 0.853252, mean_eps: 0.369177\n",
            "  701650/1750000: episode: 2206, duration: 12.496s, episode steps: 501, steps per second:  40, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.002094, mae: 0.633617, mean_q: 0.853183, mean_eps: 0.368740\n",
            "  702279/1750000: episode: 2207, duration: 15.689s, episode steps: 629, steps per second:  40, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002274, mae: 0.639005, mean_q: 0.859522, mean_eps: 0.368232\n",
            "  703035/1750000: episode: 2208, duration: 18.836s, episode steps: 756, steps per second:  40, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001733, mae: 0.631468, mean_q: 0.850265, mean_eps: 0.367610\n",
            "  703423/1750000: episode: 2209, duration: 9.598s, episode steps: 388, steps per second:  40, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.001962, mae: 0.637155, mean_q: 0.857850, mean_eps: 0.367095\n",
            "  704241/1750000: episode: 2210, duration: 20.328s, episode steps: 818, steps per second:  40, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.001774, mae: 0.634737, mean_q: 0.854786, mean_eps: 0.366551\n",
            "  704842/1750000: episode: 2211, duration: 14.901s, episode steps: 601, steps per second:  40, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.002355, mae: 0.634112, mean_q: 0.854002, mean_eps: 0.365912\n",
            "  705279/1750000: episode: 2212, duration: 11.001s, episode steps: 437, steps per second:  40, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001695, mae: 0.638582, mean_q: 0.859792, mean_eps: 0.365446\n",
            "  706119/1750000: episode: 2213, duration: 20.993s, episode steps: 840, steps per second:  40, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.001605, mae: 0.634138, mean_q: 0.854877, mean_eps: 0.364872\n",
            "  706638/1750000: episode: 2214, duration: 13.041s, episode steps: 519, steps per second:  40, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001931, mae: 0.636688, mean_q: 0.856488, mean_eps: 0.364260\n",
            "  707263/1750000: episode: 2215, duration: 15.608s, episode steps: 625, steps per second:  40, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002127, mae: 0.638972, mean_q: 0.859657, mean_eps: 0.363745\n",
            "  708031/1750000: episode: 2216, duration: 19.161s, episode steps: 768, steps per second:  40, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002026, mae: 0.637307, mean_q: 0.857381, mean_eps: 0.363119\n",
            "  708675/1750000: episode: 2217, duration: 16.277s, episode steps: 644, steps per second:  40, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001811, mae: 0.638070, mean_q: 0.857337, mean_eps: 0.362483\n",
            "  709393/1750000: episode: 2218, duration: 17.901s, episode steps: 718, steps per second:  40, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002033, mae: 0.640260, mean_q: 0.860982, mean_eps: 0.361869\n",
            "  709819/1750000: episode: 2219, duration: 10.602s, episode steps: 426, steps per second:  40, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002092, mae: 0.637308, mean_q: 0.856161, mean_eps: 0.361355\n",
            "  710421/1750000: episode: 2220, duration: 15.199s, episode steps: 602, steps per second:  40, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.001738, mae: 0.644998, mean_q: 0.867708, mean_eps: 0.360892\n",
            "  711151/1750000: episode: 2221, duration: 18.278s, episode steps: 730, steps per second:  40, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002176, mae: 0.645910, mean_q: 0.868578, mean_eps: 0.360293\n",
            "  711706/1750000: episode: 2222, duration: 14.037s, episode steps: 555, steps per second:  40, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.001887, mae: 0.644624, mean_q: 0.869032, mean_eps: 0.359715\n",
            "  712104/1750000: episode: 2223, duration: 10.064s, episode steps: 398, steps per second:  40, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002015, mae: 0.654631, mean_q: 0.881642, mean_eps: 0.359286\n",
            "  712467/1750000: episode: 2224, duration: 9.203s, episode steps: 363, steps per second:  39, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.380 [0.000, 3.000],  loss: 0.001925, mae: 0.639470, mean_q: 0.861235, mean_eps: 0.358944\n",
            "  713114/1750000: episode: 2225, duration: 16.356s, episode steps: 647, steps per second:  40, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.001655, mae: 0.650777, mean_q: 0.877272, mean_eps: 0.358489\n",
            "  713588/1750000: episode: 2226, duration: 11.942s, episode steps: 474, steps per second:  40, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.002101, mae: 0.647817, mean_q: 0.871400, mean_eps: 0.357985\n",
            "  714168/1750000: episode: 2227, duration: 14.808s, episode steps: 580, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002269, mae: 0.652223, mean_q: 0.878238, mean_eps: 0.357512\n",
            "  714763/1750000: episode: 2228, duration: 15.142s, episode steps: 595, steps per second:  39, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.002176, mae: 0.644391, mean_q: 0.869297, mean_eps: 0.356982\n",
            "  715289/1750000: episode: 2229, duration: 13.318s, episode steps: 526, steps per second:  39, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.001894, mae: 0.645758, mean_q: 0.870089, mean_eps: 0.356477\n",
            "  715896/1750000: episode: 2230, duration: 15.251s, episode steps: 607, steps per second:  40, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.787 [0.000, 3.000],  loss: 0.001720, mae: 0.643080, mean_q: 0.865955, mean_eps: 0.355967\n",
            "  716493/1750000: episode: 2231, duration: 15.127s, episode steps: 597, steps per second:  39, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001867, mae: 0.645445, mean_q: 0.869345, mean_eps: 0.355425\n",
            "  717171/1750000: episode: 2232, duration: 16.861s, episode steps: 678, steps per second:  40, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001894, mae: 0.646979, mean_q: 0.872586, mean_eps: 0.354851\n",
            "  717876/1750000: episode: 2233, duration: 17.699s, episode steps: 705, steps per second:  40, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001994, mae: 0.649741, mean_q: 0.875341, mean_eps: 0.354230\n",
            "  718680/1750000: episode: 2234, duration: 20.379s, episode steps: 804, steps per second:  39, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001827, mae: 0.642851, mean_q: 0.866851, mean_eps: 0.353552\n",
            "  719154/1750000: episode: 2235, duration: 11.910s, episode steps: 474, steps per second:  40, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001840, mae: 0.647114, mean_q: 0.870925, mean_eps: 0.352976\n",
            "  719955/1750000: episode: 2236, duration: 20.142s, episode steps: 801, steps per second:  40, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.001933, mae: 0.646460, mean_q: 0.870746, mean_eps: 0.352401\n",
            "  720749/1750000: episode: 2237, duration: 20.097s, episode steps: 794, steps per second:  40, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.001935, mae: 0.656339, mean_q: 0.882160, mean_eps: 0.351683\n",
            "  721253/1750000: episode: 2238, duration: 12.784s, episode steps: 504, steps per second:  39, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002138, mae: 0.657841, mean_q: 0.884102, mean_eps: 0.351098\n",
            "  721907/1750000: episode: 2239, duration: 16.448s, episode steps: 654, steps per second:  40, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001926, mae: 0.659932, mean_q: 0.888209, mean_eps: 0.350578\n",
            "  722472/1750000: episode: 2240, duration: 14.242s, episode steps: 565, steps per second:  40, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002175, mae: 0.654055, mean_q: 0.879813, mean_eps: 0.350031\n",
            "  723104/1750000: episode: 2241, duration: 15.857s, episode steps: 632, steps per second:  40, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001805, mae: 0.659933, mean_q: 0.887209, mean_eps: 0.349493\n",
            "  723749/1750000: episode: 2242, duration: 16.054s, episode steps: 645, steps per second:  40, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.001672, mae: 0.662745, mean_q: 0.892274, mean_eps: 0.348917\n",
            "  724356/1750000: episode: 2243, duration: 15.415s, episode steps: 607, steps per second:  39, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001594, mae: 0.653518, mean_q: 0.879909, mean_eps: 0.348353\n",
            "  724902/1750000: episode: 2244, duration: 13.916s, episode steps: 546, steps per second:  39, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.001895, mae: 0.660458, mean_q: 0.888779, mean_eps: 0.347835\n",
            "  725528/1750000: episode: 2245, duration: 16.043s, episode steps: 626, steps per second:  39, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.002051, mae: 0.657841, mean_q: 0.885168, mean_eps: 0.347307\n",
            "  726495/1750000: episode: 2246, duration: 24.352s, episode steps: 967, steps per second:  40, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.809 [0.000, 3.000],  loss: 0.001880, mae: 0.659487, mean_q: 0.887710, mean_eps: 0.346591\n",
            "  727111/1750000: episode: 2247, duration: 15.664s, episode steps: 616, steps per second:  39, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002140, mae: 0.658034, mean_q: 0.885954, mean_eps: 0.345878\n",
            "  728012/1750000: episode: 2248, duration: 22.905s, episode steps: 901, steps per second:  39, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.001643, mae: 0.669915, mean_q: 0.900514, mean_eps: 0.345196\n",
            "  728475/1750000: episode: 2249, duration: 11.741s, episode steps: 463, steps per second:  39, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001823, mae: 0.654198, mean_q: 0.880537, mean_eps: 0.344582\n",
            "  729197/1750000: episode: 2250, duration: 18.374s, episode steps: 722, steps per second:  39, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001964, mae: 0.662899, mean_q: 0.890738, mean_eps: 0.344048\n",
            "  729842/1750000: episode: 2251, duration: 16.299s, episode steps: 645, steps per second:  40, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001636, mae: 0.656446, mean_q: 0.882646, mean_eps: 0.343432\n",
            "  730602/1750000: episode: 2252, duration: 19.243s, episode steps: 760, steps per second:  39, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001631, mae: 0.660488, mean_q: 0.888270, mean_eps: 0.342800\n",
            "  731237/1750000: episode: 2253, duration: 16.151s, episode steps: 635, steps per second:  39, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002115, mae: 0.660608, mean_q: 0.887482, mean_eps: 0.342172\n",
            "  731933/1750000: episode: 2254, duration: 17.680s, episode steps: 696, steps per second:  39, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.001487, mae: 0.671165, mean_q: 0.901850, mean_eps: 0.341573\n",
            "  732753/1750000: episode: 2255, duration: 20.745s, episode steps: 820, steps per second:  40, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002119, mae: 0.664380, mean_q: 0.893624, mean_eps: 0.340890\n",
            "  733260/1750000: episode: 2256, duration: 12.882s, episode steps: 507, steps per second:  39, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.001658, mae: 0.670924, mean_q: 0.903895, mean_eps: 0.340295\n",
            "  733881/1750000: episode: 2257, duration: 15.999s, episode steps: 621, steps per second:  39, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002053, mae: 0.667572, mean_q: 0.897497, mean_eps: 0.339787\n",
            "  734565/1750000: episode: 2258, duration: 17.431s, episode steps: 684, steps per second:  39, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.001728, mae: 0.671777, mean_q: 0.902523, mean_eps: 0.339198\n",
            "  735287/1750000: episode: 2259, duration: 18.312s, episode steps: 722, steps per second:  39, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002583, mae: 0.666218, mean_q: 0.895097, mean_eps: 0.338567\n",
            "  736032/1750000: episode: 2260, duration: 19.103s, episode steps: 745, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002273, mae: 0.662230, mean_q: 0.889340, mean_eps: 0.337908\n",
            "  736761/1750000: episode: 2261, duration: 18.578s, episode steps: 729, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.257 [0.000, 3.000],  loss: 0.002059, mae: 0.664279, mean_q: 0.892262, mean_eps: 0.337244\n",
            "  737177/1750000: episode: 2262, duration: 10.707s, episode steps: 416, steps per second:  39, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.002217, mae: 0.663381, mean_q: 0.891645, mean_eps: 0.336727\n",
            "  737650/1750000: episode: 2263, duration: 12.047s, episode steps: 473, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.001969, mae: 0.667178, mean_q: 0.895564, mean_eps: 0.336327\n",
            "  738383/1750000: episode: 2264, duration: 18.413s, episode steps: 733, steps per second:  40, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001804, mae: 0.668766, mean_q: 0.898122, mean_eps: 0.335786\n",
            "  739126/1750000: episode: 2265, duration: 18.810s, episode steps: 743, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.002101, mae: 0.670411, mean_q: 0.900218, mean_eps: 0.335121\n",
            "  739689/1750000: episode: 2266, duration: 14.403s, episode steps: 563, steps per second:  39, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002519, mae: 0.666490, mean_q: 0.896479, mean_eps: 0.334533\n",
            "  740098/1750000: episode: 2267, duration: 10.440s, episode steps: 409, steps per second:  39, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002050, mae: 0.670645, mean_q: 0.901517, mean_eps: 0.334095\n",
            "  740710/1750000: episode: 2268, duration: 15.718s, episode steps: 612, steps per second:  39, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001734, mae: 0.681324, mean_q: 0.914650, mean_eps: 0.333636\n",
            "  741421/1750000: episode: 2269, duration: 18.126s, episode steps: 711, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002119, mae: 0.679200, mean_q: 0.912102, mean_eps: 0.333041\n",
            "  741943/1750000: episode: 2270, duration: 13.332s, episode steps: 522, steps per second:  39, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002238, mae: 0.680118, mean_q: 0.913278, mean_eps: 0.332486\n",
            "  742847/1750000: episode: 2271, duration: 23.138s, episode steps: 904, steps per second:  39, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001936, mae: 0.677554, mean_q: 0.909583, mean_eps: 0.331845\n",
            "  743587/1750000: episode: 2272, duration: 18.998s, episode steps: 740, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001848, mae: 0.682407, mean_q: 0.916089, mean_eps: 0.331106\n",
            "  744036/1750000: episode: 2273, duration: 11.443s, episode steps: 449, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.274 [0.000, 3.000],  loss: 0.001925, mae: 0.684131, mean_q: 0.917556, mean_eps: 0.330571\n",
            "  744687/1750000: episode: 2274, duration: 16.675s, episode steps: 651, steps per second:  39, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.001913, mae: 0.679677, mean_q: 0.912557, mean_eps: 0.330076\n",
            "  745342/1750000: episode: 2275, duration: 16.883s, episode steps: 655, steps per second:  39, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002032, mae: 0.677041, mean_q: 0.908031, mean_eps: 0.329487\n",
            "  745973/1750000: episode: 2276, duration: 16.200s, episode steps: 631, steps per second:  39, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.002068, mae: 0.674030, mean_q: 0.905273, mean_eps: 0.328908\n",
            "  746473/1750000: episode: 2277, duration: 12.832s, episode steps: 500, steps per second:  39, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001929, mae: 0.680134, mean_q: 0.913052, mean_eps: 0.328398\n",
            "  747046/1750000: episode: 2278, duration: 14.673s, episode steps: 573, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002058, mae: 0.680635, mean_q: 0.914689, mean_eps: 0.327916\n",
            "  747766/1750000: episode: 2279, duration: 18.479s, episode steps: 720, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002068, mae: 0.675303, mean_q: 0.906533, mean_eps: 0.327335\n",
            "  748522/1750000: episode: 2280, duration: 19.509s, episode steps: 756, steps per second:  39, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002242, mae: 0.673713, mean_q: 0.904236, mean_eps: 0.326670\n",
            "  749055/1750000: episode: 2281, duration: 13.609s, episode steps: 533, steps per second:  39, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001648, mae: 0.681384, mean_q: 0.915529, mean_eps: 0.326091\n",
            "  749410/1750000: episode: 2282, duration: 9.252s, episode steps: 355, steps per second:  38, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.001960, mae: 0.670680, mean_q: 0.900441, mean_eps: 0.325691\n",
            "  750221/1750000: episode: 2283, duration: 20.951s, episode steps: 811, steps per second:  39, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001986, mae: 0.681960, mean_q: 0.914393, mean_eps: 0.325166\n",
            "  750810/1750000: episode: 2284, duration: 15.073s, episode steps: 589, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002228, mae: 0.686328, mean_q: 0.920918, mean_eps: 0.324536\n",
            "  751478/1750000: episode: 2285, duration: 17.079s, episode steps: 668, steps per second:  39, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.001452, mae: 0.691946, mean_q: 0.928824, mean_eps: 0.323970\n",
            "  752028/1750000: episode: 2286, duration: 14.141s, episode steps: 550, steps per second:  39, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.001899, mae: 0.693465, mean_q: 0.931174, mean_eps: 0.323423\n",
            "  752788/1750000: episode: 2287, duration: 19.652s, episode steps: 760, steps per second:  39, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.002017, mae: 0.691405, mean_q: 0.927724, mean_eps: 0.322835\n",
            "  753529/1750000: episode: 2288, duration: 19.229s, episode steps: 741, steps per second:  39, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.780 [0.000, 3.000],  loss: 0.001848, mae: 0.690365, mean_q: 0.927052, mean_eps: 0.322158\n",
            "  754327/1750000: episode: 2289, duration: 20.457s, episode steps: 798, steps per second:  39, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.001772, mae: 0.691165, mean_q: 0.928310, mean_eps: 0.321465\n",
            "  755349/1750000: episode: 2290, duration: 26.252s, episode steps: 1022, steps per second:  39, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002167, mae: 0.691353, mean_q: 0.927334, mean_eps: 0.320646\n",
            "  755831/1750000: episode: 2291, duration: 12.235s, episode steps: 482, steps per second:  39, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.001735, mae: 0.696476, mean_q: 0.935484, mean_eps: 0.319969\n",
            "  756430/1750000: episode: 2292, duration: 15.627s, episode steps: 599, steps per second:  38, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002437, mae: 0.691164, mean_q: 0.927089, mean_eps: 0.319483\n",
            "  757175/1750000: episode: 2293, duration: 19.240s, episode steps: 745, steps per second:  39, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002172, mae: 0.691799, mean_q: 0.929228, mean_eps: 0.318878\n",
            "  757786/1750000: episode: 2294, duration: 15.794s, episode steps: 611, steps per second:  39, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.001913, mae: 0.689509, mean_q: 0.925017, mean_eps: 0.318268\n",
            "  758371/1750000: episode: 2295, duration: 14.852s, episode steps: 585, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.002000, mae: 0.692073, mean_q: 0.928178, mean_eps: 0.317730\n",
            "  759261/1750000: episode: 2296, duration: 22.867s, episode steps: 890, steps per second:  39, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002204, mae: 0.690034, mean_q: 0.925743, mean_eps: 0.317066\n",
            "  759777/1750000: episode: 2297, duration: 13.280s, episode steps: 516, steps per second:  39, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001689, mae: 0.695633, mean_q: 0.934281, mean_eps: 0.316432\n",
            "  760302/1750000: episode: 2298, duration: 13.586s, episode steps: 525, steps per second:  39, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.002056, mae: 0.703268, mean_q: 0.944600, mean_eps: 0.315964\n",
            "  760881/1750000: episode: 2299, duration: 14.947s, episode steps: 579, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.820 [0.000, 3.000],  loss: 0.001548, mae: 0.702320, mean_q: 0.943610, mean_eps: 0.315467\n",
            "  761344/1750000: episode: 2300, duration: 11.930s, episode steps: 463, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002024, mae: 0.707361, mean_q: 0.948630, mean_eps: 0.314999\n",
            "  761965/1750000: episode: 2301, duration: 16.031s, episode steps: 621, steps per second:  39, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002166, mae: 0.697400, mean_q: 0.937773, mean_eps: 0.314511\n",
            "  762727/1750000: episode: 2302, duration: 19.715s, episode steps: 762, steps per second:  39, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001705, mae: 0.698338, mean_q: 0.936725, mean_eps: 0.313889\n",
            "  763230/1750000: episode: 2303, duration: 12.977s, episode steps: 503, steps per second:  39, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001695, mae: 0.701096, mean_q: 0.939883, mean_eps: 0.313320\n",
            "  763802/1750000: episode: 2304, duration: 14.706s, episode steps: 572, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002090, mae: 0.696611, mean_q: 0.934647, mean_eps: 0.312836\n",
            "  764510/1750000: episode: 2305, duration: 18.211s, episode steps: 708, steps per second:  39, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002006, mae: 0.698972, mean_q: 0.938056, mean_eps: 0.312260\n",
            "  765067/1750000: episode: 2306, duration: 14.330s, episode steps: 557, steps per second:  39, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.002252, mae: 0.702748, mean_q: 0.942578, mean_eps: 0.311691\n",
            "  765611/1750000: episode: 2307, duration: 14.289s, episode steps: 544, steps per second:  38, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001746, mae: 0.704042, mean_q: 0.944261, mean_eps: 0.311196\n",
            "  766184/1750000: episode: 2308, duration: 14.860s, episode steps: 573, steps per second:  39, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.002018, mae: 0.704960, mean_q: 0.944949, mean_eps: 0.310694\n",
            "  766563/1750000: episode: 2309, duration: 9.866s, episode steps: 379, steps per second:  38, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001955, mae: 0.695557, mean_q: 0.931881, mean_eps: 0.310265\n",
            "  767394/1750000: episode: 2310, duration: 21.527s, episode steps: 831, steps per second:  39, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001703, mae: 0.706382, mean_q: 0.946808, mean_eps: 0.309720\n",
            "  768101/1750000: episode: 2311, duration: 18.416s, episode steps: 707, steps per second:  38, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001587, mae: 0.698800, mean_q: 0.938300, mean_eps: 0.309027\n",
            "  768826/1750000: episode: 2312, duration: 18.803s, episode steps: 725, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001836, mae: 0.702858, mean_q: 0.942879, mean_eps: 0.308382\n",
            "  769370/1750000: episode: 2313, duration: 14.118s, episode steps: 544, steps per second:  39, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001825, mae: 0.705091, mean_q: 0.945970, mean_eps: 0.307812\n",
            "  769846/1750000: episode: 2314, duration: 12.213s, episode steps: 476, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.001857, mae: 0.701906, mean_q: 0.940978, mean_eps: 0.307353\n",
            "  770505/1750000: episode: 2315, duration: 17.117s, episode steps: 659, steps per second:  39, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.001868, mae: 0.710822, mean_q: 0.954241, mean_eps: 0.306842\n",
            "  770954/1750000: episode: 2316, duration: 11.640s, episode steps: 449, steps per second:  39, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002021, mae: 0.716621, mean_q: 0.960735, mean_eps: 0.306343\n",
            "  771539/1750000: episode: 2317, duration: 15.180s, episode steps: 585, steps per second:  39, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.002185, mae: 0.712166, mean_q: 0.954881, mean_eps: 0.305879\n",
            "  772086/1750000: episode: 2318, duration: 14.411s, episode steps: 547, steps per second:  38, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002262, mae: 0.712794, mean_q: 0.955418, mean_eps: 0.305369\n",
            "  772707/1750000: episode: 2319, duration: 16.112s, episode steps: 621, steps per second:  39, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.374 [0.000, 3.000],  loss: 0.001534, mae: 0.707482, mean_q: 0.949141, mean_eps: 0.304844\n",
            "  773510/1750000: episode: 2320, duration: 20.925s, episode steps: 803, steps per second:  38, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.001788, mae: 0.713096, mean_q: 0.955603, mean_eps: 0.304203\n",
            "  774024/1750000: episode: 2321, duration: 13.560s, episode steps: 514, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.001804, mae: 0.708039, mean_q: 0.949571, mean_eps: 0.303611\n",
            "  774617/1750000: episode: 2322, duration: 15.456s, episode steps: 593, steps per second:  38, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.001893, mae: 0.713847, mean_q: 0.956570, mean_eps: 0.303112\n",
            "  775262/1750000: episode: 2323, duration: 16.954s, episode steps: 645, steps per second:  38, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: 0.002221, mae: 0.713269, mean_q: 0.955304, mean_eps: 0.302554\n",
            "  776109/1750000: episode: 2324, duration: 22.088s, episode steps: 847, steps per second:  38, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001946, mae: 0.712975, mean_q: 0.955901, mean_eps: 0.301883\n",
            "  776736/1750000: episode: 2325, duration: 16.444s, episode steps: 627, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.001611, mae: 0.714039, mean_q: 0.958130, mean_eps: 0.301220\n",
            "  777669/1750000: episode: 2326, duration: 24.074s, episode steps: 933, steps per second:  39, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002055, mae: 0.709217, mean_q: 0.949871, mean_eps: 0.300518\n",
            "  778286/1750000: episode: 2327, duration: 16.116s, episode steps: 617, steps per second:  38, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.002165, mae: 0.714938, mean_q: 0.959297, mean_eps: 0.299820\n",
            "  778807/1750000: episode: 2328, duration: 13.518s, episode steps: 521, steps per second:  39, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.845 [0.000, 3.000],  loss: 0.002078, mae: 0.714448, mean_q: 0.959033, mean_eps: 0.299309\n",
            "  779224/1750000: episode: 2329, duration: 10.980s, episode steps: 417, steps per second:  38, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.312 [0.000, 3.000],  loss: 0.001838, mae: 0.717847, mean_q: 0.962172, mean_eps: 0.298887\n",
            "  779792/1750000: episode: 2330, duration: 14.822s, episode steps: 568, steps per second:  38, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.870 [0.000, 3.000],  loss: 0.001983, mae: 0.712713, mean_q: 0.956642, mean_eps: 0.298445\n",
            "  780401/1750000: episode: 2331, duration: 16.041s, episode steps: 609, steps per second:  38, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001840, mae: 0.727700, mean_q: 0.974424, mean_eps: 0.297914\n",
            "  781042/1750000: episode: 2332, duration: 16.654s, episode steps: 641, steps per second:  38, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.001884, mae: 0.727582, mean_q: 0.975156, mean_eps: 0.297350\n",
            "  781970/1750000: episode: 2333, duration: 24.274s, episode steps: 928, steps per second:  38, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001647, mae: 0.724745, mean_q: 0.971470, mean_eps: 0.296645\n",
            "  782643/1750000: episode: 2334, duration: 17.567s, episode steps: 673, steps per second:  38, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.001641, mae: 0.725244, mean_q: 0.973026, mean_eps: 0.295925\n",
            "  783304/1750000: episode: 2335, duration: 17.287s, episode steps: 661, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.002059, mae: 0.726926, mean_q: 0.972717, mean_eps: 0.295325\n",
            "  783687/1750000: episode: 2336, duration: 9.958s, episode steps: 383, steps per second:  38, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.001881, mae: 0.720569, mean_q: 0.967068, mean_eps: 0.294855\n",
            "  784296/1750000: episode: 2337, duration: 16.109s, episode steps: 609, steps per second:  38, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.228 [0.000, 3.000],  loss: 0.001692, mae: 0.726595, mean_q: 0.975183, mean_eps: 0.294409\n",
            "  784950/1750000: episode: 2338, duration: 17.117s, episode steps: 654, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.794 [0.000, 3.000],  loss: 0.002208, mae: 0.722169, mean_q: 0.968955, mean_eps: 0.293840\n",
            "  785655/1750000: episode: 2339, duration: 18.536s, episode steps: 705, steps per second:  38, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002142, mae: 0.723228, mean_q: 0.969643, mean_eps: 0.293228\n",
            "  786297/1750000: episode: 2340, duration: 16.861s, episode steps: 642, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.001942, mae: 0.721920, mean_q: 0.968031, mean_eps: 0.292622\n",
            "  786743/1750000: episode: 2341, duration: 11.745s, episode steps: 446, steps per second:  38, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.002368, mae: 0.730137, mean_q: 0.976822, mean_eps: 0.292132\n",
            "  787248/1750000: episode: 2342, duration: 13.316s, episode steps: 505, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001985, mae: 0.720370, mean_q: 0.964634, mean_eps: 0.291705\n",
            "  787750/1750000: episode: 2343, duration: 13.243s, episode steps: 502, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.002369, mae: 0.724628, mean_q: 0.971955, mean_eps: 0.291252\n",
            "  788423/1750000: episode: 2344, duration: 17.606s, episode steps: 673, steps per second:  38, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.001982, mae: 0.720715, mean_q: 0.964891, mean_eps: 0.290723\n",
            "  789099/1750000: episode: 2345, duration: 17.716s, episode steps: 676, steps per second:  38, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001701, mae: 0.723330, mean_q: 0.969448, mean_eps: 0.290116\n",
            "  789700/1750000: episode: 2346, duration: 15.867s, episode steps: 601, steps per second:  38, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.002198, mae: 0.723738, mean_q: 0.969929, mean_eps: 0.289542\n",
            "  790355/1750000: episode: 2347, duration: 17.219s, episode steps: 655, steps per second:  38, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.001634, mae: 0.731547, mean_q: 0.980378, mean_eps: 0.288977\n",
            "  791099/1750000: episode: 2348, duration: 19.309s, episode steps: 744, steps per second:  39, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.002041, mae: 0.732204, mean_q: 0.980046, mean_eps: 0.288347\n",
            "  791638/1750000: episode: 2349, duration: 14.101s, episode steps: 539, steps per second:  38, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001863, mae: 0.736074, mean_q: 0.986026, mean_eps: 0.287769\n",
            "  792131/1750000: episode: 2350, duration: 12.923s, episode steps: 493, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002002, mae: 0.734124, mean_q: 0.983664, mean_eps: 0.287304\n",
            "  792718/1750000: episode: 2351, duration: 15.375s, episode steps: 587, steps per second:  38, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001961, mae: 0.733162, mean_q: 0.982808, mean_eps: 0.286818\n",
            "  793289/1750000: episode: 2352, duration: 15.102s, episode steps: 571, steps per second:  38, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002222, mae: 0.733480, mean_q: 0.983729, mean_eps: 0.286296\n",
            "  793730/1750000: episode: 2353, duration: 11.666s, episode steps: 441, steps per second:  38, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 0.001685, mae: 0.735544, mean_q: 0.984917, mean_eps: 0.285841\n",
            "  794254/1750000: episode: 2354, duration: 13.808s, episode steps: 524, steps per second:  38, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.002158, mae: 0.739747, mean_q: 0.991431, mean_eps: 0.285407\n",
            "  795034/1750000: episode: 2355, duration: 20.416s, episode steps: 780, steps per second:  38, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.001821, mae: 0.741654, mean_q: 0.993684, mean_eps: 0.284820\n",
            "  795693/1750000: episode: 2356, duration: 17.442s, episode steps: 659, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001810, mae: 0.740893, mean_q: 0.991955, mean_eps: 0.284172\n",
            "  796364/1750000: episode: 2357, duration: 17.681s, episode steps: 671, steps per second:  38, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.001697, mae: 0.743666, mean_q: 0.995506, mean_eps: 0.283575\n",
            "  797030/1750000: episode: 2358, duration: 17.629s, episode steps: 666, steps per second:  38, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.002042, mae: 0.737442, mean_q: 0.986755, mean_eps: 0.282974\n",
            "  797627/1750000: episode: 2359, duration: 15.786s, episode steps: 597, steps per second:  38, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.001728, mae: 0.738879, mean_q: 0.989975, mean_eps: 0.282405\n",
            "  798135/1750000: episode: 2360, duration: 13.426s, episode steps: 508, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.217 [0.000, 3.000],  loss: 0.001584, mae: 0.734088, mean_q: 0.982246, mean_eps: 0.281908\n",
            "  798807/1750000: episode: 2361, duration: 17.611s, episode steps: 672, steps per second:  38, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.001996, mae: 0.732380, mean_q: 0.981193, mean_eps: 0.281377\n",
            "  799464/1750000: episode: 2362, duration: 17.400s, episode steps: 657, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002034, mae: 0.740216, mean_q: 0.989991, mean_eps: 0.280779\n",
            "  800240/1750000: episode: 2363, duration: 20.689s, episode steps: 776, steps per second:  38, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.405 [0.000, 3.000],  loss: 0.001724, mae: 0.742709, mean_q: 0.994836, mean_eps: 0.280135\n",
            "  800752/1750000: episode: 2364, duration: 13.688s, episode steps: 512, steps per second:  37, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001940, mae: 0.737429, mean_q: 0.987269, mean_eps: 0.279555\n",
            "  801612/1750000: episode: 2365, duration: 22.644s, episode steps: 860, steps per second:  38, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.330 [0.000, 3.000],  loss: 0.001954, mae: 0.748814, mean_q: 1.002195, mean_eps: 0.278938\n",
            "  801969/1750000: episode: 2366, duration: 9.604s, episode steps: 357, steps per second:  37, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.001829, mae: 0.748008, mean_q: 1.001329, mean_eps: 0.278389\n",
            "  802817/1750000: episode: 2367, duration: 22.546s, episode steps: 848, steps per second:  38, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001906, mae: 0.747420, mean_q: 1.000454, mean_eps: 0.277845\n",
            "  803263/1750000: episode: 2368, duration: 11.802s, episode steps: 446, steps per second:  38, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.309 [0.000, 3.000],  loss: 0.001914, mae: 0.749899, mean_q: 1.004664, mean_eps: 0.277264\n",
            "  804005/1750000: episode: 2369, duration: 19.637s, episode steps: 742, steps per second:  38, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.830 [0.000, 3.000],  loss: 0.001866, mae: 0.751629, mean_q: 1.005991, mean_eps: 0.276729\n",
            "  804627/1750000: episode: 2370, duration: 16.398s, episode steps: 622, steps per second:  38, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.002292, mae: 0.750876, mean_q: 1.004744, mean_eps: 0.276116\n",
            "  805367/1750000: episode: 2371, duration: 19.419s, episode steps: 740, steps per second:  38, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002288, mae: 0.750960, mean_q: 1.004505, mean_eps: 0.275504\n",
            "  806062/1750000: episode: 2372, duration: 18.506s, episode steps: 695, steps per second:  38, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.858 [0.000, 3.000],  loss: 0.001814, mae: 0.748465, mean_q: 1.001192, mean_eps: 0.274857\n",
            "  806713/1750000: episode: 2373, duration: 17.235s, episode steps: 651, steps per second:  38, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002289, mae: 0.748479, mean_q: 1.000405, mean_eps: 0.274251\n",
            "  807277/1750000: episode: 2374, duration: 14.969s, episode steps: 564, steps per second:  38, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.002175, mae: 0.749664, mean_q: 1.004470, mean_eps: 0.273704\n",
            "  808125/1750000: episode: 2375, duration: 22.330s, episode steps: 848, steps per second:  38, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002113, mae: 0.750749, mean_q: 1.003526, mean_eps: 0.273068\n",
            "  808940/1750000: episode: 2376, duration: 21.739s, episode steps: 815, steps per second:  37, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001975, mae: 0.753678, mean_q: 1.007520, mean_eps: 0.272321\n",
            "  809616/1750000: episode: 2377, duration: 18.172s, episode steps: 676, steps per second:  37, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.002216, mae: 0.742469, mean_q: 0.992483, mean_eps: 0.271652\n",
            "  810387/1750000: episode: 2378, duration: 20.305s, episode steps: 771, steps per second:  38, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001567, mae: 0.752782, mean_q: 1.007131, mean_eps: 0.271000\n",
            "  811175/1750000: episode: 2379, duration: 20.981s, episode steps: 788, steps per second:  38, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002177, mae: 0.751738, mean_q: 1.004513, mean_eps: 0.270298\n",
            "  811727/1750000: episode: 2380, duration: 14.769s, episode steps: 552, steps per second:  37, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002372, mae: 0.745083, mean_q: 0.995847, mean_eps: 0.269695\n",
            "  812393/1750000: episode: 2381, duration: 17.741s, episode steps: 666, steps per second:  38, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001960, mae: 0.752784, mean_q: 1.006479, mean_eps: 0.269146\n",
            "  813058/1750000: episode: 2382, duration: 17.375s, episode steps: 665, steps per second:  38, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001591, mae: 0.751604, mean_q: 1.006787, mean_eps: 0.268547\n",
            "  813967/1750000: episode: 2383, duration: 23.976s, episode steps: 909, steps per second:  38, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.001775, mae: 0.749981, mean_q: 1.003620, mean_eps: 0.267839\n",
            "  814662/1750000: episode: 2384, duration: 18.599s, episode steps: 695, steps per second:  37, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002500, mae: 0.749225, mean_q: 1.001914, mean_eps: 0.267117\n",
            "  815316/1750000: episode: 2385, duration: 17.513s, episode steps: 654, steps per second:  37, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.309 [0.000, 3.000],  loss: 0.002740, mae: 0.750577, mean_q: 1.003771, mean_eps: 0.266511\n",
            "  816017/1750000: episode: 2386, duration: 18.699s, episode steps: 701, steps per second:  37, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002306, mae: 0.753146, mean_q: 1.007503, mean_eps: 0.265901\n",
            "  816636/1750000: episode: 2387, duration: 16.540s, episode steps: 619, steps per second:  37, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.002180, mae: 0.750386, mean_q: 1.004531, mean_eps: 0.265307\n",
            "  817251/1750000: episode: 2388, duration: 16.290s, episode steps: 615, steps per second:  38, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.002252, mae: 0.752257, mean_q: 1.006740, mean_eps: 0.264752\n",
            "  817905/1750000: episode: 2389, duration: 17.455s, episode steps: 654, steps per second:  37, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001657, mae: 0.742634, mean_q: 0.993553, mean_eps: 0.264180\n",
            "  818483/1750000: episode: 2390, duration: 15.320s, episode steps: 578, steps per second:  38, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.001777, mae: 0.753135, mean_q: 1.007382, mean_eps: 0.263625\n",
            "  819179/1750000: episode: 2391, duration: 18.525s, episode steps: 696, steps per second:  38, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001711, mae: 0.751018, mean_q: 1.004823, mean_eps: 0.263053\n",
            "  819735/1750000: episode: 2392, duration: 14.742s, episode steps: 556, steps per second:  38, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 0.002241, mae: 0.750663, mean_q: 1.004029, mean_eps: 0.262490\n",
            "  820381/1750000: episode: 2393, duration: 17.313s, episode steps: 646, steps per second:  37, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.001871, mae: 0.760926, mean_q: 1.018214, mean_eps: 0.261948\n",
            "  820662/1750000: episode: 2394, duration: 7.438s, episode steps: 281, steps per second:  38, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001895, mae: 0.757868, mean_q: 1.015306, mean_eps: 0.261530\n",
            "  821361/1750000: episode: 2395, duration: 18.570s, episode steps: 699, steps per second:  38, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.001795, mae: 0.763534, mean_q: 1.022387, mean_eps: 0.261089\n",
            "  821874/1750000: episode: 2396, duration: 13.672s, episode steps: 513, steps per second:  38, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.797 [0.000, 3.000],  loss: 0.002001, mae: 0.760557, mean_q: 1.018666, mean_eps: 0.260544\n",
            "  822408/1750000: episode: 2397, duration: 14.428s, episode steps: 534, steps per second:  37, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.002746, mae: 0.755351, mean_q: 1.009809, mean_eps: 0.260074\n",
            "  823135/1750000: episode: 2398, duration: 19.562s, episode steps: 727, steps per second:  37, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002010, mae: 0.763723, mean_q: 1.021275, mean_eps: 0.259507\n",
            "  823882/1750000: episode: 2399, duration: 19.874s, episode steps: 747, steps per second:  38, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.001917, mae: 0.764819, mean_q: 1.023389, mean_eps: 0.258843\n",
            "  824421/1750000: episode: 2400, duration: 14.543s, episode steps: 539, steps per second:  37, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002332, mae: 0.764140, mean_q: 1.022018, mean_eps: 0.258263\n",
            "  825156/1750000: episode: 2401, duration: 19.566s, episode steps: 735, steps per second:  38, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002184, mae: 0.760114, mean_q: 1.015468, mean_eps: 0.257691\n",
            "  825902/1750000: episode: 2402, duration: 20.037s, episode steps: 746, steps per second:  37, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002175, mae: 0.761013, mean_q: 1.018410, mean_eps: 0.257025\n",
            "  826744/1750000: episode: 2403, duration: 22.516s, episode steps: 842, steps per second:  37, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.002351, mae: 0.757516, mean_q: 1.013509, mean_eps: 0.256310\n",
            "  827275/1750000: episode: 2404, duration: 14.270s, episode steps: 531, steps per second:  37, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002506, mae: 0.759330, mean_q: 1.015387, mean_eps: 0.255693\n",
            "  827788/1750000: episode: 2405, duration: 13.895s, episode steps: 513, steps per second:  37, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.860 [0.000, 3.000],  loss: 0.002305, mae: 0.765375, mean_q: 1.023344, mean_eps: 0.255223\n",
            "  828461/1750000: episode: 2406, duration: 18.114s, episode steps: 673, steps per second:  37, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001895, mae: 0.762728, mean_q: 1.019516, mean_eps: 0.254688\n",
            "  829081/1750000: episode: 2407, duration: 16.631s, episode steps: 620, steps per second:  37, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002593, mae: 0.761752, mean_q: 1.019142, mean_eps: 0.254105\n",
            "  829762/1750000: episode: 2408, duration: 18.190s, episode steps: 681, steps per second:  37, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.002309, mae: 0.758124, mean_q: 1.013747, mean_eps: 0.253520\n",
            "  830564/1750000: episode: 2409, duration: 21.498s, episode steps: 802, steps per second:  37, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.002246, mae: 0.766372, mean_q: 1.025050, mean_eps: 0.252854\n",
            "  831392/1750000: episode: 2410, duration: 22.218s, episode steps: 828, steps per second:  37, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.002516, mae: 0.772679, mean_q: 1.034058, mean_eps: 0.252122\n",
            "  832208/1750000: episode: 2411, duration: 22.084s, episode steps: 816, steps per second:  37, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002311, mae: 0.766722, mean_q: 1.025858, mean_eps: 0.251382\n",
            "  832825/1750000: episode: 2412, duration: 16.664s, episode steps: 617, steps per second:  37, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001998, mae: 0.773102, mean_q: 1.034609, mean_eps: 0.250736\n",
            "  833866/1750000: episode: 2413, duration: 27.512s, episode steps: 1041, steps per second:  38, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.001986, mae: 0.768308, mean_q: 1.028870, mean_eps: 0.249989\n",
            "  834449/1750000: episode: 2414, duration: 15.699s, episode steps: 583, steps per second:  37, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.870 [0.000, 3.000],  loss: 0.002166, mae: 0.769181, mean_q: 1.030372, mean_eps: 0.249258\n",
            "  835238/1750000: episode: 2415, duration: 21.136s, episode steps: 789, steps per second:  37, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.736 [0.000, 3.000],  loss: 0.002596, mae: 0.770047, mean_q: 1.030649, mean_eps: 0.248640\n",
            "  835881/1750000: episode: 2416, duration: 17.439s, episode steps: 643, steps per second:  37, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002318, mae: 0.775505, mean_q: 1.037197, mean_eps: 0.247996\n",
            "  836444/1750000: episode: 2417, duration: 15.172s, episode steps: 563, steps per second:  37, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002671, mae: 0.771160, mean_q: 1.031551, mean_eps: 0.247454\n",
            "  837171/1750000: episode: 2418, duration: 19.598s, episode steps: 727, steps per second:  37, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001810, mae: 0.771041, mean_q: 1.032854, mean_eps: 0.246875\n",
            "  837721/1750000: episode: 2419, duration: 14.907s, episode steps: 550, steps per second:  37, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.847 [0.000, 3.000],  loss: 0.001843, mae: 0.769632, mean_q: 1.030566, mean_eps: 0.246299\n",
            "  838570/1750000: episode: 2420, duration: 22.802s, episode steps: 849, steps per second:  37, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001916, mae: 0.775179, mean_q: 1.038228, mean_eps: 0.245669\n",
            "  839323/1750000: episode: 2421, duration: 20.424s, episode steps: 753, steps per second:  37, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002058, mae: 0.770775, mean_q: 1.032996, mean_eps: 0.244949\n",
            "  840109/1750000: episode: 2422, duration: 21.209s, episode steps: 786, steps per second:  37, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.990 [0.000, 3.000],  loss: 0.002383, mae: 0.774190, mean_q: 1.036790, mean_eps: 0.244256\n",
            "  840729/1750000: episode: 2423, duration: 16.558s, episode steps: 620, steps per second:  37, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002557, mae: 0.784130, mean_q: 1.049109, mean_eps: 0.243622\n",
            "  841176/1750000: episode: 2424, duration: 12.113s, episode steps: 447, steps per second:  37, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002564, mae: 0.785484, mean_q: 1.051729, mean_eps: 0.243143\n",
            "  841835/1750000: episode: 2425, duration: 17.951s, episode steps: 659, steps per second:  37, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002451, mae: 0.790595, mean_q: 1.058010, mean_eps: 0.242646\n",
            "  842344/1750000: episode: 2426, duration: 14.058s, episode steps: 509, steps per second:  36, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.002169, mae: 0.783061, mean_q: 1.048509, mean_eps: 0.242121\n",
            "  842859/1750000: episode: 2427, duration: 13.967s, episode steps: 515, steps per second:  37, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001885, mae: 0.786705, mean_q: 1.053033, mean_eps: 0.241660\n",
            "  843553/1750000: episode: 2428, duration: 18.885s, episode steps: 694, steps per second:  37, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002455, mae: 0.785885, mean_q: 1.051367, mean_eps: 0.241115\n",
            "  844039/1750000: episode: 2429, duration: 13.167s, episode steps: 486, steps per second:  37, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001695, mae: 0.788604, mean_q: 1.056194, mean_eps: 0.240584\n",
            "  844715/1750000: episode: 2430, duration: 18.270s, episode steps: 676, steps per second:  37, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002392, mae: 0.779447, mean_q: 1.043411, mean_eps: 0.240062\n",
            "  845478/1750000: episode: 2431, duration: 20.579s, episode steps: 763, steps per second:  37, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.002651, mae: 0.790309, mean_q: 1.056346, mean_eps: 0.239414\n",
            "  846242/1750000: episode: 2432, duration: 20.482s, episode steps: 764, steps per second:  37, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.001948, mae: 0.786311, mean_q: 1.051710, mean_eps: 0.238726\n",
            "  846950/1750000: episode: 2433, duration: 19.125s, episode steps: 708, steps per second:  37, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001646, mae: 0.787660, mean_q: 1.053374, mean_eps: 0.238064\n",
            "  847774/1750000: episode: 2434, duration: 22.288s, episode steps: 824, steps per second:  37, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.902 [0.000, 3.000],  loss: 0.002085, mae: 0.787291, mean_q: 1.053720, mean_eps: 0.237374\n",
            "  848548/1750000: episode: 2435, duration: 21.166s, episode steps: 774, steps per second:  37, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.944 [0.000, 3.000],  loss: 0.001992, mae: 0.785423, mean_q: 1.050510, mean_eps: 0.236656\n",
            "  849377/1750000: episode: 2436, duration: 22.637s, episode steps: 829, steps per second:  37, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.934 [0.000, 3.000],  loss: 0.002380, mae: 0.788504, mean_q: 1.054905, mean_eps: 0.235934\n",
            "  850228/1750000: episode: 2437, duration: 23.059s, episode steps: 851, steps per second:  37, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.787 [0.000, 3.000],  loss: 0.002443, mae: 0.787710, mean_q: 1.054331, mean_eps: 0.235178\n",
            "  850931/1750000: episode: 2438, duration: 19.028s, episode steps: 703, steps per second:  37, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002256, mae: 0.787960, mean_q: 1.056012, mean_eps: 0.234480\n",
            "  851698/1750000: episode: 2439, duration: 20.996s, episode steps: 767, steps per second:  37, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.958 [0.000, 3.000],  loss: 0.001849, mae: 0.789964, mean_q: 1.058159, mean_eps: 0.233817\n",
            "  852359/1750000: episode: 2440, duration: 17.956s, episode steps: 661, steps per second:  37, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.823 [0.000, 3.000],  loss: 0.001850, mae: 0.787669, mean_q: 1.054826, mean_eps: 0.233175\n",
            "  853058/1750000: episode: 2441, duration: 18.924s, episode steps: 699, steps per second:  37, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.104 [0.000, 3.000],  loss: 0.002268, mae: 0.788439, mean_q: 1.055793, mean_eps: 0.232563\n",
            "  853848/1750000: episode: 2442, duration: 21.222s, episode steps: 790, steps per second:  37, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002239, mae: 0.788663, mean_q: 1.055700, mean_eps: 0.231893\n",
            "  854626/1750000: episode: 2443, duration: 21.191s, episode steps: 778, steps per second:  37, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002115, mae: 0.788316, mean_q: 1.055979, mean_eps: 0.231188\n",
            "  855280/1750000: episode: 2444, duration: 18.013s, episode steps: 654, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002126, mae: 0.788229, mean_q: 1.054505, mean_eps: 0.230543\n",
            "  855960/1750000: episode: 2445, duration: 18.589s, episode steps: 680, steps per second:  37, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002236, mae: 0.790217, mean_q: 1.057446, mean_eps: 0.229944\n",
            "  856797/1750000: episode: 2446, duration: 22.735s, episode steps: 837, steps per second:  37, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002507, mae: 0.789258, mean_q: 1.056687, mean_eps: 0.229260\n",
            "  857588/1750000: episode: 2447, duration: 21.742s, episode steps: 791, steps per second:  36, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.002528, mae: 0.788317, mean_q: 1.055661, mean_eps: 0.228527\n",
            "  858313/1750000: episode: 2448, duration: 19.918s, episode steps: 725, steps per second:  36, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.002204, mae: 0.792260, mean_q: 1.060874, mean_eps: 0.227845\n",
            "  858798/1750000: episode: 2449, duration: 12.985s, episode steps: 485, steps per second:  37, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.002605, mae: 0.791289, mean_q: 1.060008, mean_eps: 0.227300\n",
            "  859656/1750000: episode: 2450, duration: 23.426s, episode steps: 858, steps per second:  37, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002041, mae: 0.788613, mean_q: 1.056383, mean_eps: 0.226697\n",
            "  860374/1750000: episode: 2451, duration: 19.787s, episode steps: 718, steps per second:  36, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002721, mae: 0.797770, mean_q: 1.067347, mean_eps: 0.225987\n",
            "  861191/1750000: episode: 2452, duration: 22.269s, episode steps: 817, steps per second:  37, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002346, mae: 0.791144, mean_q: 1.058066, mean_eps: 0.225296\n",
            "  862035/1750000: episode: 2453, duration: 23.049s, episode steps: 844, steps per second:  37, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002132, mae: 0.798084, mean_q: 1.067488, mean_eps: 0.224549\n",
            "  862746/1750000: episode: 2454, duration: 19.371s, episode steps: 711, steps per second:  37, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.001947, mae: 0.794099, mean_q: 1.063430, mean_eps: 0.223849\n",
            "  863373/1750000: episode: 2455, duration: 17.327s, episode steps: 627, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.001810, mae: 0.789400, mean_q: 1.057786, mean_eps: 0.223246\n",
            "  864166/1750000: episode: 2456, duration: 21.600s, episode steps: 793, steps per second:  37, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002254, mae: 0.793604, mean_q: 1.063163, mean_eps: 0.222607\n",
            "  864825/1750000: episode: 2457, duration: 18.052s, episode steps: 659, steps per second:  37, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001423, mae: 0.798736, mean_q: 1.071138, mean_eps: 0.221954\n",
            "  865744/1750000: episode: 2458, duration: 25.302s, episode steps: 919, steps per second:  36, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.001765, mae: 0.788972, mean_q: 1.056833, mean_eps: 0.221244\n",
            "  866452/1750000: episode: 2459, duration: 19.764s, episode steps: 708, steps per second:  36, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001957, mae: 0.791461, mean_q: 1.061127, mean_eps: 0.220514\n",
            "  867527/1750000: episode: 2460, duration: 29.543s, episode steps: 1075, steps per second:  36, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.002234, mae: 0.791343, mean_q: 1.060899, mean_eps: 0.219711\n",
            "  868095/1750000: episode: 2461, duration: 15.650s, episode steps: 568, steps per second:  36, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.002601, mae: 0.796312, mean_q: 1.066551, mean_eps: 0.218971\n",
            "  869092/1750000: episode: 2462, duration: 27.421s, episode steps: 997, steps per second:  36, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002856, mae: 0.791042, mean_q: 1.059742, mean_eps: 0.218267\n",
            "  869829/1750000: episode: 2463, duration: 20.253s, episode steps: 737, steps per second:  36, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.293 [0.000, 3.000],  loss: 0.002130, mae: 0.803636, mean_q: 1.076414, mean_eps: 0.217486\n",
            "  870640/1750000: episode: 2464, duration: 22.149s, episode steps: 811, steps per second:  37, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002171, mae: 0.800875, mean_q: 1.072586, mean_eps: 0.216789\n",
            "  871437/1750000: episode: 2465, duration: 21.821s, episode steps: 797, steps per second:  37, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002061, mae: 0.805359, mean_q: 1.077968, mean_eps: 0.216066\n",
            "  872178/1750000: episode: 2466, duration: 20.425s, episode steps: 741, steps per second:  36, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002529, mae: 0.814357, mean_q: 1.089335, mean_eps: 0.215373\n",
            "  872782/1750000: episode: 2467, duration: 16.575s, episode steps: 604, steps per second:  36, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002190, mae: 0.802482, mean_q: 1.074852, mean_eps: 0.214768\n",
            "  873735/1750000: episode: 2468, duration: 25.858s, episode steps: 953, steps per second:  37, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.001882, mae: 0.807063, mean_q: 1.082393, mean_eps: 0.214068\n",
            "  874393/1750000: episode: 2469, duration: 18.028s, episode steps: 658, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002031, mae: 0.809918, mean_q: 1.085528, mean_eps: 0.213342\n",
            "  875023/1750000: episode: 2470, duration: 17.320s, episode steps: 630, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002647, mae: 0.813417, mean_q: 1.089924, mean_eps: 0.212763\n",
            "  875849/1750000: episode: 2471, duration: 22.862s, episode steps: 826, steps per second:  36, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002374, mae: 0.808390, mean_q: 1.083835, mean_eps: 0.212108\n",
            "  876484/1750000: episode: 2472, duration: 17.515s, episode steps: 635, steps per second:  36, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002512, mae: 0.811830, mean_q: 1.087983, mean_eps: 0.211451\n",
            "  877252/1750000: episode: 2473, duration: 21.296s, episode steps: 768, steps per second:  36, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001988, mae: 0.806747, mean_q: 1.082138, mean_eps: 0.210821\n",
            "  877746/1750000: episode: 2474, duration: 13.855s, episode steps: 494, steps per second:  36, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.001741, mae: 0.815193, mean_q: 1.092274, mean_eps: 0.210252\n",
            "  878465/1750000: episode: 2475, duration: 19.700s, episode steps: 719, steps per second:  36, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.771 [0.000, 3.000],  loss: 0.002071, mae: 0.810257, mean_q: 1.085396, mean_eps: 0.209705\n",
            "  879328/1750000: episode: 2476, duration: 23.764s, episode steps: 863, steps per second:  36, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002155, mae: 0.807728, mean_q: 1.081718, mean_eps: 0.208994\n",
            "  879923/1750000: episode: 2477, duration: 16.460s, episode steps: 595, steps per second:  36, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002485, mae: 0.800523, mean_q: 1.074353, mean_eps: 0.208338\n",
            "  880550/1750000: episode: 2478, duration: 17.430s, episode steps: 627, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002113, mae: 0.821956, mean_q: 1.102358, mean_eps: 0.207788\n",
            "  881338/1750000: episode: 2479, duration: 21.747s, episode steps: 788, steps per second:  36, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002642, mae: 0.818432, mean_q: 1.096447, mean_eps: 0.207150\n",
            "  882061/1750000: episode: 2480, duration: 19.964s, episode steps: 723, steps per second:  36, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.898 [0.000, 3.000],  loss: 0.002159, mae: 0.822895, mean_q: 1.101806, mean_eps: 0.206470\n",
            "  882507/1750000: episode: 2481, duration: 12.277s, episode steps: 446, steps per second:  36, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002711, mae: 0.813684, mean_q: 1.091033, mean_eps: 0.205944\n",
            "  883161/1750000: episode: 2482, duration: 18.093s, episode steps: 654, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002629, mae: 0.820784, mean_q: 1.100637, mean_eps: 0.205449\n",
            "  884037/1750000: episode: 2483, duration: 24.338s, episode steps: 876, steps per second:  36, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.002118, mae: 0.819759, mean_q: 1.099927, mean_eps: 0.204760\n",
            "  884841/1750000: episode: 2484, duration: 21.903s, episode steps: 804, steps per second:  37, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.002559, mae: 0.817266, mean_q: 1.096406, mean_eps: 0.204004\n",
            "  885423/1750000: episode: 2485, duration: 15.988s, episode steps: 582, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002612, mae: 0.820141, mean_q: 1.099722, mean_eps: 0.203381\n",
            "  885887/1750000: episode: 2486, duration: 12.790s, episode steps: 464, steps per second:  36, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.909 [0.000, 3.000],  loss: 0.002280, mae: 0.812664, mean_q: 1.089284, mean_eps: 0.202911\n",
            "  886562/1750000: episode: 2487, duration: 18.736s, episode steps: 675, steps per second:  36, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.002892, mae: 0.819696, mean_q: 1.097417, mean_eps: 0.202398\n",
            "  887259/1750000: episode: 2488, duration: 19.320s, episode steps: 697, steps per second:  36, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.002124, mae: 0.823265, mean_q: 1.103471, mean_eps: 0.201781\n",
            "  887770/1750000: episode: 2489, duration: 14.166s, episode steps: 511, steps per second:  36, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002372, mae: 0.821907, mean_q: 1.102551, mean_eps: 0.201237\n",
            "  888362/1750000: episode: 2490, duration: 16.349s, episode steps: 592, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.002472, mae: 0.822584, mean_q: 1.103848, mean_eps: 0.200741\n",
            "  889140/1750000: episode: 2491, duration: 21.653s, episode steps: 778, steps per second:  36, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.002194, mae: 0.819556, mean_q: 1.097870, mean_eps: 0.200125\n",
            "  889790/1750000: episode: 2492, duration: 18.110s, episode steps: 650, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002182, mae: 0.818628, mean_q: 1.097770, mean_eps: 0.199482\n",
            "  890759/1750000: episode: 2493, duration: 26.776s, episode steps: 969, steps per second:  36, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.001809, mae: 0.827885, mean_q: 1.109803, mean_eps: 0.198753\n",
            "  891401/1750000: episode: 2494, duration: 17.789s, episode steps: 642, steps per second:  36, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002677, mae: 0.827626, mean_q: 1.108357, mean_eps: 0.198028\n",
            "  892120/1750000: episode: 2495, duration: 19.978s, episode steps: 719, steps per second:  36, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002564, mae: 0.832616, mean_q: 1.115557, mean_eps: 0.197416\n",
            "  892792/1750000: episode: 2496, duration: 18.683s, episode steps: 672, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002616, mae: 0.829850, mean_q: 1.111551, mean_eps: 0.196791\n",
            "  893797/1750000: episode: 2497, duration: 27.868s, episode steps: 1005, steps per second:  36, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.038 [0.000, 3.000],  loss: 0.001927, mae: 0.832065, mean_q: 1.115224, mean_eps: 0.196035\n",
            "  894433/1750000: episode: 2498, duration: 17.729s, episode steps: 636, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002303, mae: 0.829238, mean_q: 1.111223, mean_eps: 0.195296\n",
            "  894967/1750000: episode: 2499, duration: 14.819s, episode steps: 534, steps per second:  36, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002560, mae: 0.827925, mean_q: 1.109494, mean_eps: 0.194770\n",
            "  895557/1750000: episode: 2500, duration: 16.368s, episode steps: 590, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002644, mae: 0.830464, mean_q: 1.112124, mean_eps: 0.194264\n",
            "  896288/1750000: episode: 2501, duration: 20.228s, episode steps: 731, steps per second:  36, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002537, mae: 0.826492, mean_q: 1.108104, mean_eps: 0.193670\n",
            "  897133/1750000: episode: 2502, duration: 23.647s, episode steps: 845, steps per second:  36, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002817, mae: 0.834416, mean_q: 1.118392, mean_eps: 0.192961\n",
            "  897737/1750000: episode: 2503, duration: 16.845s, episode steps: 604, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002303, mae: 0.832105, mean_q: 1.115877, mean_eps: 0.192308\n",
            "  898183/1750000: episode: 2504, duration: 12.394s, episode steps: 446, steps per second:  36, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.872 [0.000, 3.000],  loss: 0.001974, mae: 0.832376, mean_q: 1.117459, mean_eps: 0.191836\n",
            "  898846/1750000: episode: 2505, duration: 18.195s, episode steps: 663, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.002649, mae: 0.830285, mean_q: 1.111938, mean_eps: 0.191337\n",
            "  899322/1750000: episode: 2506, duration: 13.162s, episode steps: 476, steps per second:  36, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002312, mae: 0.829524, mean_q: 1.112222, mean_eps: 0.190824\n",
            "  900033/1750000: episode: 2507, duration: 19.900s, episode steps: 711, steps per second:  36, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.002691, mae: 0.831317, mean_q: 1.113365, mean_eps: 0.190290\n",
            "  900418/1750000: episode: 2508, duration: 10.627s, episode steps: 385, steps per second:  36, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.002386, mae: 0.835077, mean_q: 1.118040, mean_eps: 0.189797\n",
            "  901004/1750000: episode: 2509, duration: 16.401s, episode steps: 586, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.957 [0.000, 3.000],  loss: 0.002107, mae: 0.834482, mean_q: 1.117675, mean_eps: 0.189361\n",
            "  901742/1750000: episode: 2510, duration: 20.761s, episode steps: 738, steps per second:  36, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.002529, mae: 0.832101, mean_q: 1.115556, mean_eps: 0.188765\n",
            "  902288/1750000: episode: 2511, duration: 15.133s, episode steps: 546, steps per second:  36, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.002128, mae: 0.833898, mean_q: 1.116990, mean_eps: 0.188187\n",
            "  903179/1750000: episode: 2512, duration: 24.811s, episode steps: 891, steps per second:  36, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.001876, mae: 0.830726, mean_q: 1.111284, mean_eps: 0.187541\n",
            "  903969/1750000: episode: 2513, duration: 22.218s, episode steps: 790, steps per second:  36, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001865, mae: 0.831871, mean_q: 1.114633, mean_eps: 0.186783\n",
            "  904484/1750000: episode: 2514, duration: 14.351s, episode steps: 515, steps per second:  36, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002363, mae: 0.832107, mean_q: 1.115331, mean_eps: 0.186197\n",
            "  904995/1750000: episode: 2515, duration: 14.291s, episode steps: 511, steps per second:  36, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.002333, mae: 0.830886, mean_q: 1.114923, mean_eps: 0.185736\n",
            "  905685/1750000: episode: 2516, duration: 19.127s, episode steps: 690, steps per second:  36, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002614, mae: 0.832710, mean_q: 1.116127, mean_eps: 0.185194\n",
            "  906499/1750000: episode: 2517, duration: 22.637s, episode steps: 814, steps per second:  36, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.343 [0.000, 3.000],  loss: 0.002478, mae: 0.836234, mean_q: 1.119605, mean_eps: 0.184517\n",
            "  907368/1750000: episode: 2518, duration: 24.495s, episode steps: 869, steps per second:  35, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.002234, mae: 0.837136, mean_q: 1.122783, mean_eps: 0.183761\n",
            "  907993/1750000: episode: 2519, duration: 17.597s, episode steps: 625, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002444, mae: 0.833082, mean_q: 1.117226, mean_eps: 0.183088\n",
            "  908680/1750000: episode: 2520, duration: 19.438s, episode steps: 687, steps per second:  35, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002217, mae: 0.832473, mean_q: 1.114300, mean_eps: 0.182498\n",
            "  909168/1750000: episode: 2521, duration: 13.847s, episode steps: 488, steps per second:  35, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.914 [0.000, 3.000],  loss: 0.001984, mae: 0.832042, mean_q: 1.114393, mean_eps: 0.181970\n",
            "  909749/1750000: episode: 2522, duration: 16.407s, episode steps: 581, steps per second:  35, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.002286, mae: 0.830578, mean_q: 1.113379, mean_eps: 0.181488\n",
            "  910486/1750000: episode: 2523, duration: 21.053s, episode steps: 737, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002569, mae: 0.833001, mean_q: 1.115498, mean_eps: 0.180894\n",
            "  911390/1750000: episode: 2524, duration: 25.853s, episode steps: 904, steps per second:  35, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002437, mae: 0.839512, mean_q: 1.124792, mean_eps: 0.180156\n",
            "  911983/1750000: episode: 2525, duration: 16.611s, episode steps: 593, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.863 [0.000, 3.000],  loss: 0.002981, mae: 0.839981, mean_q: 1.124764, mean_eps: 0.179483\n",
            "  912828/1750000: episode: 2526, duration: 23.912s, episode steps: 845, steps per second:  35, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.001961, mae: 0.839905, mean_q: 1.125824, mean_eps: 0.178836\n",
            "  913465/1750000: episode: 2527, duration: 17.935s, episode steps: 637, steps per second:  36, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002687, mae: 0.834130, mean_q: 1.116715, mean_eps: 0.178169\n",
            "  914168/1750000: episode: 2528, duration: 19.683s, episode steps: 703, steps per second:  36, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.002116, mae: 0.838508, mean_q: 1.124179, mean_eps: 0.177566\n",
            "  914553/1750000: episode: 2529, duration: 11.103s, episode steps: 385, steps per second:  35, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002618, mae: 0.833595, mean_q: 1.117828, mean_eps: 0.177076\n",
            "  915221/1750000: episode: 2530, duration: 18.754s, episode steps: 668, steps per second:  36, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.002170, mae: 0.844013, mean_q: 1.131452, mean_eps: 0.176601\n",
            "  916185/1750000: episode: 2531, duration: 27.312s, episode steps: 964, steps per second:  35, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.002088, mae: 0.838310, mean_q: 1.124262, mean_eps: 0.175866\n",
            "  916983/1750000: episode: 2532, duration: 22.969s, episode steps: 798, steps per second:  35, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.771 [0.000, 3.000],  loss: 0.002433, mae: 0.838610, mean_q: 1.123249, mean_eps: 0.175074\n",
            "  917552/1750000: episode: 2533, duration: 16.545s, episode steps: 569, steps per second:  34, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002128, mae: 0.839775, mean_q: 1.124591, mean_eps: 0.174461\n",
            "  918292/1750000: episode: 2534, duration: 21.771s, episode steps: 740, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.002349, mae: 0.835209, mean_q: 1.118544, mean_eps: 0.173872\n",
            "  918884/1750000: episode: 2535, duration: 17.023s, episode steps: 592, steps per second:  35, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002509, mae: 0.828970, mean_q: 1.111740, mean_eps: 0.173273\n",
            "  919645/1750000: episode: 2536, duration: 21.526s, episode steps: 761, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002202, mae: 0.837445, mean_q: 1.122912, mean_eps: 0.172662\n",
            "  920674/1750000: episode: 2537, duration: 29.058s, episode steps: 1029, steps per second:  35, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.002086, mae: 0.843705, mean_q: 1.129119, mean_eps: 0.171856\n",
            "  921576/1750000: episode: 2538, duration: 25.295s, episode steps: 902, steps per second:  36, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.002265, mae: 0.852566, mean_q: 1.142277, mean_eps: 0.170988\n",
            "  922046/1750000: episode: 2539, duration: 13.319s, episode steps: 470, steps per second:  35, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.266 [0.000, 3.000],  loss: 0.002913, mae: 0.850000, mean_q: 1.138968, mean_eps: 0.170371\n",
            "  923160/1750000: episode: 2540, duration: 31.499s, episode steps: 1114, steps per second:  35, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.930 [0.000, 3.000],  loss: 0.002287, mae: 0.852646, mean_q: 1.143079, mean_eps: 0.169658\n",
            "  924049/1750000: episode: 2541, duration: 25.186s, episode steps: 889, steps per second:  35, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.953 [0.000, 3.000],  loss: 0.002291, mae: 0.846578, mean_q: 1.134741, mean_eps: 0.168756\n",
            "  925049/1750000: episode: 2542, duration: 28.257s, episode steps: 1000, steps per second:  35, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.950 [0.000, 3.000],  loss: 0.002413, mae: 0.848075, mean_q: 1.136229, mean_eps: 0.167905\n",
            "  925671/1750000: episode: 2543, duration: 17.472s, episode steps: 622, steps per second:  36, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002882, mae: 0.845703, mean_q: 1.131711, mean_eps: 0.167176\n",
            "  926752/1750000: episode: 2544, duration: 30.506s, episode steps: 1081, steps per second:  35, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.002154, mae: 0.850933, mean_q: 1.140421, mean_eps: 0.166411\n",
            "  927556/1750000: episode: 2545, duration: 22.879s, episode steps: 804, steps per second:  35, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002674, mae: 0.850055, mean_q: 1.138091, mean_eps: 0.165563\n",
            "  928461/1750000: episode: 2546, duration: 25.752s, episode steps: 905, steps per second:  35, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002243, mae: 0.849593, mean_q: 1.137480, mean_eps: 0.164793\n",
            "  929242/1750000: episode: 2547, duration: 21.952s, episode steps: 781, steps per second:  36, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002134, mae: 0.846152, mean_q: 1.133660, mean_eps: 0.164033\n",
            "  929986/1750000: episode: 2548, duration: 21.217s, episode steps: 744, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002132, mae: 0.845654, mean_q: 1.133416, mean_eps: 0.163347\n",
            "  930737/1750000: episode: 2549, duration: 21.263s, episode steps: 751, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.002463, mae: 0.859926, mean_q: 1.151707, mean_eps: 0.162674\n",
            "  931339/1750000: episode: 2550, duration: 16.919s, episode steps: 602, steps per second:  36, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.001781, mae: 0.856491, mean_q: 1.147771, mean_eps: 0.162066\n",
            "  932353/1750000: episode: 2551, duration: 28.509s, episode steps: 1014, steps per second:  36, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.002641, mae: 0.857508, mean_q: 1.148414, mean_eps: 0.161339\n",
            "  933277/1750000: episode: 2552, duration: 26.338s, episode steps: 924, steps per second:  35, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.823 [0.000, 3.000],  loss: 0.002713, mae: 0.858448, mean_q: 1.149324, mean_eps: 0.160466\n",
            "  933980/1750000: episode: 2553, duration: 19.904s, episode steps: 703, steps per second:  35, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.002901, mae: 0.859079, mean_q: 1.150358, mean_eps: 0.159735\n",
            "  934550/1750000: episode: 2554, duration: 16.199s, episode steps: 570, steps per second:  35, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001823, mae: 0.856873, mean_q: 1.148482, mean_eps: 0.159162\n",
            "  935342/1750000: episode: 2555, duration: 22.576s, episode steps: 792, steps per second:  35, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001950, mae: 0.856148, mean_q: 1.147406, mean_eps: 0.158549\n",
            "  936100/1750000: episode: 2556, duration: 21.359s, episode steps: 758, steps per second:  35, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.002560, mae: 0.858142, mean_q: 1.150571, mean_eps: 0.157852\n",
            "  937316/1750000: episode: 2557, duration: 34.368s, episode steps: 1216, steps per second:  35, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.843 [0.000, 3.000],  loss: 0.002354, mae: 0.859965, mean_q: 1.153554, mean_eps: 0.156965\n",
            "  938266/1750000: episode: 2558, duration: 27.149s, episode steps: 950, steps per second:  35, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.002332, mae: 0.860632, mean_q: 1.153047, mean_eps: 0.155989\n",
            "  939280/1750000: episode: 2559, duration: 28.854s, episode steps: 1014, steps per second:  35, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.002900, mae: 0.856173, mean_q: 1.147754, mean_eps: 0.155105\n",
            "  940247/1750000: episode: 2560, duration: 27.498s, episode steps: 967, steps per second:  35, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002562, mae: 0.858698, mean_q: 1.150268, mean_eps: 0.154214\n",
            "  941002/1750000: episode: 2561, duration: 21.500s, episode steps: 755, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002064, mae: 0.862349, mean_q: 1.155908, mean_eps: 0.153438\n",
            "  941774/1750000: episode: 2562, duration: 22.111s, episode steps: 772, steps per second:  35, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.002488, mae: 0.865186, mean_q: 1.158392, mean_eps: 0.152751\n",
            "  942215/1750000: episode: 2563, duration: 12.576s, episode steps: 441, steps per second:  35, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.839 [0.000, 3.000],  loss: 0.002153, mae: 0.873185, mean_q: 1.168954, mean_eps: 0.152205\n",
            "  942932/1750000: episode: 2564, duration: 20.454s, episode steps: 717, steps per second:  35, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.894 [0.000, 3.000],  loss: 0.002546, mae: 0.860697, mean_q: 1.152278, mean_eps: 0.151685\n",
            "  943489/1750000: episode: 2565, duration: 15.830s, episode steps: 557, steps per second:  35, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.925 [0.000, 3.000],  loss: 0.002446, mae: 0.859463, mean_q: 1.151443, mean_eps: 0.151111\n",
            "  944265/1750000: episode: 2566, duration: 22.221s, episode steps: 776, steps per second:  35, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.002464, mae: 0.861115, mean_q: 1.153907, mean_eps: 0.150510\n",
            "  944820/1750000: episode: 2567, duration: 15.879s, episode steps: 555, steps per second:  35, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.002132, mae: 0.863886, mean_q: 1.156706, mean_eps: 0.149912\n",
            "  945693/1750000: episode: 2568, duration: 24.842s, episode steps: 873, steps per second:  35, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002672, mae: 0.864610, mean_q: 1.156682, mean_eps: 0.149270\n",
            "  946451/1750000: episode: 2569, duration: 21.713s, episode steps: 758, steps per second:  35, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.908 [0.000, 3.000],  loss: 0.002412, mae: 0.859293, mean_q: 1.150458, mean_eps: 0.148535\n",
            "  947008/1750000: episode: 2570, duration: 16.188s, episode steps: 557, steps per second:  34, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001970, mae: 0.868702, mean_q: 1.163292, mean_eps: 0.147945\n",
            "  947848/1750000: episode: 2571, duration: 24.169s, episode steps: 840, steps per second:  35, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002997, mae: 0.863377, mean_q: 1.155408, mean_eps: 0.147317\n",
            "  948405/1750000: episode: 2572, duration: 15.946s, episode steps: 557, steps per second:  35, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002399, mae: 0.863483, mean_q: 1.156023, mean_eps: 0.146687\n",
            "  949573/1750000: episode: 2573, duration: 33.190s, episode steps: 1168, steps per second:  35, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.049 [0.000, 3.000],  loss: 0.001859, mae: 0.863093, mean_q: 1.155745, mean_eps: 0.145909\n",
            "  950210/1750000: episode: 2574, duration: 18.347s, episode steps: 637, steps per second:  35, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002453, mae: 0.862832, mean_q: 1.155007, mean_eps: 0.145097\n",
            "  950843/1750000: episode: 2575, duration: 18.268s, episode steps: 633, steps per second:  35, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002311, mae: 0.868627, mean_q: 1.163447, mean_eps: 0.144527\n",
            "  951520/1750000: episode: 2576, duration: 19.432s, episode steps: 677, steps per second:  35, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002390, mae: 0.869469, mean_q: 1.163494, mean_eps: 0.143938\n",
            "  952483/1750000: episode: 2577, duration: 27.684s, episode steps: 963, steps per second:  35, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002810, mae: 0.867403, mean_q: 1.160994, mean_eps: 0.143200\n",
            "  953083/1750000: episode: 2578, duration: 17.311s, episode steps: 600, steps per second:  35, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.002225, mae: 0.868093, mean_q: 1.162407, mean_eps: 0.142496\n",
            "  953750/1750000: episode: 2579, duration: 19.079s, episode steps: 667, steps per second:  35, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.828 [0.000, 3.000],  loss: 0.002023, mae: 0.869886, mean_q: 1.165458, mean_eps: 0.141926\n",
            "  954307/1750000: episode: 2580, duration: 15.970s, episode steps: 557, steps per second:  35, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.794 [0.000, 3.000],  loss: 0.002701, mae: 0.868339, mean_q: 1.161635, mean_eps: 0.141375\n",
            "  955106/1750000: episode: 2581, duration: 22.818s, episode steps: 799, steps per second:  35, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002189, mae: 0.867501, mean_q: 1.160854, mean_eps: 0.140765\n",
            "  955986/1750000: episode: 2582, duration: 25.361s, episode steps: 880, steps per second:  35, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002072, mae: 0.866886, mean_q: 1.160689, mean_eps: 0.140009\n",
            "  956844/1750000: episode: 2583, duration: 24.920s, episode steps: 858, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002397, mae: 0.870909, mean_q: 1.165792, mean_eps: 0.139227\n",
            "  958112/1750000: episode: 2584, duration: 37.048s, episode steps: 1268, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002138, mae: 0.870958, mean_q: 1.165544, mean_eps: 0.138272\n",
            "  958975/1750000: episode: 2585, duration: 25.378s, episode steps: 863, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002117, mae: 0.867357, mean_q: 1.161938, mean_eps: 0.137312\n",
            "  960128/1750000: episode: 2586, duration: 33.953s, episode steps: 1153, steps per second:  34, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.814 [0.000, 3.000],  loss: 0.002954, mae: 0.872232, mean_q: 1.168011, mean_eps: 0.136405\n",
            "  960930/1750000: episode: 2587, duration: 23.621s, episode steps: 802, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002552, mae: 0.870665, mean_q: 1.165348, mean_eps: 0.135525\n",
            "  961545/1750000: episode: 2588, duration: 18.207s, episode steps: 615, steps per second:  34, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001584, mae: 0.877215, mean_q: 1.174663, mean_eps: 0.134886\n",
            "  962266/1750000: episode: 2589, duration: 21.079s, episode steps: 721, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002699, mae: 0.871961, mean_q: 1.167887, mean_eps: 0.134285\n",
            "  963240/1750000: episode: 2590, duration: 28.633s, episode steps: 974, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002693, mae: 0.877252, mean_q: 1.174267, mean_eps: 0.133523\n",
            "  963838/1750000: episode: 2591, duration: 17.698s, episode steps: 598, steps per second:  34, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002251, mae: 0.873308, mean_q: 1.170848, mean_eps: 0.132816\n",
            "  964481/1750000: episode: 2592, duration: 18.929s, episode steps: 643, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002109, mae: 0.872823, mean_q: 1.169797, mean_eps: 0.132256\n",
            "  965174/1750000: episode: 2593, duration: 20.541s, episode steps: 693, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.002464, mae: 0.874167, mean_q: 1.170348, mean_eps: 0.131655\n",
            "  965924/1750000: episode: 2594, duration: 22.187s, episode steps: 750, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.855 [0.000, 3.000],  loss: 0.002156, mae: 0.873136, mean_q: 1.169182, mean_eps: 0.131007\n",
            "  967479/1750000: episode: 2595, duration: 45.772s, episode steps: 1555, steps per second:  34, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.908 [0.000, 3.000],  loss: 0.002404, mae: 0.876422, mean_q: 1.174098, mean_eps: 0.129970\n",
            "  968195/1750000: episode: 2596, duration: 21.040s, episode steps: 716, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001780, mae: 0.874845, mean_q: 1.170544, mean_eps: 0.128948\n",
            "  969178/1750000: episode: 2597, duration: 29.011s, episode steps: 983, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002255, mae: 0.873402, mean_q: 1.169324, mean_eps: 0.128183\n",
            "  969827/1750000: episode: 2598, duration: 19.223s, episode steps: 649, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.217 [0.000, 3.000],  loss: 0.002291, mae: 0.868968, mean_q: 1.163675, mean_eps: 0.127448\n",
            "  970665/1750000: episode: 2599, duration: 24.869s, episode steps: 838, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.305 [0.000, 3.000],  loss: 0.002586, mae: 0.884294, mean_q: 1.182532, mean_eps: 0.126779\n",
            "  971307/1750000: episode: 2600, duration: 18.851s, episode steps: 642, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.290 [0.000, 3.000],  loss: 0.002413, mae: 0.886474, mean_q: 1.186908, mean_eps: 0.126113\n",
            "  972261/1750000: episode: 2601, duration: 28.177s, episode steps: 954, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002367, mae: 0.888202, mean_q: 1.189867, mean_eps: 0.125394\n",
            "  972908/1750000: episode: 2602, duration: 19.149s, episode steps: 647, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002667, mae: 0.890564, mean_q: 1.191983, mean_eps: 0.124674\n",
            "  973821/1750000: episode: 2603, duration: 26.938s, episode steps: 913, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.002514, mae: 0.885171, mean_q: 1.186264, mean_eps: 0.123972\n",
            "  974531/1750000: episode: 2604, duration: 21.086s, episode steps: 710, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002885, mae: 0.885899, mean_q: 1.186651, mean_eps: 0.123242\n",
            "  975076/1750000: episode: 2605, duration: 16.324s, episode steps: 545, steps per second:  33, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.003053, mae: 0.887029, mean_q: 1.187806, mean_eps: 0.122678\n",
            "  975815/1750000: episode: 2606, duration: 21.805s, episode steps: 739, steps per second:  34, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.843 [0.000, 3.000],  loss: 0.002529, mae: 0.890775, mean_q: 1.194083, mean_eps: 0.122100\n",
            "  977259/1750000: episode: 2607, duration: 42.430s, episode steps: 1444, steps per second:  34, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.814 [0.000, 3.000],  loss: 0.002172, mae: 0.886160, mean_q: 1.187270, mean_eps: 0.121118\n",
            "  978104/1750000: episode: 2608, duration: 25.028s, episode steps: 845, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.003016, mae: 0.895684, mean_q: 1.199836, mean_eps: 0.120088\n",
            "  978571/1750000: episode: 2609, duration: 13.815s, episode steps: 467, steps per second:  34, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002651, mae: 0.880415, mean_q: 1.178697, mean_eps: 0.119498\n",
            "  979165/1750000: episode: 2610, duration: 17.605s, episode steps: 594, steps per second:  34, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002433, mae: 0.883225, mean_q: 1.182156, mean_eps: 0.119019\n",
            "  979749/1750000: episode: 2611, duration: 17.388s, episode steps: 584, steps per second:  34, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.002535, mae: 0.886264, mean_q: 1.187063, mean_eps: 0.118488\n",
            "  980714/1750000: episode: 2612, duration: 28.270s, episode steps: 965, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.002041, mae: 0.886539, mean_q: 1.188298, mean_eps: 0.117791\n",
            "  981806/1750000: episode: 2613, duration: 32.569s, episode steps: 1092, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002950, mae: 0.890233, mean_q: 1.192686, mean_eps: 0.116866\n",
            "  982427/1750000: episode: 2614, duration: 18.525s, episode steps: 621, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002950, mae: 0.891936, mean_q: 1.193680, mean_eps: 0.116096\n",
            "  983222/1750000: episode: 2615, duration: 23.680s, episode steps: 795, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002213, mae: 0.888749, mean_q: 1.189454, mean_eps: 0.115458\n",
            "  983983/1750000: episode: 2616, duration: 22.509s, episode steps: 761, steps per second:  34, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.002464, mae: 0.896976, mean_q: 1.201271, mean_eps: 0.114758\n",
            "  984718/1750000: episode: 2617, duration: 21.668s, episode steps: 735, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.002691, mae: 0.893600, mean_q: 1.195606, mean_eps: 0.114085\n",
            "  985376/1750000: episode: 2618, duration: 19.539s, episode steps: 658, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.002345, mae: 0.888471, mean_q: 1.189969, mean_eps: 0.113459\n",
            "  986313/1750000: episode: 2619, duration: 27.952s, episode steps: 937, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.855 [0.000, 3.000],  loss: 0.002510, mae: 0.889860, mean_q: 1.190781, mean_eps: 0.112740\n",
            "  986931/1750000: episode: 2620, duration: 18.382s, episode steps: 618, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001594, mae: 0.886233, mean_q: 1.187590, mean_eps: 0.112040\n",
            "  987603/1750000: episode: 2621, duration: 20.094s, episode steps: 672, steps per second:  33, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.002237, mae: 0.894634, mean_q: 1.198630, mean_eps: 0.111461\n",
            "  988422/1750000: episode: 2622, duration: 24.517s, episode steps: 819, steps per second:  33, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002241, mae: 0.887689, mean_q: 1.187837, mean_eps: 0.110789\n",
            "  989570/1750000: episode: 2623, duration: 34.283s, episode steps: 1148, steps per second:  33, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.002508, mae: 0.887769, mean_q: 1.188777, mean_eps: 0.109904\n",
            "  990470/1750000: episode: 2624, duration: 26.692s, episode steps: 900, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002544, mae: 0.891348, mean_q: 1.193032, mean_eps: 0.108982\n",
            "  991087/1750000: episode: 2625, duration: 18.386s, episode steps: 617, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002628, mae: 0.888676, mean_q: 1.190269, mean_eps: 0.108300\n",
            "  991948/1750000: episode: 2626, duration: 25.683s, episode steps: 861, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.390 [0.000, 3.000],  loss: 0.002454, mae: 0.895196, mean_q: 1.198921, mean_eps: 0.107636\n",
            "  992705/1750000: episode: 2627, duration: 22.410s, episode steps: 757, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.950 [0.000, 3.000],  loss: 0.002277, mae: 0.900083, mean_q: 1.206536, mean_eps: 0.106907\n",
            "  993361/1750000: episode: 2628, duration: 19.598s, episode steps: 656, steps per second:  33, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.002677, mae: 0.896579, mean_q: 1.201670, mean_eps: 0.106269\n",
            "  994119/1750000: episode: 2629, duration: 22.743s, episode steps: 758, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.801 [0.000, 3.000],  loss: 0.001843, mae: 0.900312, mean_q: 1.206889, mean_eps: 0.105634\n",
            "  994899/1750000: episode: 2630, duration: 23.245s, episode steps: 780, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.002183, mae: 0.895320, mean_q: 1.199300, mean_eps: 0.104943\n",
            "  995761/1750000: episode: 2631, duration: 26.014s, episode steps: 862, steps per second:  33, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.760 [0.000, 3.000],  loss: 0.002352, mae: 0.897932, mean_q: 1.202073, mean_eps: 0.104203\n",
            "  996348/1750000: episode: 2632, duration: 17.640s, episode steps: 587, steps per second:  33, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002721, mae: 0.896273, mean_q: 1.199930, mean_eps: 0.103551\n",
            "  997176/1750000: episode: 2633, duration: 25.087s, episode steps: 828, steps per second:  33, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002805, mae: 0.894083, mean_q: 1.197445, mean_eps: 0.102916\n",
            "  998239/1750000: episode: 2634, duration: 31.566s, episode steps: 1063, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002191, mae: 0.897285, mean_q: 1.201134, mean_eps: 0.102065\n",
            "  999106/1750000: episode: 2635, duration: 25.711s, episode steps: 867, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.002168, mae: 0.896733, mean_q: 1.200290, mean_eps: 0.101195\n",
            " 1000083/1750000: episode: 2636, duration: 29.277s, episode steps: 977, steps per second:  33, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002243, mae: 0.894655, mean_q: 1.197748, mean_eps: 0.100368\n",
            " 1000725/1750000: episode: 2637, duration: 19.174s, episode steps: 642, steps per second:  33, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.002038, mae: 0.903215, mean_q: 1.209266, mean_eps: 0.100000\n",
            " 1002006/1750000: episode: 2638, duration: 38.258s, episode steps: 1281, steps per second:  33, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.002594, mae: 0.897948, mean_q: 1.202200, mean_eps: 0.100000\n",
            " 1002850/1750000: episode: 2639, duration: 25.147s, episode steps: 844, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.002489, mae: 0.901668, mean_q: 1.207740, mean_eps: 0.100000\n",
            " 1003613/1750000: episode: 2640, duration: 22.845s, episode steps: 763, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002883, mae: 0.903105, mean_q: 1.209818, mean_eps: 0.100000\n",
            " 1004557/1750000: episode: 2641, duration: 28.188s, episode steps: 944, steps per second:  33, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002082, mae: 0.901932, mean_q: 1.207718, mean_eps: 0.100000\n",
            " 1005498/1750000: episode: 2642, duration: 28.235s, episode steps: 941, steps per second:  33, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002567, mae: 0.899644, mean_q: 1.204874, mean_eps: 0.100000\n",
            " 1006274/1750000: episode: 2643, duration: 23.306s, episode steps: 776, steps per second:  33, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.002265, mae: 0.898675, mean_q: 1.203522, mean_eps: 0.100000\n",
            " 1007099/1750000: episode: 2644, duration: 24.593s, episode steps: 825, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002641, mae: 0.898077, mean_q: 1.201849, mean_eps: 0.100000\n",
            " 1008097/1750000: episode: 2645, duration: 29.838s, episode steps: 998, steps per second:  33, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.815 [0.000, 3.000],  loss: 0.002577, mae: 0.898305, mean_q: 1.203217, mean_eps: 0.100000\n",
            " 1008940/1750000: episode: 2646, duration: 25.279s, episode steps: 843, steps per second:  33, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.002721, mae: 0.898664, mean_q: 1.202862, mean_eps: 0.100000\n",
            " 1010232/1750000: episode: 2647, duration: 38.984s, episode steps: 1292, steps per second:  33, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002422, mae: 0.904279, mean_q: 1.211030, mean_eps: 0.100000\n",
            " 1011414/1750000: episode: 2648, duration: 35.458s, episode steps: 1182, steps per second:  33, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001900, mae: 0.906711, mean_q: 1.214280, mean_eps: 0.100000\n",
            " 1012400/1750000: episode: 2649, duration: 29.689s, episode steps: 986, steps per second:  33, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.001964, mae: 0.906265, mean_q: 1.213988, mean_eps: 0.100000\n",
            " 1013245/1750000: episode: 2650, duration: 25.360s, episode steps: 845, steps per second:  33, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.273 [0.000, 3.000],  loss: 0.002143, mae: 0.905625, mean_q: 1.212267, mean_eps: 0.100000\n",
            " 1013926/1750000: episode: 2651, duration: 20.485s, episode steps: 681, steps per second:  33, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001845, mae: 0.907690, mean_q: 1.214838, mean_eps: 0.100000\n",
            " 1014743/1750000: episode: 2652, duration: 24.507s, episode steps: 817, steps per second:  33, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.002718, mae: 0.904309, mean_q: 1.211350, mean_eps: 0.100000\n",
            " 1015688/1750000: episode: 2653, duration: 28.592s, episode steps: 945, steps per second:  33, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002377, mae: 0.903668, mean_q: 1.209881, mean_eps: 0.100000\n",
            " 1016594/1750000: episode: 2654, duration: 27.157s, episode steps: 906, steps per second:  33, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.002263, mae: 0.904124, mean_q: 1.210444, mean_eps: 0.100000\n",
            " 1017511/1750000: episode: 2655, duration: 27.549s, episode steps: 917, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002173, mae: 0.912020, mean_q: 1.221743, mean_eps: 0.100000\n",
            " 1018205/1750000: episode: 2656, duration: 20.873s, episode steps: 694, steps per second:  33, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.002717, mae: 0.904288, mean_q: 1.211015, mean_eps: 0.100000\n",
            " 1019259/1750000: episode: 2657, duration: 31.649s, episode steps: 1054, steps per second:  33, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.839 [0.000, 3.000],  loss: 0.002724, mae: 0.905886, mean_q: 1.212322, mean_eps: 0.100000\n",
            " 1020246/1750000: episode: 2658, duration: 29.482s, episode steps: 987, steps per second:  33, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002202, mae: 0.905385, mean_q: 1.212929, mean_eps: 0.100000\n",
            " 1020981/1750000: episode: 2659, duration: 22.213s, episode steps: 735, steps per second:  33, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.239 [0.000, 3.000],  loss: 0.002407, mae: 0.922355, mean_q: 1.235294, mean_eps: 0.100000\n",
            " 1021844/1750000: episode: 2660, duration: 25.924s, episode steps: 863, steps per second:  33, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001735, mae: 0.923709, mean_q: 1.236227, mean_eps: 0.100000\n",
            " 1022440/1750000: episode: 2661, duration: 18.085s, episode steps: 596, steps per second:  33, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.003144, mae: 0.923901, mean_q: 1.238821, mean_eps: 0.100000\n",
            " 1023570/1750000: episode: 2662, duration: 33.740s, episode steps: 1130, steps per second:  33, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002528, mae: 0.921990, mean_q: 1.234437, mean_eps: 0.100000\n",
            " 1024380/1750000: episode: 2663, duration: 24.004s, episode steps: 810, steps per second:  34, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002793, mae: 0.920272, mean_q: 1.231083, mean_eps: 0.100000\n",
            " 1025132/1750000: episode: 2664, duration: 22.540s, episode steps: 752, steps per second:  33, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002212, mae: 0.927259, mean_q: 1.241914, mean_eps: 0.100000\n",
            " 1025836/1750000: episode: 2665, duration: 21.102s, episode steps: 704, steps per second:  33, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.002299, mae: 0.923990, mean_q: 1.237687, mean_eps: 0.100000\n",
            " 1026581/1750000: episode: 2666, duration: 22.237s, episode steps: 745, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.002299, mae: 0.926122, mean_q: 1.239653, mean_eps: 0.100000\n",
            " 1027582/1750000: episode: 2667, duration: 29.841s, episode steps: 1001, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002538, mae: 0.921300, mean_q: 1.234521, mean_eps: 0.100000\n",
            " 1028846/1750000: episode: 2668, duration: 37.588s, episode steps: 1264, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.002412, mae: 0.920469, mean_q: 1.232911, mean_eps: 0.100000\n",
            " 1029628/1750000: episode: 2669, duration: 23.440s, episode steps: 782, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.003018, mae: 0.926167, mean_q: 1.240117, mean_eps: 0.100000\n",
            " 1030164/1750000: episode: 2670, duration: 16.162s, episode steps: 536, steps per second:  33, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002836, mae: 0.928593, mean_q: 1.243764, mean_eps: 0.100000\n",
            " 1031078/1750000: episode: 2671, duration: 27.174s, episode steps: 914, steps per second:  34, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002423, mae: 0.926132, mean_q: 1.239302, mean_eps: 0.100000\n",
            " 1032066/1750000: episode: 2672, duration: 29.379s, episode steps: 988, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.001955, mae: 0.927537, mean_q: 1.243743, mean_eps: 0.100000\n",
            " 1033053/1750000: episode: 2673, duration: 29.542s, episode steps: 987, steps per second:  33, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002249, mae: 0.923018, mean_q: 1.237165, mean_eps: 0.100000\n",
            " 1033780/1750000: episode: 2674, duration: 21.640s, episode steps: 727, steps per second:  34, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.890 [0.000, 3.000],  loss: 0.002710, mae: 0.928771, mean_q: 1.243358, mean_eps: 0.100000\n",
            " 1034523/1750000: episode: 2675, duration: 22.322s, episode steps: 743, steps per second:  33, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002383, mae: 0.927214, mean_q: 1.241095, mean_eps: 0.100000\n",
            " 1035317/1750000: episode: 2676, duration: 23.970s, episode steps: 794, steps per second:  33, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002501, mae: 0.926507, mean_q: 1.240166, mean_eps: 0.100000\n",
            " 1036433/1750000: episode: 2677, duration: 33.242s, episode steps: 1116, steps per second:  34, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002503, mae: 0.924365, mean_q: 1.238054, mean_eps: 0.100000\n",
            " 1037181/1750000: episode: 2678, duration: 22.244s, episode steps: 748, steps per second:  34, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002905, mae: 0.923331, mean_q: 1.236507, mean_eps: 0.100000\n",
            " 1037884/1750000: episode: 2679, duration: 21.031s, episode steps: 703, steps per second:  33, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002409, mae: 0.928228, mean_q: 1.243800, mean_eps: 0.100000\n",
            " 1038781/1750000: episode: 2680, duration: 26.993s, episode steps: 897, steps per second:  33, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.002865, mae: 0.923272, mean_q: 1.237867, mean_eps: 0.100000\n",
            " 1039532/1750000: episode: 2681, duration: 22.471s, episode steps: 751, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.933 [0.000, 3.000],  loss: 0.002451, mae: 0.926433, mean_q: 1.242226, mean_eps: 0.100000\n",
            " 1040595/1750000: episode: 2682, duration: 31.886s, episode steps: 1063, steps per second:  33, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002320, mae: 0.929387, mean_q: 1.245721, mean_eps: 0.100000\n",
            " 1041560/1750000: episode: 2683, duration: 28.916s, episode steps: 965, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002244, mae: 0.939424, mean_q: 1.258838, mean_eps: 0.100000\n",
            " 1042792/1750000: episode: 2684, duration: 36.886s, episode steps: 1232, steps per second:  33, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002597, mae: 0.939465, mean_q: 1.258225, mean_eps: 0.100000\n",
            " 1043690/1750000: episode: 2685, duration: 27.111s, episode steps: 898, steps per second:  33, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.206 [0.000, 3.000],  loss: 0.002850, mae: 0.936990, mean_q: 1.255709, mean_eps: 0.100000\n",
            " 1044245/1750000: episode: 2686, duration: 16.629s, episode steps: 555, steps per second:  33, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.195 [0.000, 3.000],  loss: 0.003472, mae: 0.940750, mean_q: 1.259255, mean_eps: 0.100000\n",
            " 1045051/1750000: episode: 2687, duration: 24.131s, episode steps: 806, steps per second:  33, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.929 [0.000, 3.000],  loss: 0.002225, mae: 0.936355, mean_q: 1.253678, mean_eps: 0.100000\n",
            " 1045726/1750000: episode: 2688, duration: 20.368s, episode steps: 675, steps per second:  33, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002145, mae: 0.936015, mean_q: 1.252541, mean_eps: 0.100000\n",
            " 1046530/1750000: episode: 2689, duration: 23.892s, episode steps: 804, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002375, mae: 0.941221, mean_q: 1.259746, mean_eps: 0.100000\n",
            " 1047358/1750000: episode: 2690, duration: 24.805s, episode steps: 828, steps per second:  33, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002226, mae: 0.934092, mean_q: 1.250928, mean_eps: 0.100000\n",
            " 1048267/1750000: episode: 2691, duration: 27.281s, episode steps: 909, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.002363, mae: 0.939265, mean_q: 1.256832, mean_eps: 0.100000\n",
            " 1049170/1750000: episode: 2692, duration: 27.096s, episode steps: 903, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002808, mae: 0.936388, mean_q: 1.253645, mean_eps: 0.100000\n",
            " 1049938/1750000: episode: 2693, duration: 22.831s, episode steps: 768, steps per second:  34, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002184, mae: 0.930331, mean_q: 1.246504, mean_eps: 0.100000\n",
            " 1050669/1750000: episode: 2694, duration: 22.151s, episode steps: 731, steps per second:  33, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002594, mae: 0.943175, mean_q: 1.262171, mean_eps: 0.100000\n",
            " 1051657/1750000: episode: 2695, duration: 29.458s, episode steps: 988, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.002484, mae: 0.941007, mean_q: 1.258834, mean_eps: 0.100000\n",
            " 1052370/1750000: episode: 2696, duration: 21.147s, episode steps: 713, steps per second:  34, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.002563, mae: 0.947976, mean_q: 1.267919, mean_eps: 0.100000\n",
            " 1053489/1750000: episode: 2697, duration: 33.392s, episode steps: 1119, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001933, mae: 0.941665, mean_q: 1.260674, mean_eps: 0.100000\n",
            " 1054324/1750000: episode: 2698, duration: 24.958s, episode steps: 835, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.002213, mae: 0.942446, mean_q: 1.260351, mean_eps: 0.100000\n",
            " 1055288/1750000: episode: 2699, duration: 28.988s, episode steps: 964, steps per second:  33, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002157, mae: 0.939401, mean_q: 1.258191, mean_eps: 0.100000\n",
            " 1056056/1750000: episode: 2700, duration: 22.951s, episode steps: 768, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002265, mae: 0.940608, mean_q: 1.258955, mean_eps: 0.100000\n",
            " 1057086/1750000: episode: 2701, duration: 30.982s, episode steps: 1030, steps per second:  33, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.002464, mae: 0.941720, mean_q: 1.261707, mean_eps: 0.100000\n",
            " 1058534/1750000: episode: 2702, duration: 43.005s, episode steps: 1448, steps per second:  34, episode reward: 23.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.808 [0.000, 3.000],  loss: 0.002523, mae: 0.942023, mean_q: 1.260938, mean_eps: 0.100000\n",
            " 1059426/1750000: episode: 2703, duration: 26.882s, episode steps: 892, steps per second:  33, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.002922, mae: 0.941495, mean_q: 1.260432, mean_eps: 0.100000\n",
            " 1060496/1750000: episode: 2704, duration: 31.975s, episode steps: 1070, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002711, mae: 0.947368, mean_q: 1.267812, mean_eps: 0.100000\n",
            " 1061244/1750000: episode: 2705, duration: 22.474s, episode steps: 748, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002707, mae: 0.954164, mean_q: 1.277042, mean_eps: 0.100000\n",
            " 1062578/1750000: episode: 2706, duration: 40.058s, episode steps: 1334, steps per second:  33, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.229 [0.000, 3.000],  loss: 0.002349, mae: 0.950255, mean_q: 1.272027, mean_eps: 0.100000\n",
            " 1063536/1750000: episode: 2707, duration: 28.639s, episode steps: 958, steps per second:  33, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002552, mae: 0.954755, mean_q: 1.277287, mean_eps: 0.100000\n",
            " 1064487/1750000: episode: 2708, duration: 28.733s, episode steps: 951, steps per second:  33, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002823, mae: 0.947718, mean_q: 1.267988, mean_eps: 0.100000\n",
            " 1065091/1750000: episode: 2709, duration: 17.960s, episode steps: 604, steps per second:  34, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.003075, mae: 0.942787, mean_q: 1.260731, mean_eps: 0.100000\n",
            " 1066516/1750000: episode: 2710, duration: 42.698s, episode steps: 1425, steps per second:  33, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.002153, mae: 0.954237, mean_q: 1.277121, mean_eps: 0.100000\n",
            " 1067268/1750000: episode: 2711, duration: 22.648s, episode steps: 752, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002683, mae: 0.950306, mean_q: 1.271961, mean_eps: 0.100000\n",
            " 1067769/1750000: episode: 2712, duration: 15.090s, episode steps: 501, steps per second:  33, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002363, mae: 0.957101, mean_q: 1.280676, mean_eps: 0.100000\n",
            " 1068591/1750000: episode: 2713, duration: 24.486s, episode steps: 822, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002614, mae: 0.950414, mean_q: 1.271744, mean_eps: 0.100000\n",
            " 1069416/1750000: episode: 2714, duration: 24.725s, episode steps: 825, steps per second:  33, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002888, mae: 0.953397, mean_q: 1.274931, mean_eps: 0.100000\n",
            " 1070215/1750000: episode: 2715, duration: 24.189s, episode steps: 799, steps per second:  33, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.002485, mae: 0.954987, mean_q: 1.277754, mean_eps: 0.100000\n",
            " 1071292/1750000: episode: 2716, duration: 32.418s, episode steps: 1077, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.002591, mae: 0.955878, mean_q: 1.278200, mean_eps: 0.100000\n",
            " 1072194/1750000: episode: 2717, duration: 27.220s, episode steps: 902, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002522, mae: 0.956743, mean_q: 1.279107, mean_eps: 0.100000\n",
            " 1072789/1750000: episode: 2718, duration: 17.956s, episode steps: 595, steps per second:  33, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.002922, mae: 0.961000, mean_q: 1.284790, mean_eps: 0.100000\n",
            " 1073326/1750000: episode: 2719, duration: 16.051s, episode steps: 537, steps per second:  33, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.002245, mae: 0.958455, mean_q: 1.281790, mean_eps: 0.100000\n",
            " 1074383/1750000: episode: 2720, duration: 31.651s, episode steps: 1057, steps per second:  33, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.011 [0.000, 3.000],  loss: 0.002456, mae: 0.956271, mean_q: 1.279192, mean_eps: 0.100000\n",
            " 1075216/1750000: episode: 2721, duration: 25.122s, episode steps: 833, steps per second:  33, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.002712, mae: 0.957904, mean_q: 1.281398, mean_eps: 0.100000\n",
            " 1076229/1750000: episode: 2722, duration: 30.113s, episode steps: 1013, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.333 [0.000, 3.000],  loss: 0.002179, mae: 0.952154, mean_q: 1.274469, mean_eps: 0.100000\n",
            " 1077088/1750000: episode: 2723, duration: 25.674s, episode steps: 859, steps per second:  33, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002393, mae: 0.956806, mean_q: 1.281113, mean_eps: 0.100000\n",
            " 1077631/1750000: episode: 2724, duration: 16.287s, episode steps: 543, steps per second:  33, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002063, mae: 0.958106, mean_q: 1.282771, mean_eps: 0.100000\n",
            " 1078683/1750000: episode: 2725, duration: 31.501s, episode steps: 1052, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.002594, mae: 0.957861, mean_q: 1.282286, mean_eps: 0.100000\n",
            " 1079386/1750000: episode: 2726, duration: 21.138s, episode steps: 703, steps per second:  33, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.002718, mae: 0.957158, mean_q: 1.281018, mean_eps: 0.100000\n",
            " 1080455/1750000: episode: 2727, duration: 31.903s, episode steps: 1069, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.003300, mae: 0.960347, mean_q: 1.283680, mean_eps: 0.100000\n",
            " 1081197/1750000: episode: 2728, duration: 22.302s, episode steps: 742, steps per second:  33, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.796 [0.000, 3.000],  loss: 0.002694, mae: 0.961219, mean_q: 1.285260, mean_eps: 0.100000\n",
            " 1081897/1750000: episode: 2729, duration: 20.587s, episode steps: 700, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002798, mae: 0.959751, mean_q: 1.283445, mean_eps: 0.100000\n",
            " 1082741/1750000: episode: 2730, duration: 24.892s, episode steps: 844, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002948, mae: 0.957409, mean_q: 1.280782, mean_eps: 0.100000\n",
            " 1083440/1750000: episode: 2731, duration: 21.099s, episode steps: 699, steps per second:  33, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.003100, mae: 0.960966, mean_q: 1.286048, mean_eps: 0.100000\n",
            " 1084333/1750000: episode: 2732, duration: 26.850s, episode steps: 893, steps per second:  33, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001984, mae: 0.959779, mean_q: 1.283955, mean_eps: 0.100000\n",
            " 1085364/1750000: episode: 2733, duration: 30.925s, episode steps: 1031, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002419, mae: 0.963629, mean_q: 1.289187, mean_eps: 0.100000\n",
            " 1086233/1750000: episode: 2734, duration: 26.052s, episode steps: 869, steps per second:  33, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.002307, mae: 0.960195, mean_q: 1.284587, mean_eps: 0.100000\n",
            " 1086859/1750000: episode: 2735, duration: 18.490s, episode steps: 626, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.831 [0.000, 3.000],  loss: 0.002548, mae: 0.967982, mean_q: 1.295210, mean_eps: 0.100000\n",
            " 1087474/1750000: episode: 2736, duration: 18.399s, episode steps: 615, steps per second:  33, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002518, mae: 0.957725, mean_q: 1.281383, mean_eps: 0.100000\n",
            " 1088328/1750000: episode: 2737, duration: 25.490s, episode steps: 854, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002292, mae: 0.963484, mean_q: 1.288594, mean_eps: 0.100000\n",
            " 1089262/1750000: episode: 2738, duration: 27.996s, episode steps: 934, steps per second:  33, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.762 [0.000, 3.000],  loss: 0.002822, mae: 0.963405, mean_q: 1.290100, mean_eps: 0.100000\n",
            " 1090434/1750000: episode: 2739, duration: 34.597s, episode steps: 1172, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002212, mae: 0.964149, mean_q: 1.290124, mean_eps: 0.100000\n",
            " 1091197/1750000: episode: 2740, duration: 22.448s, episode steps: 763, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.002870, mae: 0.970295, mean_q: 1.297949, mean_eps: 0.100000\n",
            " 1091871/1750000: episode: 2741, duration: 19.943s, episode steps: 674, steps per second:  34, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.346 [0.000, 3.000],  loss: 0.002187, mae: 0.965571, mean_q: 1.291697, mean_eps: 0.100000\n",
            " 1092917/1750000: episode: 2742, duration: 31.087s, episode steps: 1046, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002961, mae: 0.971128, mean_q: 1.299309, mean_eps: 0.100000\n",
            " 1093713/1750000: episode: 2743, duration: 23.640s, episode steps: 796, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002329, mae: 0.965132, mean_q: 1.291173, mean_eps: 0.100000\n",
            " 1095166/1750000: episode: 2744, duration: 43.163s, episode steps: 1453, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.002069, mae: 0.972515, mean_q: 1.301869, mean_eps: 0.100000\n",
            " 1096000/1750000: episode: 2745, duration: 24.843s, episode steps: 834, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.354 [0.000, 3.000],  loss: 0.002796, mae: 0.970630, mean_q: 1.298167, mean_eps: 0.100000\n",
            " 1096814/1750000: episode: 2746, duration: 24.309s, episode steps: 814, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002503, mae: 0.974627, mean_q: 1.303427, mean_eps: 0.100000\n",
            " 1097739/1750000: episode: 2747, duration: 27.421s, episode steps: 925, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.002327, mae: 0.970748, mean_q: 1.299233, mean_eps: 0.100000\n",
            " 1098540/1750000: episode: 2748, duration: 23.804s, episode steps: 801, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002173, mae: 0.970758, mean_q: 1.298960, mean_eps: 0.100000\n",
            " 1099503/1750000: episode: 2749, duration: 28.716s, episode steps: 963, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002157, mae: 0.966829, mean_q: 1.293366, mean_eps: 0.100000\n",
            " 1100447/1750000: episode: 2750, duration: 27.960s, episode steps: 944, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.002160, mae: 0.969518, mean_q: 1.296742, mean_eps: 0.100000\n",
            " 1101071/1750000: episode: 2751, duration: 18.747s, episode steps: 624, steps per second:  33, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001931, mae: 0.973091, mean_q: 1.302976, mean_eps: 0.100000\n",
            " 1102068/1750000: episode: 2752, duration: 29.921s, episode steps: 997, steps per second:  33, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.796 [0.000, 3.000],  loss: 0.001893, mae: 0.981893, mean_q: 1.314829, mean_eps: 0.100000\n",
            " 1102927/1750000: episode: 2753, duration: 25.600s, episode steps: 859, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002347, mae: 0.974795, mean_q: 1.304012, mean_eps: 0.100000\n",
            " 1103955/1750000: episode: 2754, duration: 30.456s, episode steps: 1028, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002375, mae: 0.977602, mean_q: 1.308692, mean_eps: 0.100000\n",
            " 1104737/1750000: episode: 2755, duration: 23.077s, episode steps: 782, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002579, mae: 0.975094, mean_q: 1.303735, mean_eps: 0.100000\n",
            " 1105616/1750000: episode: 2756, duration: 26.028s, episode steps: 879, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002275, mae: 0.977194, mean_q: 1.308122, mean_eps: 0.100000\n",
            " 1106591/1750000: episode: 2757, duration: 28.848s, episode steps: 975, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002076, mae: 0.976708, mean_q: 1.307358, mean_eps: 0.100000\n",
            " 1107291/1750000: episode: 2758, duration: 20.812s, episode steps: 700, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001961, mae: 0.970341, mean_q: 1.298942, mean_eps: 0.100000\n",
            " 1108244/1750000: episode: 2759, duration: 28.324s, episode steps: 953, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002551, mae: 0.974637, mean_q: 1.303679, mean_eps: 0.100000\n",
            " 1109283/1750000: episode: 2760, duration: 30.745s, episode steps: 1039, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.002563, mae: 0.973377, mean_q: 1.301762, mean_eps: 0.100000\n",
            " 1110466/1750000: episode: 2761, duration: 35.172s, episode steps: 1183, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.003229, mae: 0.981669, mean_q: 1.312836, mean_eps: 0.100000\n",
            " 1111723/1750000: episode: 2762, duration: 37.362s, episode steps: 1257, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002485, mae: 0.978535, mean_q: 1.309448, mean_eps: 0.100000\n",
            " 1112394/1750000: episode: 2763, duration: 19.928s, episode steps: 671, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.301 [0.000, 3.000],  loss: 0.002577, mae: 0.980955, mean_q: 1.312587, mean_eps: 0.100000\n",
            " 1113445/1750000: episode: 2764, duration: 31.540s, episode steps: 1051, steps per second:  33, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.002547, mae: 0.978710, mean_q: 1.309935, mean_eps: 0.100000\n",
            " 1114441/1750000: episode: 2765, duration: 29.500s, episode steps: 996, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002385, mae: 0.983363, mean_q: 1.316172, mean_eps: 0.100000\n",
            " 1114936/1750000: episode: 2766, duration: 14.766s, episode steps: 495, steps per second:  34, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.003154, mae: 0.982202, mean_q: 1.314594, mean_eps: 0.100000\n",
            " 1116281/1750000: episode: 2767, duration: 40.091s, episode steps: 1345, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.842 [0.000, 3.000],  loss: 0.002480, mae: 0.981064, mean_q: 1.312976, mean_eps: 0.100000\n",
            " 1117062/1750000: episode: 2768, duration: 23.011s, episode steps: 781, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002642, mae: 0.980041, mean_q: 1.311069, mean_eps: 0.100000\n",
            " 1118122/1750000: episode: 2769, duration: 31.614s, episode steps: 1060, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.002434, mae: 0.984048, mean_q: 1.317171, mean_eps: 0.100000\n",
            " 1118840/1750000: episode: 2770, duration: 21.374s, episode steps: 718, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.002229, mae: 0.988463, mean_q: 1.322606, mean_eps: 0.100000\n",
            " 1119981/1750000: episode: 2771, duration: 34.050s, episode steps: 1141, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002682, mae: 0.983066, mean_q: 1.315451, mean_eps: 0.100000\n",
            " 1120550/1750000: episode: 2772, duration: 17.028s, episode steps: 569, steps per second:  33, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.002325, mae: 0.997201, mean_q: 1.334567, mean_eps: 0.100000\n",
            " 1121198/1750000: episode: 2773, duration: 19.220s, episode steps: 648, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.003092, mae: 0.996692, mean_q: 1.332930, mean_eps: 0.100000\n",
            " 1122157/1750000: episode: 2774, duration: 28.651s, episode steps: 959, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.003343, mae: 0.991129, mean_q: 1.325778, mean_eps: 0.100000\n",
            " 1122878/1750000: episode: 2775, duration: 21.580s, episode steps: 721, steps per second:  33, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002915, mae: 0.995180, mean_q: 1.331747, mean_eps: 0.100000\n",
            " 1124101/1750000: episode: 2776, duration: 36.556s, episode steps: 1223, steps per second:  33, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.003249, mae: 0.992218, mean_q: 1.326622, mean_eps: 0.100000\n",
            " 1125158/1750000: episode: 2777, duration: 31.639s, episode steps: 1057, steps per second:  33, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.002497, mae: 0.991467, mean_q: 1.326055, mean_eps: 0.100000\n",
            " 1126499/1750000: episode: 2778, duration: 39.941s, episode steps: 1341, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.002582, mae: 0.995590, mean_q: 1.331430, mean_eps: 0.100000\n",
            " 1127743/1750000: episode: 2779, duration: 37.229s, episode steps: 1244, steps per second:  33, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002829, mae: 0.994110, mean_q: 1.329878, mean_eps: 0.100000\n",
            " 1128860/1750000: episode: 2780, duration: 33.383s, episode steps: 1117, steps per second:  33, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.002753, mae: 0.993829, mean_q: 1.330630, mean_eps: 0.100000\n",
            " 1129646/1750000: episode: 2781, duration: 23.663s, episode steps: 786, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.002733, mae: 0.989317, mean_q: 1.322925, mean_eps: 0.100000\n",
            " 1130167/1750000: episode: 2782, duration: 15.710s, episode steps: 521, steps per second:  33, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002197, mae: 0.997768, mean_q: 1.334634, mean_eps: 0.100000\n",
            " 1130799/1750000: episode: 2783, duration: 18.862s, episode steps: 632, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002721, mae: 1.003716, mean_q: 1.342965, mean_eps: 0.100000\n",
            " 1131878/1750000: episode: 2784, duration: 32.036s, episode steps: 1079, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.002656, mae: 1.005484, mean_q: 1.345161, mean_eps: 0.100000\n",
            " 1133018/1750000: episode: 2785, duration: 33.996s, episode steps: 1140, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002369, mae: 1.003304, mean_q: 1.342584, mean_eps: 0.100000\n",
            " 1134296/1750000: episode: 2786, duration: 38.481s, episode steps: 1278, steps per second:  33, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.003056, mae: 1.008086, mean_q: 1.348163, mean_eps: 0.100000\n",
            " 1135437/1750000: episode: 2787, duration: 34.381s, episode steps: 1141, steps per second:  33, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002065, mae: 1.003001, mean_q: 1.341429, mean_eps: 0.100000\n",
            " 1136063/1750000: episode: 2788, duration: 18.704s, episode steps: 626, steps per second:  33, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.805 [0.000, 3.000],  loss: 0.002345, mae: 1.005633, mean_q: 1.344412, mean_eps: 0.100000\n",
            " 1136734/1750000: episode: 2789, duration: 20.230s, episode steps: 671, steps per second:  33, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.003642, mae: 1.003511, mean_q: 1.341357, mean_eps: 0.100000\n",
            " 1137912/1750000: episode: 2790, duration: 35.060s, episode steps: 1178, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.002504, mae: 1.006215, mean_q: 1.345543, mean_eps: 0.100000\n",
            " 1138790/1750000: episode: 2791, duration: 26.394s, episode steps: 878, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: 0.002121, mae: 1.007118, mean_q: 1.347267, mean_eps: 0.100000\n",
            " 1139921/1750000: episode: 2792, duration: 34.001s, episode steps: 1131, steps per second:  33, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002371, mae: 1.006958, mean_q: 1.346102, mean_eps: 0.100000\n",
            " 1140909/1750000: episode: 2793, duration: 29.568s, episode steps: 988, steps per second:  33, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002486, mae: 1.013526, mean_q: 1.354668, mean_eps: 0.100000\n",
            " 1141797/1750000: episode: 2794, duration: 26.364s, episode steps: 888, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001871, mae: 1.016386, mean_q: 1.359496, mean_eps: 0.100000\n",
            " 1142428/1750000: episode: 2795, duration: 18.970s, episode steps: 631, steps per second:  33, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.002902, mae: 1.013462, mean_q: 1.354266, mean_eps: 0.100000\n",
            " 1143341/1750000: episode: 2796, duration: 27.368s, episode steps: 913, steps per second:  33, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.849 [0.000, 3.000],  loss: 0.002233, mae: 1.019779, mean_q: 1.363919, mean_eps: 0.100000\n",
            " 1144086/1750000: episode: 2797, duration: 21.936s, episode steps: 745, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.003022, mae: 1.011981, mean_q: 1.352306, mean_eps: 0.100000\n",
            " 1144760/1750000: episode: 2798, duration: 20.113s, episode steps: 674, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.002147, mae: 1.015027, mean_q: 1.356469, mean_eps: 0.100000\n",
            " 1146033/1750000: episode: 2799, duration: 38.182s, episode steps: 1273, steps per second:  33, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.830 [0.000, 3.000],  loss: 0.002447, mae: 1.014344, mean_q: 1.355627, mean_eps: 0.100000\n",
            " 1147258/1750000: episode: 2800, duration: 36.171s, episode steps: 1225, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.002471, mae: 1.011114, mean_q: 1.351800, mean_eps: 0.100000\n",
            " 1147945/1750000: episode: 2801, duration: 20.652s, episode steps: 687, steps per second:  33, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002528, mae: 1.018013, mean_q: 1.361793, mean_eps: 0.100000\n",
            " 1148926/1750000: episode: 2802, duration: 29.033s, episode steps: 981, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.841 [0.000, 3.000],  loss: 0.002812, mae: 1.018405, mean_q: 1.361503, mean_eps: 0.100000\n",
            " 1149884/1750000: episode: 2803, duration: 28.337s, episode steps: 958, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002462, mae: 1.015802, mean_q: 1.357669, mean_eps: 0.100000\n",
            " 1150710/1750000: episode: 2804, duration: 24.924s, episode steps: 826, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002427, mae: 1.013421, mean_q: 1.355700, mean_eps: 0.100000\n",
            " 1151555/1750000: episode: 2805, duration: 25.149s, episode steps: 845, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002141, mae: 1.012657, mean_q: 1.354004, mean_eps: 0.100000\n",
            " 1152627/1750000: episode: 2806, duration: 31.693s, episode steps: 1072, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002456, mae: 1.015360, mean_q: 1.358321, mean_eps: 0.100000\n",
            " 1153664/1750000: episode: 2807, duration: 30.911s, episode steps: 1037, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.002361, mae: 1.013855, mean_q: 1.355574, mean_eps: 0.100000\n",
            " 1154659/1750000: episode: 2808, duration: 29.741s, episode steps: 995, steps per second:  33, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002376, mae: 1.018264, mean_q: 1.362249, mean_eps: 0.100000\n",
            " 1155530/1750000: episode: 2809, duration: 26.030s, episode steps: 871, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002463, mae: 1.019006, mean_q: 1.362153, mean_eps: 0.100000\n",
            " 1156450/1750000: episode: 2810, duration: 27.452s, episode steps: 920, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.002731, mae: 1.011309, mean_q: 1.354128, mean_eps: 0.100000\n",
            " 1157234/1750000: episode: 2811, duration: 23.518s, episode steps: 784, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.950 [0.000, 3.000],  loss: 0.002344, mae: 1.018632, mean_q: 1.363284, mean_eps: 0.100000\n",
            " 1158119/1750000: episode: 2812, duration: 26.639s, episode steps: 885, steps per second:  33, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002128, mae: 1.012621, mean_q: 1.354200, mean_eps: 0.100000\n",
            " 1159088/1750000: episode: 2813, duration: 29.182s, episode steps: 969, steps per second:  33, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.003021, mae: 1.015387, mean_q: 1.358369, mean_eps: 0.100000\n",
            " 1160130/1750000: episode: 2814, duration: 31.409s, episode steps: 1042, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.002461, mae: 1.020040, mean_q: 1.365278, mean_eps: 0.100000\n",
            " 1160867/1750000: episode: 2815, duration: 22.021s, episode steps: 737, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002212, mae: 1.019092, mean_q: 1.362615, mean_eps: 0.100000\n",
            " 1161604/1750000: episode: 2816, duration: 22.092s, episode steps: 737, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.002173, mae: 1.020847, mean_q: 1.364891, mean_eps: 0.100000\n",
            " 1162465/1750000: episode: 2817, duration: 25.941s, episode steps: 861, steps per second:  33, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.499 [0.000, 3.000],  loss: 0.002357, mae: 1.023369, mean_q: 1.367361, mean_eps: 0.100000\n",
            " 1163321/1750000: episode: 2818, duration: 25.785s, episode steps: 856, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.002191, mae: 1.018007, mean_q: 1.360763, mean_eps: 0.100000\n",
            " 1163983/1750000: episode: 2819, duration: 19.724s, episode steps: 662, steps per second:  34, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.002200, mae: 1.019089, mean_q: 1.363325, mean_eps: 0.100000\n",
            " 1164812/1750000: episode: 2820, duration: 24.942s, episode steps: 829, steps per second:  33, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002468, mae: 1.015372, mean_q: 1.358036, mean_eps: 0.100000\n",
            " 1166196/1750000: episode: 2821, duration: 41.424s, episode steps: 1384, steps per second:  33, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002426, mae: 1.015821, mean_q: 1.358529, mean_eps: 0.100000\n",
            " 1166939/1750000: episode: 2822, duration: 21.874s, episode steps: 743, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002308, mae: 1.020200, mean_q: 1.364381, mean_eps: 0.100000\n",
            " 1167740/1750000: episode: 2823, duration: 23.897s, episode steps: 801, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.263 [0.000, 3.000],  loss: 0.002961, mae: 1.021923, mean_q: 1.365626, mean_eps: 0.100000\n",
            " 1168597/1750000: episode: 2824, duration: 25.403s, episode steps: 857, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.952 [0.000, 3.000],  loss: 0.002929, mae: 1.018198, mean_q: 1.361975, mean_eps: 0.100000\n",
            " 1169747/1750000: episode: 2825, duration: 34.266s, episode steps: 1150, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002376, mae: 1.020359, mean_q: 1.364745, mean_eps: 0.100000\n",
            " 1170765/1750000: episode: 2826, duration: 30.403s, episode steps: 1018, steps per second:  33, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002204, mae: 1.027073, mean_q: 1.373830, mean_eps: 0.100000\n",
            " 1171371/1750000: episode: 2827, duration: 17.902s, episode steps: 606, steps per second:  34, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 0.002976, mae: 1.027500, mean_q: 1.373589, mean_eps: 0.100000\n",
            " 1172713/1750000: episode: 2828, duration: 39.844s, episode steps: 1342, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.963 [0.000, 3.000],  loss: 0.002315, mae: 1.027756, mean_q: 1.373974, mean_eps: 0.100000\n",
            " 1173905/1750000: episode: 2829, duration: 35.643s, episode steps: 1192, steps per second:  33, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002794, mae: 1.019730, mean_q: 1.363315, mean_eps: 0.100000\n",
            " 1174596/1750000: episode: 2830, duration: 20.695s, episode steps: 691, steps per second:  33, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002780, mae: 1.025073, mean_q: 1.370666, mean_eps: 0.100000\n",
            " 1175971/1750000: episode: 2831, duration: 41.276s, episode steps: 1375, steps per second:  33, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.807 [0.000, 3.000],  loss: 0.002760, mae: 1.023973, mean_q: 1.369424, mean_eps: 0.100000\n",
            " 1177136/1750000: episode: 2832, duration: 34.392s, episode steps: 1165, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002981, mae: 1.029066, mean_q: 1.375433, mean_eps: 0.100000\n",
            " 1178444/1750000: episode: 2833, duration: 38.601s, episode steps: 1308, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002266, mae: 1.027654, mean_q: 1.373050, mean_eps: 0.100000\n",
            " 1179440/1750000: episode: 2834, duration: 29.260s, episode steps: 996, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002236, mae: 1.023594, mean_q: 1.369014, mean_eps: 0.100000\n",
            " 1180203/1750000: episode: 2835, duration: 22.466s, episode steps: 763, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.339 [0.000, 3.000],  loss: 0.003205, mae: 1.022902, mean_q: 1.367591, mean_eps: 0.100000\n",
            " 1181357/1750000: episode: 2836, duration: 34.090s, episode steps: 1154, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002665, mae: 1.030358, mean_q: 1.377184, mean_eps: 0.100000\n",
            " 1182116/1750000: episode: 2837, duration: 22.489s, episode steps: 759, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: 0.002332, mae: 1.033109, mean_q: 1.381295, mean_eps: 0.100000\n",
            " 1182906/1750000: episode: 2838, duration: 23.265s, episode steps: 790, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.002465, mae: 1.024886, mean_q: 1.370194, mean_eps: 0.100000\n",
            " 1183880/1750000: episode: 2839, duration: 28.782s, episode steps: 974, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.863 [0.000, 3.000],  loss: 0.003082, mae: 1.032243, mean_q: 1.380199, mean_eps: 0.100000\n",
            " 1184918/1750000: episode: 2840, duration: 30.591s, episode steps: 1038, steps per second:  34, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002228, mae: 1.032708, mean_q: 1.381528, mean_eps: 0.100000\n",
            " 1185700/1750000: episode: 2841, duration: 22.760s, episode steps: 782, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002688, mae: 1.029064, mean_q: 1.375776, mean_eps: 0.100000\n",
            " 1186559/1750000: episode: 2842, duration: 25.465s, episode steps: 859, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002802, mae: 1.029843, mean_q: 1.376740, mean_eps: 0.100000\n",
            " 1187182/1750000: episode: 2843, duration: 18.465s, episode steps: 623, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.886 [0.000, 3.000],  loss: 0.002103, mae: 1.035647, mean_q: 1.384025, mean_eps: 0.100000\n",
            " 1188185/1750000: episode: 2844, duration: 29.485s, episode steps: 1003, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001967, mae: 1.034814, mean_q: 1.382993, mean_eps: 0.100000\n",
            " 1189055/1750000: episode: 2845, duration: 25.452s, episode steps: 870, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.002175, mae: 1.031464, mean_q: 1.379342, mean_eps: 0.100000\n",
            " 1189963/1750000: episode: 2846, duration: 26.792s, episode steps: 908, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.307 [0.000, 3.000],  loss: 0.002291, mae: 1.035951, mean_q: 1.385627, mean_eps: 0.100000\n",
            " 1190703/1750000: episode: 2847, duration: 21.620s, episode steps: 740, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.002262, mae: 1.037728, mean_q: 1.387604, mean_eps: 0.100000\n",
            " 1191704/1750000: episode: 2848, duration: 29.531s, episode steps: 1001, steps per second:  34, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002313, mae: 1.037507, mean_q: 1.386709, mean_eps: 0.100000\n",
            " 1192584/1750000: episode: 2849, duration: 26.060s, episode steps: 880, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.869 [0.000, 3.000],  loss: 0.002577, mae: 1.035615, mean_q: 1.384826, mean_eps: 0.100000\n",
            " 1193427/1750000: episode: 2850, duration: 24.531s, episode steps: 843, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.002331, mae: 1.034856, mean_q: 1.383167, mean_eps: 0.100000\n",
            " 1194238/1750000: episode: 2851, duration: 23.613s, episode steps: 811, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.003034, mae: 1.036154, mean_q: 1.384426, mean_eps: 0.100000\n",
            " 1195121/1750000: episode: 2852, duration: 26.086s, episode steps: 883, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.002645, mae: 1.030870, mean_q: 1.380193, mean_eps: 0.100000\n",
            " 1195993/1750000: episode: 2853, duration: 25.488s, episode steps: 872, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002628, mae: 1.041912, mean_q: 1.393151, mean_eps: 0.100000\n",
            " 1197221/1750000: episode: 2854, duration: 36.184s, episode steps: 1228, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.862 [0.000, 3.000],  loss: 0.002274, mae: 1.041028, mean_q: 1.392269, mean_eps: 0.100000\n",
            " 1198396/1750000: episode: 2855, duration: 34.478s, episode steps: 1175, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.762 [0.000, 3.000],  loss: 0.002485, mae: 1.036953, mean_q: 1.386448, mean_eps: 0.100000\n",
            " 1199606/1750000: episode: 2856, duration: 35.416s, episode steps: 1210, steps per second:  34, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002790, mae: 1.032847, mean_q: 1.381264, mean_eps: 0.100000\n",
            " 1200672/1750000: episode: 2857, duration: 31.357s, episode steps: 1066, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.002622, mae: 1.046558, mean_q: 1.398166, mean_eps: 0.100000\n",
            " 1201558/1750000: episode: 2858, duration: 25.942s, episode steps: 886, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002857, mae: 1.045890, mean_q: 1.397578, mean_eps: 0.100000\n",
            " 1202705/1750000: episode: 2859, duration: 33.747s, episode steps: 1147, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002886, mae: 1.043172, mean_q: 1.395024, mean_eps: 0.100000\n",
            " 1203946/1750000: episode: 2860, duration: 36.356s, episode steps: 1241, steps per second:  34, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.002757, mae: 1.047105, mean_q: 1.399258, mean_eps: 0.100000\n",
            " 1205256/1750000: episode: 2861, duration: 38.648s, episode steps: 1310, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.785 [0.000, 3.000],  loss: 0.002736, mae: 1.042528, mean_q: 1.395036, mean_eps: 0.100000\n",
            " 1206251/1750000: episode: 2862, duration: 29.080s, episode steps: 995, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002237, mae: 1.042273, mean_q: 1.393942, mean_eps: 0.100000\n",
            " 1207330/1750000: episode: 2863, duration: 31.579s, episode steps: 1079, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002877, mae: 1.044618, mean_q: 1.396775, mean_eps: 0.100000\n",
            " 1208191/1750000: episode: 2864, duration: 25.215s, episode steps: 861, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.002684, mae: 1.044437, mean_q: 1.395579, mean_eps: 0.100000\n",
            " 1208998/1750000: episode: 2865, duration: 23.727s, episode steps: 807, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.003199, mae: 1.046742, mean_q: 1.399356, mean_eps: 0.100000\n",
            " 1209672/1750000: episode: 2866, duration: 19.816s, episode steps: 674, steps per second:  34, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.002738, mae: 1.050695, mean_q: 1.405741, mean_eps: 0.100000\n",
            " 1210592/1750000: episode: 2867, duration: 27.060s, episode steps: 920, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.002230, mae: 1.040629, mean_q: 1.391970, mean_eps: 0.100000\n",
            " 1211463/1750000: episode: 2868, duration: 25.607s, episode steps: 871, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.002509, mae: 1.044372, mean_q: 1.396811, mean_eps: 0.100000\n",
            " 1212250/1750000: episode: 2869, duration: 23.037s, episode steps: 787, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.002698, mae: 1.051599, mean_q: 1.406004, mean_eps: 0.100000\n",
            " 1213745/1750000: episode: 2870, duration: 43.738s, episode steps: 1495, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002542, mae: 1.045242, mean_q: 1.397675, mean_eps: 0.100000\n",
            " 1214600/1750000: episode: 2871, duration: 25.137s, episode steps: 855, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.002531, mae: 1.054255, mean_q: 1.408734, mean_eps: 0.100000\n",
            " 1215564/1750000: episode: 2872, duration: 28.295s, episode steps: 964, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002511, mae: 1.046698, mean_q: 1.398593, mean_eps: 0.100000\n",
            " 1216513/1750000: episode: 2873, duration: 28.023s, episode steps: 949, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002723, mae: 1.051927, mean_q: 1.406116, mean_eps: 0.100000\n",
            " 1217170/1750000: episode: 2874, duration: 19.480s, episode steps: 657, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.002478, mae: 1.047707, mean_q: 1.400796, mean_eps: 0.100000\n",
            " 1217896/1750000: episode: 2875, duration: 21.289s, episode steps: 726, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.036 [0.000, 3.000],  loss: 0.002626, mae: 1.052734, mean_q: 1.406979, mean_eps: 0.100000\n",
            " 1218653/1750000: episode: 2876, duration: 22.104s, episode steps: 757, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.003354, mae: 1.050236, mean_q: 1.404135, mean_eps: 0.100000\n",
            " 1219665/1750000: episode: 2877, duration: 29.751s, episode steps: 1012, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002508, mae: 1.057225, mean_q: 1.413156, mean_eps: 0.100000\n",
            " 1220482/1750000: episode: 2878, duration: 23.976s, episode steps: 817, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.941 [0.000, 3.000],  loss: 0.002505, mae: 1.052406, mean_q: 1.407097, mean_eps: 0.100000\n",
            " 1221129/1750000: episode: 2879, duration: 18.928s, episode steps: 647, steps per second:  34, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002096, mae: 1.057831, mean_q: 1.414154, mean_eps: 0.100000\n",
            " 1221815/1750000: episode: 2880, duration: 19.898s, episode steps: 686, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.002566, mae: 1.052584, mean_q: 1.407926, mean_eps: 0.100000\n",
            " 1223095/1750000: episode: 2881, duration: 37.477s, episode steps: 1280, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002724, mae: 1.055243, mean_q: 1.410273, mean_eps: 0.100000\n",
            " 1223848/1750000: episode: 2882, duration: 22.043s, episode steps: 753, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.002297, mae: 1.055045, mean_q: 1.411009, mean_eps: 0.100000\n",
            " 1224806/1750000: episode: 2883, duration: 28.017s, episode steps: 958, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002772, mae: 1.054360, mean_q: 1.409869, mean_eps: 0.100000\n",
            " 1225914/1750000: episode: 2884, duration: 32.644s, episode steps: 1108, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.949 [0.000, 3.000],  loss: 0.002824, mae: 1.054270, mean_q: 1.409552, mean_eps: 0.100000\n",
            " 1226740/1750000: episode: 2885, duration: 24.185s, episode steps: 826, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002612, mae: 1.053098, mean_q: 1.408687, mean_eps: 0.100000\n",
            " 1227688/1750000: episode: 2886, duration: 27.825s, episode steps: 948, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002896, mae: 1.055804, mean_q: 1.411156, mean_eps: 0.100000\n",
            " 1228670/1750000: episode: 2887, duration: 28.716s, episode steps: 982, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.003049, mae: 1.052435, mean_q: 1.407139, mean_eps: 0.100000\n",
            " 1230249/1750000: episode: 2888, duration: 45.926s, episode steps: 1579, steps per second:  34, episode reward: 29.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002836, mae: 1.055308, mean_q: 1.410593, mean_eps: 0.100000\n",
            " 1231331/1750000: episode: 2889, duration: 31.625s, episode steps: 1082, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.003423, mae: 1.051936, mean_q: 1.405496, mean_eps: 0.100000\n",
            " 1232470/1750000: episode: 2890, duration: 33.417s, episode steps: 1139, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002718, mae: 1.057404, mean_q: 1.413621, mean_eps: 0.100000\n",
            " 1233467/1750000: episode: 2891, duration: 29.096s, episode steps: 997, steps per second:  34, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002847, mae: 1.056295, mean_q: 1.411123, mean_eps: 0.100000\n",
            " 1234333/1750000: episode: 2892, duration: 25.502s, episode steps: 866, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.003010, mae: 1.056647, mean_q: 1.412017, mean_eps: 0.100000\n",
            " 1235021/1750000: episode: 2893, duration: 20.151s, episode steps: 688, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002267, mae: 1.055757, mean_q: 1.411319, mean_eps: 0.100000\n",
            " 1236285/1750000: episode: 2894, duration: 37.101s, episode steps: 1264, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002467, mae: 1.060227, mean_q: 1.417350, mean_eps: 0.100000\n",
            " 1237212/1750000: episode: 2895, duration: 27.353s, episode steps: 927, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.837 [0.000, 3.000],  loss: 0.002267, mae: 1.055759, mean_q: 1.411363, mean_eps: 0.100000\n",
            " 1238121/1750000: episode: 2896, duration: 26.717s, episode steps: 909, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002474, mae: 1.052967, mean_q: 1.407559, mean_eps: 0.100000\n",
            " 1239182/1750000: episode: 2897, duration: 31.069s, episode steps: 1061, steps per second:  34, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: 0.002528, mae: 1.063684, mean_q: 1.422364, mean_eps: 0.100000\n",
            " 1240045/1750000: episode: 2898, duration: 25.017s, episode steps: 863, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.002304, mae: 1.059044, mean_q: 1.416512, mean_eps: 0.100000\n",
            " 1240823/1750000: episode: 2899, duration: 22.662s, episode steps: 778, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.002519, mae: 1.057401, mean_q: 1.413732, mean_eps: 0.100000\n",
            " 1241855/1750000: episode: 2900, duration: 30.471s, episode steps: 1032, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.003012, mae: 1.056161, mean_q: 1.412277, mean_eps: 0.100000\n",
            " 1242424/1750000: episode: 2901, duration: 16.894s, episode steps: 569, steps per second:  34, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003299, mae: 1.059349, mean_q: 1.414916, mean_eps: 0.100000\n",
            " 1243684/1750000: episode: 2902, duration: 37.110s, episode steps: 1260, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.002664, mae: 1.057024, mean_q: 1.412370, mean_eps: 0.100000\n",
            " 1244386/1750000: episode: 2903, duration: 20.673s, episode steps: 702, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.003478, mae: 1.062300, mean_q: 1.419630, mean_eps: 0.100000\n",
            " 1245335/1750000: episode: 2904, duration: 27.765s, episode steps: 949, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.003335, mae: 1.053659, mean_q: 1.408437, mean_eps: 0.100000\n",
            " 1246026/1750000: episode: 2905, duration: 20.008s, episode steps: 691, steps per second:  35, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002678, mae: 1.054342, mean_q: 1.409971, mean_eps: 0.100000\n",
            " 1246655/1750000: episode: 2906, duration: 18.361s, episode steps: 629, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.003059, mae: 1.057304, mean_q: 1.413401, mean_eps: 0.100000\n",
            " 1247585/1750000: episode: 2907, duration: 27.436s, episode steps: 930, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.003313, mae: 1.054813, mean_q: 1.409238, mean_eps: 0.100000\n",
            " 1248550/1750000: episode: 2908, duration: 28.295s, episode steps: 965, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002699, mae: 1.055021, mean_q: 1.411179, mean_eps: 0.100000\n",
            " 1249283/1750000: episode: 2909, duration: 21.449s, episode steps: 733, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.323 [0.000, 3.000],  loss: 0.003188, mae: 1.058506, mean_q: 1.415255, mean_eps: 0.100000\n",
            " 1250367/1750000: episode: 2910, duration: 31.680s, episode steps: 1084, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002878, mae: 1.062792, mean_q: 1.421620, mean_eps: 0.100000\n",
            " 1251077/1750000: episode: 2911, duration: 20.754s, episode steps: 710, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002442, mae: 1.084831, mean_q: 1.450504, mean_eps: 0.100000\n",
            " 1251970/1750000: episode: 2912, duration: 26.227s, episode steps: 893, steps per second:  34, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002515, mae: 1.080624, mean_q: 1.444711, mean_eps: 0.100000\n",
            " 1252928/1750000: episode: 2913, duration: 28.138s, episode steps: 958, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.120 [0.000, 3.000],  loss: 0.003050, mae: 1.078469, mean_q: 1.442255, mean_eps: 0.100000\n",
            " 1253735/1750000: episode: 2914, duration: 23.778s, episode steps: 807, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.003269, mae: 1.079174, mean_q: 1.443893, mean_eps: 0.100000\n",
            " 1254604/1750000: episode: 2915, duration: 25.469s, episode steps: 869, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.003379, mae: 1.074937, mean_q: 1.437922, mean_eps: 0.100000\n",
            " 1255764/1750000: episode: 2916, duration: 34.237s, episode steps: 1160, steps per second:  34, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.002616, mae: 1.072453, mean_q: 1.433888, mean_eps: 0.100000\n",
            " 1256705/1750000: episode: 2917, duration: 27.298s, episode steps: 941, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002789, mae: 1.077333, mean_q: 1.440590, mean_eps: 0.100000\n",
            " 1257408/1750000: episode: 2918, duration: 20.470s, episode steps: 703, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.841 [0.000, 3.000],  loss: 0.003107, mae: 1.078959, mean_q: 1.443989, mean_eps: 0.100000\n",
            " 1258191/1750000: episode: 2919, duration: 22.944s, episode steps: 783, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.002733, mae: 1.076785, mean_q: 1.439878, mean_eps: 0.100000\n",
            " 1258943/1750000: episode: 2920, duration: 21.980s, episode steps: 752, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.002736, mae: 1.069912, mean_q: 1.430393, mean_eps: 0.100000\n",
            " 1260024/1750000: episode: 2921, duration: 31.907s, episode steps: 1081, steps per second:  34, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.002907, mae: 1.075549, mean_q: 1.438645, mean_eps: 0.100000\n",
            " 1261106/1750000: episode: 2922, duration: 31.538s, episode steps: 1082, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002990, mae: 1.086612, mean_q: 1.451779, mean_eps: 0.100000\n",
            " 1261901/1750000: episode: 2923, duration: 23.392s, episode steps: 795, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.003162, mae: 1.086432, mean_q: 1.451397, mean_eps: 0.100000\n",
            " 1263061/1750000: episode: 2924, duration: 33.944s, episode steps: 1160, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.002466, mae: 1.085551, mean_q: 1.451590, mean_eps: 0.100000\n",
            " 1264206/1750000: episode: 2925, duration: 33.415s, episode steps: 1145, steps per second:  34, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002496, mae: 1.087804, mean_q: 1.454948, mean_eps: 0.100000\n",
            " 1265115/1750000: episode: 2926, duration: 26.669s, episode steps: 909, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.003314, mae: 1.088189, mean_q: 1.456024, mean_eps: 0.100000\n",
            " 1265891/1750000: episode: 2927, duration: 22.669s, episode steps: 776, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002865, mae: 1.085742, mean_q: 1.451353, mean_eps: 0.100000\n",
            " 1267004/1750000: episode: 2928, duration: 32.680s, episode steps: 1113, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.844 [0.000, 3.000],  loss: 0.003486, mae: 1.087377, mean_q: 1.454347, mean_eps: 0.100000\n",
            " 1267755/1750000: episode: 2929, duration: 21.862s, episode steps: 751, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002570, mae: 1.082260, mean_q: 1.446426, mean_eps: 0.100000\n",
            " 1268565/1750000: episode: 2930, duration: 23.517s, episode steps: 810, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.002835, mae: 1.086158, mean_q: 1.451906, mean_eps: 0.100000\n",
            " 1269658/1750000: episode: 2931, duration: 32.161s, episode steps: 1093, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002737, mae: 1.086919, mean_q: 1.452995, mean_eps: 0.100000\n",
            " 1270466/1750000: episode: 2932, duration: 23.841s, episode steps: 808, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.002392, mae: 1.090363, mean_q: 1.457538, mean_eps: 0.100000\n",
            " 1271738/1750000: episode: 2933, duration: 37.065s, episode steps: 1272, steps per second:  34, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.002783, mae: 1.087479, mean_q: 1.453490, mean_eps: 0.100000\n",
            " 1272958/1750000: episode: 2934, duration: 36.081s, episode steps: 1220, steps per second:  34, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.003236, mae: 1.091581, mean_q: 1.459120, mean_eps: 0.100000\n",
            " 1274507/1750000: episode: 2935, duration: 45.421s, episode steps: 1549, steps per second:  34, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.878 [0.000, 3.000],  loss: 0.002703, mae: 1.091100, mean_q: 1.458205, mean_eps: 0.100000\n",
            " 1275995/1750000: episode: 2936, duration: 43.511s, episode steps: 1488, steps per second:  34, episode reward: 26.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.903 [0.000, 3.000],  loss: 0.002491, mae: 1.090579, mean_q: 1.457827, mean_eps: 0.100000\n",
            " 1277519/1750000: episode: 2937, duration: 44.681s, episode steps: 1524, steps per second:  34, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002284, mae: 1.092308, mean_q: 1.459987, mean_eps: 0.100000\n",
            " 1278606/1750000: episode: 2938, duration: 31.889s, episode steps: 1087, steps per second:  34, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002480, mae: 1.090523, mean_q: 1.458383, mean_eps: 0.100000\n",
            " 1279361/1750000: episode: 2939, duration: 22.257s, episode steps: 755, steps per second:  34, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002403, mae: 1.090764, mean_q: 1.457014, mean_eps: 0.100000\n",
            " 1280733/1750000: episode: 2940, duration: 40.196s, episode steps: 1372, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002303, mae: 1.092620, mean_q: 1.460536, mean_eps: 0.100000\n",
            " 1281534/1750000: episode: 2941, duration: 23.487s, episode steps: 801, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.916 [0.000, 3.000],  loss: 0.002678, mae: 1.104382, mean_q: 1.475315, mean_eps: 0.100000\n",
            " 1282499/1750000: episode: 2942, duration: 28.157s, episode steps: 965, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.003778, mae: 1.101872, mean_q: 1.472143, mean_eps: 0.100000\n",
            " 1283973/1750000: episode: 2943, duration: 43.141s, episode steps: 1474, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.933 [0.000, 3.000],  loss: 0.002863, mae: 1.105488, mean_q: 1.478615, mean_eps: 0.100000\n",
            " 1284944/1750000: episode: 2944, duration: 28.472s, episode steps: 971, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002806, mae: 1.110260, mean_q: 1.483803, mean_eps: 0.100000\n",
            " 1286140/1750000: episode: 2945, duration: 35.202s, episode steps: 1196, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.003193, mae: 1.107057, mean_q: 1.480556, mean_eps: 0.100000\n",
            " 1287012/1750000: episode: 2946, duration: 25.647s, episode steps: 872, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 0.002683, mae: 1.106447, mean_q: 1.479673, mean_eps: 0.100000\n",
            " 1288046/1750000: episode: 2947, duration: 30.173s, episode steps: 1034, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.002460, mae: 1.106405, mean_q: 1.479730, mean_eps: 0.100000\n",
            " 1288900/1750000: episode: 2948, duration: 25.097s, episode steps: 854, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.002865, mae: 1.104306, mean_q: 1.475852, mean_eps: 0.100000\n",
            " 1290174/1750000: episode: 2949, duration: 37.403s, episode steps: 1274, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002822, mae: 1.110453, mean_q: 1.484088, mean_eps: 0.100000\n",
            " 1291559/1750000: episode: 2950, duration: 40.426s, episode steps: 1385, steps per second:  34, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.003047, mae: 1.116225, mean_q: 1.491846, mean_eps: 0.100000\n",
            " 1292701/1750000: episode: 2951, duration: 33.572s, episode steps: 1142, steps per second:  34, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002642, mae: 1.111882, mean_q: 1.486430, mean_eps: 0.100000\n",
            " 1293714/1750000: episode: 2952, duration: 29.667s, episode steps: 1013, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002908, mae: 1.115899, mean_q: 1.491857, mean_eps: 0.100000\n",
            " 1294638/1750000: episode: 2953, duration: 27.030s, episode steps: 924, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.002897, mae: 1.116778, mean_q: 1.493895, mean_eps: 0.100000\n",
            " 1296020/1750000: episode: 2954, duration: 40.427s, episode steps: 1382, steps per second:  34, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.002523, mae: 1.113231, mean_q: 1.488253, mean_eps: 0.100000\n",
            " 1296896/1750000: episode: 2955, duration: 25.571s, episode steps: 876, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.002473, mae: 1.117532, mean_q: 1.494206, mean_eps: 0.100000\n",
            " 1298086/1750000: episode: 2956, duration: 35.092s, episode steps: 1190, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002515, mae: 1.120977, mean_q: 1.499347, mean_eps: 0.100000\n",
            " 1298965/1750000: episode: 2957, duration: 25.683s, episode steps: 879, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.002912, mae: 1.119456, mean_q: 1.497083, mean_eps: 0.100000\n",
            " 1299751/1750000: episode: 2958, duration: 22.829s, episode steps: 786, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.002813, mae: 1.110879, mean_q: 1.486002, mean_eps: 0.100000\n",
            " 1301066/1750000: episode: 2959, duration: 38.739s, episode steps: 1315, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.736 [0.000, 3.000],  loss: 0.003123, mae: 1.122265, mean_q: 1.499111, mean_eps: 0.100000\n",
            " 1302162/1750000: episode: 2960, duration: 31.952s, episode steps: 1096, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002709, mae: 1.128559, mean_q: 1.506982, mean_eps: 0.100000\n",
            " 1303194/1750000: episode: 2961, duration: 30.183s, episode steps: 1032, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002714, mae: 1.123101, mean_q: 1.500010, mean_eps: 0.100000\n",
            " 1304063/1750000: episode: 2962, duration: 25.367s, episode steps: 869, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002911, mae: 1.125881, mean_q: 1.503274, mean_eps: 0.100000\n",
            " 1305311/1750000: episode: 2963, duration: 36.435s, episode steps: 1248, steps per second:  34, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.886 [0.000, 3.000],  loss: 0.002511, mae: 1.127375, mean_q: 1.506305, mean_eps: 0.100000\n",
            " 1306579/1750000: episode: 2964, duration: 37.149s, episode steps: 1268, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.797 [0.000, 3.000],  loss: 0.002517, mae: 1.123924, mean_q: 1.501596, mean_eps: 0.100000\n",
            " 1307630/1750000: episode: 2965, duration: 30.781s, episode steps: 1051, steps per second:  34, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002510, mae: 1.125313, mean_q: 1.503738, mean_eps: 0.100000\n",
            " 1308441/1750000: episode: 2966, duration: 23.638s, episode steps: 811, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002342, mae: 1.130036, mean_q: 1.510398, mean_eps: 0.100000\n",
            " 1309156/1750000: episode: 2967, duration: 20.944s, episode steps: 715, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.003134, mae: 1.122224, mean_q: 1.499270, mean_eps: 0.100000\n",
            " 1310254/1750000: episode: 2968, duration: 32.133s, episode steps: 1098, steps per second:  34, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002890, mae: 1.129034, mean_q: 1.508606, mean_eps: 0.100000\n",
            " 1311411/1750000: episode: 2969, duration: 33.964s, episode steps: 1157, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.897 [0.000, 3.000],  loss: 0.002675, mae: 1.132331, mean_q: 1.512683, mean_eps: 0.100000\n",
            " 1312448/1750000: episode: 2970, duration: 30.371s, episode steps: 1037, steps per second:  34, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002769, mae: 1.133401, mean_q: 1.514523, mean_eps: 0.100000\n",
            " 1313909/1750000: episode: 2971, duration: 42.984s, episode steps: 1461, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002798, mae: 1.133989, mean_q: 1.514676, mean_eps: 0.100000\n",
            " 1314721/1750000: episode: 2972, duration: 23.834s, episode steps: 812, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002888, mae: 1.129973, mean_q: 1.510087, mean_eps: 0.100000\n",
            " 1315661/1750000: episode: 2973, duration: 27.696s, episode steps: 940, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002739, mae: 1.133919, mean_q: 1.515527, mean_eps: 0.100000\n",
            " 1316543/1750000: episode: 2974, duration: 25.826s, episode steps: 882, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.002788, mae: 1.135430, mean_q: 1.517101, mean_eps: 0.100000\n",
            " 1317509/1750000: episode: 2975, duration: 28.988s, episode steps: 966, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002547, mae: 1.136830, mean_q: 1.518726, mean_eps: 0.100000\n",
            " 1318526/1750000: episode: 2976, duration: 30.592s, episode steps: 1017, steps per second:  33, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.952 [0.000, 3.000],  loss: 0.003041, mae: 1.136153, mean_q: 1.519420, mean_eps: 0.100000\n",
            " 1319569/1750000: episode: 2977, duration: 31.571s, episode steps: 1043, steps per second:  33, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.003217, mae: 1.136160, mean_q: 1.518382, mean_eps: 0.100000\n",
            " 1320507/1750000: episode: 2978, duration: 28.047s, episode steps: 938, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002759, mae: 1.143685, mean_q: 1.528742, mean_eps: 0.100000\n",
            " 1321431/1750000: episode: 2979, duration: 27.914s, episode steps: 924, steps per second:  33, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.002322, mae: 1.144854, mean_q: 1.531821, mean_eps: 0.100000\n",
            " 1322363/1750000: episode: 2980, duration: 27.883s, episode steps: 932, steps per second:  33, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.003211, mae: 1.145160, mean_q: 1.531255, mean_eps: 0.100000\n",
            " 1323391/1750000: episode: 2981, duration: 30.868s, episode steps: 1028, steps per second:  33, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.002325, mae: 1.149037, mean_q: 1.535828, mean_eps: 0.100000\n",
            " 1324536/1750000: episode: 2982, duration: 34.339s, episode steps: 1145, steps per second:  33, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.002836, mae: 1.146381, mean_q: 1.532379, mean_eps: 0.100000\n",
            " 1325626/1750000: episode: 2983, duration: 32.969s, episode steps: 1090, steps per second:  33, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002059, mae: 1.147283, mean_q: 1.534417, mean_eps: 0.100000\n",
            " 1326811/1750000: episode: 2984, duration: 35.620s, episode steps: 1185, steps per second:  33, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.002443, mae: 1.143521, mean_q: 1.528445, mean_eps: 0.100000\n",
            " 1327766/1750000: episode: 2985, duration: 28.846s, episode steps: 955, steps per second:  33, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002709, mae: 1.140415, mean_q: 1.524886, mean_eps: 0.100000\n",
            " 1328625/1750000: episode: 2986, duration: 25.895s, episode steps: 859, steps per second:  33, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.002542, mae: 1.147454, mean_q: 1.534448, mean_eps: 0.100000\n",
            " 1329391/1750000: episode: 2987, duration: 23.115s, episode steps: 766, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.002477, mae: 1.143015, mean_q: 1.528728, mean_eps: 0.100000\n",
            " 1330315/1750000: episode: 2988, duration: 27.799s, episode steps: 924, steps per second:  33, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002993, mae: 1.147099, mean_q: 1.532811, mean_eps: 0.100000\n",
            " 1330950/1750000: episode: 2989, duration: 19.164s, episode steps: 635, steps per second:  33, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002190, mae: 1.152520, mean_q: 1.539397, mean_eps: 0.100000\n",
            " 1332052/1750000: episode: 2990, duration: 33.289s, episode steps: 1102, steps per second:  33, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.003010, mae: 1.146971, mean_q: 1.531787, mean_eps: 0.100000\n",
            " 1333142/1750000: episode: 2991, duration: 32.994s, episode steps: 1090, steps per second:  33, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002380, mae: 1.154972, mean_q: 1.543464, mean_eps: 0.100000\n",
            " 1333670/1750000: episode: 2992, duration: 15.986s, episode steps: 528, steps per second:  33, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.003092, mae: 1.150114, mean_q: 1.537149, mean_eps: 0.100000\n",
            " 1334320/1750000: episode: 2993, duration: 19.560s, episode steps: 650, steps per second:  33, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002178, mae: 1.150109, mean_q: 1.536738, mean_eps: 0.100000\n",
            " 1335651/1750000: episode: 2994, duration: 40.446s, episode steps: 1331, steps per second:  33, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002551, mae: 1.158020, mean_q: 1.547740, mean_eps: 0.100000\n",
            " 1336694/1750000: episode: 2995, duration: 31.426s, episode steps: 1043, steps per second:  33, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.003011, mae: 1.151345, mean_q: 1.538068, mean_eps: 0.100000\n",
            " 1337750/1750000: episode: 2996, duration: 31.773s, episode steps: 1056, steps per second:  33, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.375 [0.000, 3.000],  loss: 0.002930, mae: 1.156831, mean_q: 1.545475, mean_eps: 0.100000\n",
            " 1338776/1750000: episode: 2997, duration: 30.877s, episode steps: 1026, steps per second:  33, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.002137, mae: 1.149975, mean_q: 1.537530, mean_eps: 0.100000\n",
            " 1340072/1750000: episode: 2998, duration: 38.552s, episode steps: 1296, steps per second:  34, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002466, mae: 1.153466, mean_q: 1.541526, mean_eps: 0.100000\n",
            " 1340794/1750000: episode: 2999, duration: 21.064s, episode steps: 722, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.002371, mae: 1.162314, mean_q: 1.552777, mean_eps: 0.100000\n",
            " 1342000/1750000: episode: 3000, duration: 35.745s, episode steps: 1206, steps per second:  34, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002215, mae: 1.159210, mean_q: 1.549450, mean_eps: 0.100000\n",
            " 1343019/1750000: episode: 3001, duration: 29.972s, episode steps: 1019, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002554, mae: 1.163280, mean_q: 1.554493, mean_eps: 0.100000\n",
            " 1343817/1750000: episode: 3002, duration: 23.618s, episode steps: 798, steps per second:  34, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002305, mae: 1.164604, mean_q: 1.555012, mean_eps: 0.100000\n",
            " 1344486/1750000: episode: 3003, duration: 19.451s, episode steps: 669, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002698, mae: 1.159033, mean_q: 1.548543, mean_eps: 0.100000\n",
            " 1345474/1750000: episode: 3004, duration: 28.872s, episode steps: 988, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.850 [0.000, 3.000],  loss: 0.002215, mae: 1.159593, mean_q: 1.549236, mean_eps: 0.100000\n",
            " 1346458/1750000: episode: 3005, duration: 28.959s, episode steps: 984, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002593, mae: 1.164133, mean_q: 1.555106, mean_eps: 0.100000\n",
            " 1347374/1750000: episode: 3006, duration: 26.949s, episode steps: 916, steps per second:  34, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.002493, mae: 1.166091, mean_q: 1.558045, mean_eps: 0.100000\n",
            " 1348230/1750000: episode: 3007, duration: 25.101s, episode steps: 856, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002739, mae: 1.159931, mean_q: 1.549948, mean_eps: 0.100000\n",
            " 1349009/1750000: episode: 3008, duration: 22.736s, episode steps: 779, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.003001, mae: 1.160143, mean_q: 1.549545, mean_eps: 0.100000\n",
            " 1350110/1750000: episode: 3009, duration: 32.311s, episode steps: 1101, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002515, mae: 1.167555, mean_q: 1.559398, mean_eps: 0.100000\n",
            " 1350968/1750000: episode: 3010, duration: 25.096s, episode steps: 858, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.003058, mae: 1.155828, mean_q: 1.543906, mean_eps: 0.100000\n",
            " 1351713/1750000: episode: 3011, duration: 21.930s, episode steps: 745, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.003061, mae: 1.159877, mean_q: 1.550448, mean_eps: 0.100000\n",
            " 1352824/1750000: episode: 3012, duration: 32.699s, episode steps: 1111, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002767, mae: 1.153533, mean_q: 1.541632, mean_eps: 0.100000\n",
            " 1353543/1750000: episode: 3013, duration: 21.094s, episode steps: 719, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.002751, mae: 1.151022, mean_q: 1.538207, mean_eps: 0.100000\n",
            " 1354228/1750000: episode: 3014, duration: 20.387s, episode steps: 685, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.003294, mae: 1.154172, mean_q: 1.542210, mean_eps: 0.100000\n",
            " 1355420/1750000: episode: 3015, duration: 35.096s, episode steps: 1192, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.003071, mae: 1.152457, mean_q: 1.540448, mean_eps: 0.100000\n",
            " 1356360/1750000: episode: 3016, duration: 27.581s, episode steps: 940, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002510, mae: 1.159184, mean_q: 1.547804, mean_eps: 0.100000\n",
            " 1357165/1750000: episode: 3017, duration: 23.530s, episode steps: 805, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002423, mae: 1.156199, mean_q: 1.544502, mean_eps: 0.100000\n",
            " 1358624/1750000: episode: 3018, duration: 42.887s, episode steps: 1459, steps per second:  34, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002348, mae: 1.155325, mean_q: 1.543764, mean_eps: 0.100000\n",
            " 1359585/1750000: episode: 3019, duration: 28.175s, episode steps: 961, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.002503, mae: 1.155065, mean_q: 1.543346, mean_eps: 0.100000\n",
            " 1360357/1750000: episode: 3020, duration: 22.497s, episode steps: 772, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.003071, mae: 1.157565, mean_q: 1.548208, mean_eps: 0.100000\n",
            " 1361369/1750000: episode: 3021, duration: 29.554s, episode steps: 1012, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002402, mae: 1.159846, mean_q: 1.550065, mean_eps: 0.100000\n",
            " 1362669/1750000: episode: 3022, duration: 38.134s, episode steps: 1300, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002830, mae: 1.155783, mean_q: 1.544240, mean_eps: 0.100000\n",
            " 1363629/1750000: episode: 3023, duration: 28.281s, episode steps: 960, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.002474, mae: 1.151540, mean_q: 1.538723, mean_eps: 0.100000\n",
            " 1365045/1750000: episode: 3024, duration: 41.521s, episode steps: 1416, steps per second:  34, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.002603, mae: 1.157531, mean_q: 1.547286, mean_eps: 0.100000\n",
            " 1366309/1750000: episode: 3025, duration: 37.095s, episode steps: 1264, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.003078, mae: 1.153653, mean_q: 1.541940, mean_eps: 0.100000\n",
            " 1367334/1750000: episode: 3026, duration: 30.027s, episode steps: 1025, steps per second:  34, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.002645, mae: 1.152946, mean_q: 1.540511, mean_eps: 0.100000\n",
            " 1368016/1750000: episode: 3027, duration: 20.362s, episode steps: 682, steps per second:  33, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.002429, mae: 1.159458, mean_q: 1.550245, mean_eps: 0.100000\n",
            " 1368836/1750000: episode: 3028, duration: 24.870s, episode steps: 820, steps per second:  33, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002727, mae: 1.158185, mean_q: 1.548523, mean_eps: 0.100000\n",
            " 1369845/1750000: episode: 3029, duration: 29.822s, episode steps: 1009, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002696, mae: 1.153832, mean_q: 1.542521, mean_eps: 0.100000\n",
            " 1370323/1750000: episode: 3030, duration: 13.912s, episode steps: 478, steps per second:  34, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002597, mae: 1.168692, mean_q: 1.561871, mean_eps: 0.100000\n",
            " 1371012/1750000: episode: 3031, duration: 20.408s, episode steps: 689, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002299, mae: 1.164906, mean_q: 1.557440, mean_eps: 0.100000\n",
            " 1371962/1750000: episode: 3032, duration: 27.963s, episode steps: 950, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.003192, mae: 1.165615, mean_q: 1.556470, mean_eps: 0.100000\n",
            " 1373082/1750000: episode: 3033, duration: 32.946s, episode steps: 1120, steps per second:  34, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.002523, mae: 1.170357, mean_q: 1.564194, mean_eps: 0.100000\n",
            " 1374004/1750000: episode: 3034, duration: 27.404s, episode steps: 922, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002967, mae: 1.162048, mean_q: 1.553483, mean_eps: 0.100000\n",
            " 1374935/1750000: episode: 3035, duration: 27.515s, episode steps: 931, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.002609, mae: 1.163531, mean_q: 1.554811, mean_eps: 0.100000\n",
            " 1375897/1750000: episode: 3036, duration: 28.440s, episode steps: 962, steps per second:  34, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002893, mae: 1.166688, mean_q: 1.558601, mean_eps: 0.100000\n",
            " 1377853/1750000: episode: 3037, duration: 57.686s, episode steps: 1956, steps per second:  34, episode reward: 37.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.002542, mae: 1.163947, mean_q: 1.556179, mean_eps: 0.100000\n",
            " 1378958/1750000: episode: 3038, duration: 32.419s, episode steps: 1105, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.830 [0.000, 3.000],  loss: 0.002787, mae: 1.161347, mean_q: 1.551784, mean_eps: 0.100000\n",
            " 1380097/1750000: episode: 3039, duration: 34.081s, episode steps: 1139, steps per second:  33, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002767, mae: 1.165070, mean_q: 1.556589, mean_eps: 0.100000\n",
            " 1380978/1750000: episode: 3040, duration: 26.019s, episode steps: 881, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002368, mae: 1.171701, mean_q: 1.566001, mean_eps: 0.100000\n",
            " 1382525/1750000: episode: 3041, duration: 45.554s, episode steps: 1547, steps per second:  34, episode reward: 25.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.002431, mae: 1.175222, mean_q: 1.570126, mean_eps: 0.100000\n",
            " 1383346/1750000: episode: 3042, duration: 24.243s, episode steps: 821, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002892, mae: 1.182570, mean_q: 1.579316, mean_eps: 0.100000\n",
            " 1384150/1750000: episode: 3043, duration: 23.969s, episode steps: 804, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.337 [0.000, 3.000],  loss: 0.002073, mae: 1.178532, mean_q: 1.573761, mean_eps: 0.100000\n",
            " 1385078/1750000: episode: 3044, duration: 27.389s, episode steps: 928, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.816 [0.000, 3.000],  loss: 0.002883, mae: 1.172525, mean_q: 1.566490, mean_eps: 0.100000\n",
            " 1385772/1750000: episode: 3045, duration: 20.695s, episode steps: 694, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.002399, mae: 1.177044, mean_q: 1.573493, mean_eps: 0.100000\n",
            " 1386389/1750000: episode: 3046, duration: 18.318s, episode steps: 617, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.003661, mae: 1.173843, mean_q: 1.568084, mean_eps: 0.100000\n",
            " 1387065/1750000: episode: 3047, duration: 19.882s, episode steps: 676, steps per second:  34, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.287 [0.000, 3.000],  loss: 0.002864, mae: 1.171028, mean_q: 1.564435, mean_eps: 0.100000\n",
            " 1387851/1750000: episode: 3048, duration: 23.273s, episode steps: 786, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.002957, mae: 1.175588, mean_q: 1.570239, mean_eps: 0.100000\n",
            " 1388748/1750000: episode: 3049, duration: 26.527s, episode steps: 897, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.003104, mae: 1.171673, mean_q: 1.565376, mean_eps: 0.100000\n",
            " 1390401/1750000: episode: 3050, duration: 48.568s, episode steps: 1653, steps per second:  34, episode reward: 31.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.002326, mae: 1.177970, mean_q: 1.574249, mean_eps: 0.100000\n",
            " 1391457/1750000: episode: 3051, duration: 30.843s, episode steps: 1056, steps per second:  34, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.002782, mae: 1.179946, mean_q: 1.576310, mean_eps: 0.100000\n",
            " 1392446/1750000: episode: 3052, duration: 29.050s, episode steps: 989, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002625, mae: 1.175777, mean_q: 1.570518, mean_eps: 0.100000\n",
            " 1393360/1750000: episode: 3053, duration: 27.000s, episode steps: 914, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002795, mae: 1.173161, mean_q: 1.567218, mean_eps: 0.100000\n",
            " 1394183/1750000: episode: 3054, duration: 24.406s, episode steps: 823, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.002741, mae: 1.171943, mean_q: 1.565646, mean_eps: 0.100000\n",
            " 1394906/1750000: episode: 3055, duration: 21.360s, episode steps: 723, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002760, mae: 1.176396, mean_q: 1.570911, mean_eps: 0.100000\n",
            " 1396099/1750000: episode: 3056, duration: 34.964s, episode steps: 1193, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.002690, mae: 1.175347, mean_q: 1.569933, mean_eps: 0.100000\n",
            " 1396929/1750000: episode: 3057, duration: 24.348s, episode steps: 830, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.003215, mae: 1.175063, mean_q: 1.569823, mean_eps: 0.100000\n",
            " 1397899/1750000: episode: 3058, duration: 28.387s, episode steps: 970, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.002478, mae: 1.179096, mean_q: 1.574739, mean_eps: 0.100000\n",
            " 1398667/1750000: episode: 3059, duration: 22.695s, episode steps: 768, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002324, mae: 1.175981, mean_q: 1.570815, mean_eps: 0.100000\n",
            " 1399340/1750000: episode: 3060, duration: 19.870s, episode steps: 673, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.002628, mae: 1.172283, mean_q: 1.565301, mean_eps: 0.100000\n",
            " 1400528/1750000: episode: 3061, duration: 35.109s, episode steps: 1188, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002499, mae: 1.179273, mean_q: 1.576437, mean_eps: 0.100000\n",
            " 1401674/1750000: episode: 3062, duration: 33.592s, episode steps: 1146, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002533, mae: 1.185468, mean_q: 1.583808, mean_eps: 0.100000\n",
            " 1403048/1750000: episode: 3063, duration: 40.565s, episode steps: 1374, steps per second:  34, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.002369, mae: 1.189565, mean_q: 1.589621, mean_eps: 0.100000\n",
            " 1403707/1750000: episode: 3064, duration: 19.387s, episode steps: 659, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.002576, mae: 1.187691, mean_q: 1.585525, mean_eps: 0.100000\n",
            " 1404925/1750000: episode: 3065, duration: 36.083s, episode steps: 1218, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.723 [0.000, 3.000],  loss: 0.002859, mae: 1.190271, mean_q: 1.589783, mean_eps: 0.100000\n",
            " 1405792/1750000: episode: 3066, duration: 25.605s, episode steps: 867, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.003215, mae: 1.183888, mean_q: 1.581378, mean_eps: 0.100000\n",
            " 1406688/1750000: episode: 3067, duration: 26.582s, episode steps: 896, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.002956, mae: 1.185660, mean_q: 1.583213, mean_eps: 0.100000\n",
            " 1408150/1750000: episode: 3068, duration: 43.031s, episode steps: 1462, steps per second:  34, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002543, mae: 1.185756, mean_q: 1.583518, mean_eps: 0.100000\n",
            " 1408893/1750000: episode: 3069, duration: 21.965s, episode steps: 743, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002700, mae: 1.182895, mean_q: 1.578668, mean_eps: 0.100000\n",
            " 1410396/1750000: episode: 3070, duration: 43.800s, episode steps: 1503, steps per second:  34, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002577, mae: 1.191140, mean_q: 1.589622, mean_eps: 0.100000\n",
            " 1411238/1750000: episode: 3071, duration: 24.721s, episode steps: 842, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002897, mae: 1.201890, mean_q: 1.604071, mean_eps: 0.100000\n",
            " 1411854/1750000: episode: 3072, duration: 17.977s, episode steps: 616, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002623, mae: 1.205057, mean_q: 1.609727, mean_eps: 0.100000\n",
            " 1412632/1750000: episode: 3073, duration: 23.093s, episode steps: 778, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002560, mae: 1.195206, mean_q: 1.596546, mean_eps: 0.100000\n",
            " 1413187/1750000: episode: 3074, duration: 16.410s, episode steps: 555, steps per second:  34, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.003408, mae: 1.203830, mean_q: 1.607934, mean_eps: 0.100000\n",
            " 1414231/1750000: episode: 3075, duration: 30.860s, episode steps: 1044, steps per second:  34, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.839 [0.000, 3.000],  loss: 0.003092, mae: 1.200632, mean_q: 1.603565, mean_eps: 0.100000\n",
            " 1415100/1750000: episode: 3076, duration: 25.831s, episode steps: 869, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.003160, mae: 1.205098, mean_q: 1.610747, mean_eps: 0.100000\n",
            " 1415832/1750000: episode: 3077, duration: 21.630s, episode steps: 732, steps per second:  34, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.907 [0.000, 3.000],  loss: 0.002361, mae: 1.199499, mean_q: 1.602737, mean_eps: 0.100000\n",
            " 1416826/1750000: episode: 3078, duration: 29.372s, episode steps: 994, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.003309, mae: 1.206057, mean_q: 1.610720, mean_eps: 0.100000\n",
            " 1417765/1750000: episode: 3079, duration: 27.575s, episode steps: 939, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002568, mae: 1.200435, mean_q: 1.602613, mean_eps: 0.100000\n",
            " 1418730/1750000: episode: 3080, duration: 28.474s, episode steps: 965, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002254, mae: 1.200320, mean_q: 1.603147, mean_eps: 0.100000\n",
            " 1419450/1750000: episode: 3081, duration: 21.198s, episode steps: 720, steps per second:  34, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.002742, mae: 1.204721, mean_q: 1.608429, mean_eps: 0.100000\n",
            " 1420405/1750000: episode: 3082, duration: 28.265s, episode steps: 955, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002805, mae: 1.201630, mean_q: 1.604466, mean_eps: 0.100000\n",
            " 1421331/1750000: episode: 3083, duration: 27.466s, episode steps: 926, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002881, mae: 1.200948, mean_q: 1.602774, mean_eps: 0.100000\n",
            " 1422813/1750000: episode: 3084, duration: 43.204s, episode steps: 1482, steps per second:  34, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002424, mae: 1.204654, mean_q: 1.609390, mean_eps: 0.100000\n",
            " 1423703/1750000: episode: 3085, duration: 26.250s, episode steps: 890, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002448, mae: 1.208108, mean_q: 1.614033, mean_eps: 0.100000\n",
            " 1424717/1750000: episode: 3086, duration: 30.026s, episode steps: 1014, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002599, mae: 1.200686, mean_q: 1.604228, mean_eps: 0.100000\n",
            " 1425797/1750000: episode: 3087, duration: 31.726s, episode steps: 1080, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.002871, mae: 1.204015, mean_q: 1.607241, mean_eps: 0.100000\n",
            " 1427026/1750000: episode: 3088, duration: 36.510s, episode steps: 1229, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002456, mae: 1.200778, mean_q: 1.602702, mean_eps: 0.100000\n",
            " 1428088/1750000: episode: 3089, duration: 31.254s, episode steps: 1062, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.002582, mae: 1.206245, mean_q: 1.610088, mean_eps: 0.100000\n",
            " 1429161/1750000: episode: 3090, duration: 31.720s, episode steps: 1073, steps per second:  34, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002792, mae: 1.203129, mean_q: 1.606595, mean_eps: 0.100000\n",
            " 1429965/1750000: episode: 3091, duration: 23.610s, episode steps: 804, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002523, mae: 1.202965, mean_q: 1.606693, mean_eps: 0.100000\n",
            " 1430615/1750000: episode: 3092, duration: 19.213s, episode steps: 650, steps per second:  34, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002266, mae: 1.202154, mean_q: 1.605932, mean_eps: 0.100000\n",
            " 1431583/1750000: episode: 3093, duration: 28.620s, episode steps: 968, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002547, mae: 1.208679, mean_q: 1.614154, mean_eps: 0.100000\n",
            " 1432551/1750000: episode: 3094, duration: 28.514s, episode steps: 968, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.386 [0.000, 3.000],  loss: 0.002952, mae: 1.211043, mean_q: 1.616646, mean_eps: 0.100000\n",
            " 1433440/1750000: episode: 3095, duration: 26.070s, episode steps: 889, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002704, mae: 1.206758, mean_q: 1.612347, mean_eps: 0.100000\n",
            " 1434413/1750000: episode: 3096, duration: 28.541s, episode steps: 973, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.780 [0.000, 3.000],  loss: 0.003164, mae: 1.207794, mean_q: 1.614088, mean_eps: 0.100000\n",
            " 1435327/1750000: episode: 3097, duration: 27.122s, episode steps: 914, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002840, mae: 1.207123, mean_q: 1.612167, mean_eps: 0.100000\n",
            " 1436853/1750000: episode: 3098, duration: 45.140s, episode steps: 1526, steps per second:  34, episode reward: 25.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.002530, mae: 1.215343, mean_q: 1.622477, mean_eps: 0.100000\n",
            " 1437721/1750000: episode: 3099, duration: 25.641s, episode steps: 868, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.002689, mae: 1.208897, mean_q: 1.615319, mean_eps: 0.100000\n",
            " 1438624/1750000: episode: 3100, duration: 26.587s, episode steps: 903, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.002508, mae: 1.209152, mean_q: 1.615653, mean_eps: 0.100000\n",
            " 1439555/1750000: episode: 3101, duration: 27.364s, episode steps: 931, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002913, mae: 1.204447, mean_q: 1.608604, mean_eps: 0.100000\n",
            " 1440513/1750000: episode: 3102, duration: 27.882s, episode steps: 958, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.003500, mae: 1.209641, mean_q: 1.615401, mean_eps: 0.100000\n",
            " 1441816/1750000: episode: 3103, duration: 38.392s, episode steps: 1303, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.003058, mae: 1.211305, mean_q: 1.618144, mean_eps: 0.100000\n",
            " 1443114/1750000: episode: 3104, duration: 38.190s, episode steps: 1298, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.002752, mae: 1.213095, mean_q: 1.619542, mean_eps: 0.100000\n",
            " 1443849/1750000: episode: 3105, duration: 21.761s, episode steps: 735, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.002664, mae: 1.215234, mean_q: 1.622090, mean_eps: 0.100000\n",
            " 1444825/1750000: episode: 3106, duration: 28.556s, episode steps: 976, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.002982, mae: 1.214788, mean_q: 1.621831, mean_eps: 0.100000\n",
            " 1445650/1750000: episode: 3107, duration: 24.252s, episode steps: 825, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.003241, mae: 1.213864, mean_q: 1.620665, mean_eps: 0.100000\n",
            " 1447092/1750000: episode: 3108, duration: 42.204s, episode steps: 1442, steps per second:  34, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.003023, mae: 1.210002, mean_q: 1.615466, mean_eps: 0.100000\n",
            " 1447952/1750000: episode: 3109, duration: 25.566s, episode steps: 860, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.003388, mae: 1.210726, mean_q: 1.615684, mean_eps: 0.100000\n",
            " 1448723/1750000: episode: 3110, duration: 22.843s, episode steps: 771, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003195, mae: 1.211827, mean_q: 1.617831, mean_eps: 0.100000\n",
            " 1449565/1750000: episode: 3111, duration: 24.872s, episode steps: 842, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.002608, mae: 1.215376, mean_q: 1.622905, mean_eps: 0.100000\n",
            " 1450570/1750000: episode: 3112, duration: 29.321s, episode steps: 1005, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.003217, mae: 1.217450, mean_q: 1.624943, mean_eps: 0.100000\n",
            " 1452039/1750000: episode: 3113, duration: 43.447s, episode steps: 1469, steps per second:  34, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002487, mae: 1.223287, mean_q: 1.633026, mean_eps: 0.100000\n",
            " 1452822/1750000: episode: 3114, duration: 23.059s, episode steps: 783, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.003463, mae: 1.220609, mean_q: 1.628582, mean_eps: 0.100000\n",
            " 1453463/1750000: episode: 3115, duration: 18.817s, episode steps: 641, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.003051, mae: 1.229108, mean_q: 1.641162, mean_eps: 0.100000\n",
            " 1454366/1750000: episode: 3116, duration: 26.610s, episode steps: 903, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.002732, mae: 1.223188, mean_q: 1.632620, mean_eps: 0.100000\n",
            " 1455270/1750000: episode: 3117, duration: 26.587s, episode steps: 904, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002797, mae: 1.224224, mean_q: 1.634419, mean_eps: 0.100000\n",
            " 1456205/1750000: episode: 3118, duration: 27.714s, episode steps: 935, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.003468, mae: 1.229640, mean_q: 1.641742, mean_eps: 0.100000\n",
            " 1457036/1750000: episode: 3119, duration: 24.344s, episode steps: 831, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002873, mae: 1.220690, mean_q: 1.628350, mean_eps: 0.100000\n",
            " 1458240/1750000: episode: 3120, duration: 35.722s, episode steps: 1204, steps per second:  34, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.003398, mae: 1.225532, mean_q: 1.635290, mean_eps: 0.100000\n",
            " 1459236/1750000: episode: 3121, duration: 29.369s, episode steps: 996, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.002730, mae: 1.227436, mean_q: 1.638153, mean_eps: 0.100000\n",
            " 1460073/1750000: episode: 3122, duration: 24.861s, episode steps: 837, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.002474, mae: 1.230059, mean_q: 1.642383, mean_eps: 0.100000\n",
            " 1461112/1750000: episode: 3123, duration: 30.527s, episode steps: 1039, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002908, mae: 1.226188, mean_q: 1.637394, mean_eps: 0.100000\n",
            " 1462102/1750000: episode: 3124, duration: 29.059s, episode steps: 990, steps per second:  34, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002503, mae: 1.230320, mean_q: 1.642978, mean_eps: 0.100000\n",
            " 1463345/1750000: episode: 3125, duration: 36.588s, episode steps: 1243, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.002486, mae: 1.231120, mean_q: 1.643402, mean_eps: 0.100000\n",
            " 1464513/1750000: episode: 3126, duration: 34.199s, episode steps: 1168, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003151, mae: 1.230956, mean_q: 1.642906, mean_eps: 0.100000\n",
            " 1465557/1750000: episode: 3127, duration: 30.965s, episode steps: 1044, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.002597, mae: 1.229021, mean_q: 1.642157, mean_eps: 0.100000\n",
            " 1466767/1750000: episode: 3128, duration: 35.405s, episode steps: 1210, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.003019, mae: 1.223817, mean_q: 1.633513, mean_eps: 0.100000\n",
            " 1468051/1750000: episode: 3129, duration: 37.742s, episode steps: 1284, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.003098, mae: 1.234205, mean_q: 1.647443, mean_eps: 0.100000\n",
            " 1469066/1750000: episode: 3130, duration: 29.790s, episode steps: 1015, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002890, mae: 1.231464, mean_q: 1.643619, mean_eps: 0.100000\n",
            " 1469718/1750000: episode: 3131, duration: 19.131s, episode steps: 652, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002944, mae: 1.226939, mean_q: 1.637858, mean_eps: 0.100000\n",
            " 1470777/1750000: episode: 3132, duration: 31.042s, episode steps: 1059, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.002751, mae: 1.232667, mean_q: 1.645300, mean_eps: 0.100000\n",
            " 1471759/1750000: episode: 3133, duration: 28.738s, episode steps: 982, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.002857, mae: 1.231398, mean_q: 1.645173, mean_eps: 0.100000\n",
            " 1472774/1750000: episode: 3134, duration: 29.976s, episode steps: 1015, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002575, mae: 1.236502, mean_q: 1.650544, mean_eps: 0.100000\n",
            " 1473502/1750000: episode: 3135, duration: 21.454s, episode steps: 728, steps per second:  34, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.871 [0.000, 3.000],  loss: 0.002765, mae: 1.231809, mean_q: 1.644189, mean_eps: 0.100000\n",
            " 1474271/1750000: episode: 3136, duration: 22.486s, episode steps: 769, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.003056, mae: 1.231330, mean_q: 1.643125, mean_eps: 0.100000\n",
            " 1475441/1750000: episode: 3137, duration: 34.306s, episode steps: 1170, steps per second:  34, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.002426, mae: 1.235362, mean_q: 1.648300, mean_eps: 0.100000\n",
            " 1476335/1750000: episode: 3138, duration: 26.323s, episode steps: 894, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002814, mae: 1.232493, mean_q: 1.645034, mean_eps: 0.100000\n",
            " 1477516/1750000: episode: 3139, duration: 34.589s, episode steps: 1181, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002633, mae: 1.237741, mean_q: 1.652941, mean_eps: 0.100000\n",
            " 1478619/1750000: episode: 3140, duration: 32.134s, episode steps: 1103, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002464, mae: 1.237343, mean_q: 1.652593, mean_eps: 0.100000\n",
            " 1479398/1750000: episode: 3141, duration: 23.166s, episode steps: 779, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.003305, mae: 1.229353, mean_q: 1.641386, mean_eps: 0.100000\n",
            " 1480198/1750000: episode: 3142, duration: 23.590s, episode steps: 800, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002201, mae: 1.236819, mean_q: 1.651600, mean_eps: 0.100000\n",
            " 1481437/1750000: episode: 3143, duration: 36.412s, episode steps: 1239, steps per second:  34, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002900, mae: 1.239262, mean_q: 1.653971, mean_eps: 0.100000\n",
            " 1482579/1750000: episode: 3144, duration: 33.580s, episode steps: 1142, steps per second:  34, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.002988, mae: 1.237231, mean_q: 1.651805, mean_eps: 0.100000\n",
            " 1483461/1750000: episode: 3145, duration: 25.964s, episode steps: 882, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.002745, mae: 1.244486, mean_q: 1.661772, mean_eps: 0.100000\n",
            " 1484167/1750000: episode: 3146, duration: 20.695s, episode steps: 706, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.002826, mae: 1.245283, mean_q: 1.663502, mean_eps: 0.100000\n",
            " 1485204/1750000: episode: 3147, duration: 30.418s, episode steps: 1037, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002829, mae: 1.243061, mean_q: 1.660257, mean_eps: 0.100000\n",
            " 1486186/1750000: episode: 3148, duration: 29.137s, episode steps: 982, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.003212, mae: 1.237134, mean_q: 1.651467, mean_eps: 0.100000\n",
            " 1486803/1750000: episode: 3149, duration: 18.260s, episode steps: 617, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002884, mae: 1.246942, mean_q: 1.663518, mean_eps: 0.100000\n",
            " 1487729/1750000: episode: 3150, duration: 27.439s, episode steps: 926, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.003293, mae: 1.239695, mean_q: 1.654118, mean_eps: 0.100000\n",
            " 1488614/1750000: episode: 3151, duration: 26.039s, episode steps: 885, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002864, mae: 1.240612, mean_q: 1.655804, mean_eps: 0.100000\n",
            " 1489555/1750000: episode: 3152, duration: 27.674s, episode steps: 941, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002528, mae: 1.240929, mean_q: 1.656208, mean_eps: 0.100000\n",
            " 1490215/1750000: episode: 3153, duration: 19.602s, episode steps: 660, steps per second:  34, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.003001, mae: 1.246064, mean_q: 1.664083, mean_eps: 0.100000\n",
            " 1491471/1750000: episode: 3154, duration: 36.934s, episode steps: 1256, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002415, mae: 1.252859, mean_q: 1.672975, mean_eps: 0.100000\n",
            " 1492584/1750000: episode: 3155, duration: 32.914s, episode steps: 1113, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.002662, mae: 1.254036, mean_q: 1.674058, mean_eps: 0.100000\n",
            " 1493513/1750000: episode: 3156, duration: 27.394s, episode steps: 929, steps per second:  34, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002827, mae: 1.247988, mean_q: 1.665491, mean_eps: 0.100000\n",
            " 1494152/1750000: episode: 3157, duration: 18.668s, episode steps: 639, steps per second:  34, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001672, mae: 1.258417, mean_q: 1.679969, mean_eps: 0.100000\n",
            " 1495196/1750000: episode: 3158, duration: 30.824s, episode steps: 1044, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002788, mae: 1.256190, mean_q: 1.676322, mean_eps: 0.100000\n",
            " 1496460/1750000: episode: 3159, duration: 37.512s, episode steps: 1264, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002486, mae: 1.251294, mean_q: 1.669820, mean_eps: 0.100000\n",
            " 1497861/1750000: episode: 3160, duration: 41.291s, episode steps: 1401, steps per second:  34, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.002126, mae: 1.252351, mean_q: 1.671709, mean_eps: 0.100000\n",
            " 1498580/1750000: episode: 3161, duration: 21.196s, episode steps: 719, steps per second:  34, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002694, mae: 1.258158, mean_q: 1.678798, mean_eps: 0.100000\n",
            " 1499505/1750000: episode: 3162, duration: 27.292s, episode steps: 925, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.318 [0.000, 3.000],  loss: 0.002369, mae: 1.254984, mean_q: 1.676802, mean_eps: 0.100000\n",
            " 1500726/1750000: episode: 3163, duration: 36.091s, episode steps: 1221, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.002714, mae: 1.252256, mean_q: 1.670970, mean_eps: 0.100000\n",
            " 1501668/1750000: episode: 3164, duration: 27.621s, episode steps: 942, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.003052, mae: 1.252464, mean_q: 1.672426, mean_eps: 0.100000\n",
            " 1502482/1750000: episode: 3165, duration: 24.178s, episode steps: 814, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.003242, mae: 1.245034, mean_q: 1.661782, mean_eps: 0.100000\n",
            " 1503171/1750000: episode: 3166, duration: 20.244s, episode steps: 689, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002495, mae: 1.255454, mean_q: 1.675353, mean_eps: 0.100000\n",
            " 1504300/1750000: episode: 3167, duration: 33.293s, episode steps: 1129, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.002536, mae: 1.251243, mean_q: 1.671204, mean_eps: 0.100000\n",
            " 1505096/1750000: episode: 3168, duration: 23.307s, episode steps: 796, steps per second:  34, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 0.002768, mae: 1.249768, mean_q: 1.667275, mean_eps: 0.100000\n",
            " 1505954/1750000: episode: 3169, duration: 25.413s, episode steps: 858, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.002777, mae: 1.242200, mean_q: 1.658600, mean_eps: 0.100000\n",
            " 1507260/1750000: episode: 3170, duration: 38.395s, episode steps: 1306, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.002018, mae: 1.252935, mean_q: 1.672443, mean_eps: 0.100000\n",
            " 1508579/1750000: episode: 3171, duration: 38.788s, episode steps: 1319, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002857, mae: 1.247347, mean_q: 1.666414, mean_eps: 0.100000\n",
            " 1509630/1750000: episode: 3172, duration: 31.057s, episode steps: 1051, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002409, mae: 1.251326, mean_q: 1.671520, mean_eps: 0.100000\n",
            " 1510815/1750000: episode: 3173, duration: 35.041s, episode steps: 1185, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002521, mae: 1.249267, mean_q: 1.667802, mean_eps: 0.100000\n",
            " 1511854/1750000: episode: 3174, duration: 30.559s, episode steps: 1039, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.002544, mae: 1.245468, mean_q: 1.662876, mean_eps: 0.100000\n",
            " 1512619/1750000: episode: 3175, duration: 22.357s, episode steps: 765, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.966 [0.000, 3.000],  loss: 0.002535, mae: 1.251856, mean_q: 1.671562, mean_eps: 0.100000\n",
            " 1513473/1750000: episode: 3176, duration: 25.158s, episode steps: 854, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002603, mae: 1.256361, mean_q: 1.676876, mean_eps: 0.100000\n",
            " 1514227/1750000: episode: 3177, duration: 22.288s, episode steps: 754, steps per second:  34, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.002027, mae: 1.251730, mean_q: 1.671170, mean_eps: 0.100000\n",
            " 1514989/1750000: episode: 3178, duration: 22.490s, episode steps: 762, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.762 [0.000, 3.000],  loss: 0.003252, mae: 1.252462, mean_q: 1.672481, mean_eps: 0.100000\n",
            " 1516065/1750000: episode: 3179, duration: 31.770s, episode steps: 1076, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.796 [0.000, 3.000],  loss: 0.002521, mae: 1.246425, mean_q: 1.663807, mean_eps: 0.100000\n",
            " 1516550/1750000: episode: 3180, duration: 14.293s, episode steps: 485, steps per second:  34, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002300, mae: 1.248328, mean_q: 1.667899, mean_eps: 0.100000\n",
            " 1517038/1750000: episode: 3181, duration: 14.497s, episode steps: 488, steps per second:  34, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 0.992 [0.000, 3.000],  loss: 0.002198, mae: 1.249814, mean_q: 1.668399, mean_eps: 0.100000\n",
            " 1518211/1750000: episode: 3182, duration: 34.330s, episode steps: 1173, steps per second:  34, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002466, mae: 1.249348, mean_q: 1.668219, mean_eps: 0.100000\n",
            " 1519624/1750000: episode: 3183, duration: 41.773s, episode steps: 1413, steps per second:  34, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002052, mae: 1.254744, mean_q: 1.675429, mean_eps: 0.100000\n",
            " 1520186/1750000: episode: 3184, duration: 16.827s, episode steps: 562, steps per second:  33, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002358, mae: 1.258384, mean_q: 1.680523, mean_eps: 0.100000\n",
            " 1521131/1750000: episode: 3185, duration: 27.946s, episode steps: 945, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.002271, mae: 1.252396, mean_q: 1.672930, mean_eps: 0.100000\n",
            " 1521860/1750000: episode: 3186, duration: 21.372s, episode steps: 729, steps per second:  34, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002548, mae: 1.247166, mean_q: 1.665305, mean_eps: 0.100000\n",
            " 1522638/1750000: episode: 3187, duration: 22.996s, episode steps: 778, steps per second:  34, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002632, mae: 1.246840, mean_q: 1.665303, mean_eps: 0.100000\n",
            " 1524526/1750000: episode: 3188, duration: 55.518s, episode steps: 1888, steps per second:  34, episode reward: 34.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.310 [0.000, 3.000],  loss: 0.002432, mae: 1.251736, mean_q: 1.671363, mean_eps: 0.100000\n",
            " 1525356/1750000: episode: 3189, duration: 24.376s, episode steps: 830, steps per second:  34, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.002554, mae: 1.248785, mean_q: 1.667545, mean_eps: 0.100000\n",
            " 1526579/1750000: episode: 3190, duration: 35.801s, episode steps: 1223, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002995, mae: 1.247965, mean_q: 1.665953, mean_eps: 0.100000\n",
            " 1527389/1750000: episode: 3191, duration: 23.956s, episode steps: 810, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.002678, mae: 1.247927, mean_q: 1.664939, mean_eps: 0.100000\n",
            " 1528314/1750000: episode: 3192, duration: 27.047s, episode steps: 925, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002286, mae: 1.256764, mean_q: 1.677159, mean_eps: 0.100000\n",
            " 1529189/1750000: episode: 3193, duration: 25.945s, episode steps: 875, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.002757, mae: 1.251424, mean_q: 1.670717, mean_eps: 0.100000\n",
            " 1530133/1750000: episode: 3194, duration: 27.644s, episode steps: 944, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: 0.002537, mae: 1.248192, mean_q: 1.666834, mean_eps: 0.100000\n",
            " 1530849/1750000: episode: 3195, duration: 21.135s, episode steps: 716, steps per second:  34, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.002230, mae: 1.253531, mean_q: 1.673441, mean_eps: 0.100000\n",
            " 1531875/1750000: episode: 3196, duration: 30.280s, episode steps: 1026, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.003042, mae: 1.250672, mean_q: 1.670195, mean_eps: 0.100000\n",
            " 1532574/1750000: episode: 3197, duration: 20.646s, episode steps: 699, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.345 [0.000, 3.000],  loss: 0.002795, mae: 1.249423, mean_q: 1.668349, mean_eps: 0.100000\n",
            " 1533350/1750000: episode: 3198, duration: 22.830s, episode steps: 776, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.003002, mae: 1.254573, mean_q: 1.674949, mean_eps: 0.100000\n",
            " 1534340/1750000: episode: 3199, duration: 29.041s, episode steps: 990, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.003189, mae: 1.253027, mean_q: 1.672327, mean_eps: 0.100000\n",
            " 1535372/1750000: episode: 3200, duration: 30.409s, episode steps: 1032, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.002385, mae: 1.252677, mean_q: 1.671859, mean_eps: 0.100000\n",
            " 1536341/1750000: episode: 3201, duration: 28.638s, episode steps: 969, steps per second:  34, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.002846, mae: 1.250338, mean_q: 1.668264, mean_eps: 0.100000\n",
            " 1537145/1750000: episode: 3202, duration: 23.798s, episode steps: 804, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002448, mae: 1.250299, mean_q: 1.669808, mean_eps: 0.100000\n",
            " 1538476/1750000: episode: 3203, duration: 39.472s, episode steps: 1331, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002184, mae: 1.251917, mean_q: 1.670868, mean_eps: 0.100000\n",
            " 1539519/1750000: episode: 3204, duration: 31.122s, episode steps: 1043, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001967, mae: 1.251783, mean_q: 1.671301, mean_eps: 0.100000\n",
            " 1540304/1750000: episode: 3205, duration: 23.163s, episode steps: 785, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.002217, mae: 1.252077, mean_q: 1.671950, mean_eps: 0.100000\n",
            " 1541216/1750000: episode: 3206, duration: 26.907s, episode steps: 912, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002121, mae: 1.249611, mean_q: 1.668541, mean_eps: 0.100000\n",
            " 1542521/1750000: episode: 3207, duration: 38.433s, episode steps: 1305, steps per second:  34, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.808 [0.000, 3.000],  loss: 0.002579, mae: 1.248981, mean_q: 1.668252, mean_eps: 0.100000\n",
            " 1543577/1750000: episode: 3208, duration: 30.936s, episode steps: 1056, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.914 [0.000, 3.000],  loss: 0.002275, mae: 1.254694, mean_q: 1.675745, mean_eps: 0.100000\n",
            " 1544465/1750000: episode: 3209, duration: 26.158s, episode steps: 888, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.002703, mae: 1.253756, mean_q: 1.673269, mean_eps: 0.100000\n",
            " 1545386/1750000: episode: 3210, duration: 27.114s, episode steps: 921, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.002275, mae: 1.259415, mean_q: 1.680962, mean_eps: 0.100000\n",
            " 1546500/1750000: episode: 3211, duration: 32.754s, episode steps: 1114, steps per second:  34, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.881 [0.000, 3.000],  loss: 0.002912, mae: 1.255469, mean_q: 1.675664, mean_eps: 0.100000\n",
            " 1547733/1750000: episode: 3212, duration: 36.136s, episode steps: 1233, steps per second:  34, episode reward: 19.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002088, mae: 1.258655, mean_q: 1.680452, mean_eps: 0.100000\n",
            " 1548948/1750000: episode: 3213, duration: 35.854s, episode steps: 1215, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.851 [0.000, 3.000],  loss: 0.002656, mae: 1.260374, mean_q: 1.682552, mean_eps: 0.100000\n",
            " 1550044/1750000: episode: 3214, duration: 32.455s, episode steps: 1096, steps per second:  34, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002861, mae: 1.253472, mean_q: 1.671974, mean_eps: 0.100000\n",
            " 1550805/1750000: episode: 3215, duration: 22.463s, episode steps: 761, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.852 [0.000, 3.000],  loss: 0.002641, mae: 1.261329, mean_q: 1.682976, mean_eps: 0.100000\n",
            " 1551886/1750000: episode: 3216, duration: 31.773s, episode steps: 1081, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002741, mae: 1.261025, mean_q: 1.682599, mean_eps: 0.100000\n",
            " 1552893/1750000: episode: 3217, duration: 29.869s, episode steps: 1007, steps per second:  34, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.002507, mae: 1.264011, mean_q: 1.686937, mean_eps: 0.100000\n",
            " 1554073/1750000: episode: 3218, duration: 34.827s, episode steps: 1180, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.899 [0.000, 3.000],  loss: 0.002453, mae: 1.258086, mean_q: 1.679526, mean_eps: 0.100000\n",
            " 1555040/1750000: episode: 3219, duration: 28.398s, episode steps: 967, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.853 [0.000, 3.000],  loss: 0.002980, mae: 1.257012, mean_q: 1.678212, mean_eps: 0.100000\n",
            " 1555803/1750000: episode: 3220, duration: 22.585s, episode steps: 763, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.003372, mae: 1.259036, mean_q: 1.679116, mean_eps: 0.100000\n",
            " 1556950/1750000: episode: 3221, duration: 33.798s, episode steps: 1147, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002646, mae: 1.262585, mean_q: 1.685222, mean_eps: 0.100000\n",
            " 1558410/1750000: episode: 3222, duration: 42.999s, episode steps: 1460, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002858, mae: 1.259127, mean_q: 1.681110, mean_eps: 0.100000\n",
            " 1559690/1750000: episode: 3223, duration: 37.783s, episode steps: 1280, steps per second:  34, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002382, mae: 1.260881, mean_q: 1.683519, mean_eps: 0.100000\n",
            " 1560852/1750000: episode: 3224, duration: 34.162s, episode steps: 1162, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002795, mae: 1.261515, mean_q: 1.684176, mean_eps: 0.100000\n",
            " 1562046/1750000: episode: 3225, duration: 35.598s, episode steps: 1194, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.916 [0.000, 3.000],  loss: 0.002843, mae: 1.264523, mean_q: 1.688399, mean_eps: 0.100000\n",
            " 1563348/1750000: episode: 3226, duration: 38.413s, episode steps: 1302, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002488, mae: 1.255797, mean_q: 1.677049, mean_eps: 0.100000\n",
            " 1564087/1750000: episode: 3227, duration: 21.749s, episode steps: 739, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002071, mae: 1.260715, mean_q: 1.683170, mean_eps: 0.100000\n",
            " 1565436/1750000: episode: 3228, duration: 39.698s, episode steps: 1349, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002351, mae: 1.258468, mean_q: 1.679722, mean_eps: 0.100000\n",
            " 1566450/1750000: episode: 3229, duration: 29.887s, episode steps: 1014, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.002668, mae: 1.258266, mean_q: 1.681709, mean_eps: 0.100000\n",
            " 1567975/1750000: episode: 3230, duration: 44.828s, episode steps: 1525, steps per second:  34, episode reward: 28.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002398, mae: 1.258093, mean_q: 1.679479, mean_eps: 0.100000\n",
            " 1568474/1750000: episode: 3231, duration: 14.663s, episode steps: 499, steps per second:  34, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002353, mae: 1.260487, mean_q: 1.682535, mean_eps: 0.100000\n",
            " 1569649/1750000: episode: 3232, duration: 34.572s, episode steps: 1175, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.002355, mae: 1.260239, mean_q: 1.681999, mean_eps: 0.100000\n",
            " 1570793/1750000: episode: 3233, duration: 33.561s, episode steps: 1144, steps per second:  34, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002580, mae: 1.269247, mean_q: 1.694239, mean_eps: 0.100000\n",
            " 1571781/1750000: episode: 3234, duration: 29.297s, episode steps: 988, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002123, mae: 1.265372, mean_q: 1.689243, mean_eps: 0.100000\n",
            " 1572865/1750000: episode: 3235, duration: 31.979s, episode steps: 1084, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.002780, mae: 1.269112, mean_q: 1.693887, mean_eps: 0.100000\n",
            " 1574241/1750000: episode: 3236, duration: 40.567s, episode steps: 1376, steps per second:  34, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002217, mae: 1.267088, mean_q: 1.692092, mean_eps: 0.100000\n",
            " 1575062/1750000: episode: 3237, duration: 24.012s, episode steps: 821, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.001787, mae: 1.261213, mean_q: 1.683245, mean_eps: 0.100000\n",
            " 1575927/1750000: episode: 3238, duration: 25.528s, episode steps: 865, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002120, mae: 1.267878, mean_q: 1.693605, mean_eps: 0.100000\n",
            " 1576806/1750000: episode: 3239, duration: 26.020s, episode steps: 879, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002427, mae: 1.265072, mean_q: 1.688918, mean_eps: 0.100000\n",
            " 1577899/1750000: episode: 3240, duration: 32.391s, episode steps: 1093, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.859 [0.000, 3.000],  loss: 0.002222, mae: 1.267177, mean_q: 1.691598, mean_eps: 0.100000\n",
            " 1579223/1750000: episode: 3241, duration: 39.071s, episode steps: 1324, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002705, mae: 1.262954, mean_q: 1.685812, mean_eps: 0.100000\n",
            " 1580475/1750000: episode: 3242, duration: 37.053s, episode steps: 1252, steps per second:  34, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.002436, mae: 1.266420, mean_q: 1.690719, mean_eps: 0.100000\n",
            " 1581340/1750000: episode: 3243, duration: 25.495s, episode steps: 865, steps per second:  34, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002292, mae: 1.262516, mean_q: 1.685466, mean_eps: 0.100000\n",
            " 1582629/1750000: episode: 3244, duration: 38.201s, episode steps: 1289, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002951, mae: 1.266555, mean_q: 1.690652, mean_eps: 0.100000\n",
            " 1583917/1750000: episode: 3245, duration: 38.294s, episode steps: 1288, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.002246, mae: 1.268164, mean_q: 1.693079, mean_eps: 0.100000\n",
            " 1585334/1750000: episode: 3246, duration: 41.704s, episode steps: 1417, steps per second:  34, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.002480, mae: 1.266034, mean_q: 1.690109, mean_eps: 0.100000\n",
            " 1586174/1750000: episode: 3247, duration: 24.853s, episode steps: 840, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002587, mae: 1.263789, mean_q: 1.686228, mean_eps: 0.100000\n",
            " 1587092/1750000: episode: 3248, duration: 27.151s, episode steps: 918, steps per second:  34, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002370, mae: 1.268960, mean_q: 1.694156, mean_eps: 0.100000\n",
            " 1587823/1750000: episode: 3249, duration: 21.712s, episode steps: 731, steps per second:  34, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002377, mae: 1.267675, mean_q: 1.691915, mean_eps: 0.100000\n",
            " 1588849/1750000: episode: 3250, duration: 30.379s, episode steps: 1026, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.002299, mae: 1.262618, mean_q: 1.685549, mean_eps: 0.100000\n",
            " 1589820/1750000: episode: 3251, duration: 28.841s, episode steps: 971, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002196, mae: 1.267585, mean_q: 1.691885, mean_eps: 0.100000\n",
            " 1590491/1750000: episode: 3252, duration: 19.960s, episode steps: 671, steps per second:  34, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.963 [0.000, 3.000],  loss: 0.003004, mae: 1.275770, mean_q: 1.703347, mean_eps: 0.100000\n",
            " 1591510/1750000: episode: 3253, duration: 30.015s, episode steps: 1019, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002228, mae: 1.282365, mean_q: 1.711330, mean_eps: 0.100000\n",
            " 1592538/1750000: episode: 3254, duration: 30.387s, episode steps: 1028, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.002360, mae: 1.277265, mean_q: 1.705001, mean_eps: 0.100000\n",
            " 1593497/1750000: episode: 3255, duration: 28.419s, episode steps: 959, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.916 [0.000, 3.000],  loss: 0.003369, mae: 1.277293, mean_q: 1.705328, mean_eps: 0.100000\n",
            " 1594422/1750000: episode: 3256, duration: 27.362s, episode steps: 925, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002668, mae: 1.273184, mean_q: 1.699242, mean_eps: 0.100000\n",
            " 1595127/1750000: episode: 3257, duration: 20.971s, episode steps: 705, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.817 [0.000, 3.000],  loss: 0.002783, mae: 1.281139, mean_q: 1.710364, mean_eps: 0.100000\n",
            " 1595987/1750000: episode: 3258, duration: 25.352s, episode steps: 860, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002810, mae: 1.279784, mean_q: 1.708976, mean_eps: 0.100000\n",
            " 1596792/1750000: episode: 3259, duration: 23.758s, episode steps: 805, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.002552, mae: 1.282207, mean_q: 1.710821, mean_eps: 0.100000\n",
            " 1597666/1750000: episode: 3260, duration: 25.859s, episode steps: 874, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.003015, mae: 1.279132, mean_q: 1.706783, mean_eps: 0.100000\n",
            " 1598669/1750000: episode: 3261, duration: 29.584s, episode steps: 1003, steps per second:  34, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.002679, mae: 1.278759, mean_q: 1.706464, mean_eps: 0.100000\n",
            " 1599805/1750000: episode: 3262, duration: 33.435s, episode steps: 1136, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002776, mae: 1.278030, mean_q: 1.706020, mean_eps: 0.100000\n",
            " 1600579/1750000: episode: 3263, duration: 22.937s, episode steps: 774, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002717, mae: 1.282352, mean_q: 1.711604, mean_eps: 0.100000\n",
            " 1601598/1750000: episode: 3264, duration: 30.256s, episode steps: 1019, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.003268, mae: 1.283897, mean_q: 1.712792, mean_eps: 0.100000\n",
            " 1602879/1750000: episode: 3265, duration: 37.851s, episode steps: 1281, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002405, mae: 1.276903, mean_q: 1.704092, mean_eps: 0.100000\n",
            " 1604180/1750000: episode: 3266, duration: 38.362s, episode steps: 1301, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002855, mae: 1.285800, mean_q: 1.715359, mean_eps: 0.100000\n",
            " 1605366/1750000: episode: 3267, duration: 34.907s, episode steps: 1186, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002501, mae: 1.282419, mean_q: 1.711689, mean_eps: 0.100000\n",
            " 1606545/1750000: episode: 3268, duration: 34.879s, episode steps: 1179, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.863 [0.000, 3.000],  loss: 0.002843, mae: 1.278890, mean_q: 1.706640, mean_eps: 0.100000\n",
            " 1607867/1750000: episode: 3269, duration: 39.012s, episode steps: 1322, steps per second:  34, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002785, mae: 1.279551, mean_q: 1.707266, mean_eps: 0.100000\n",
            " 1608935/1750000: episode: 3270, duration: 31.336s, episode steps: 1068, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002627, mae: 1.281521, mean_q: 1.709733, mean_eps: 0.100000\n",
            " 1610224/1750000: episode: 3271, duration: 37.813s, episode steps: 1289, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.002403, mae: 1.284083, mean_q: 1.713240, mean_eps: 0.100000\n",
            " 1611308/1750000: episode: 3272, duration: 32.269s, episode steps: 1084, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.002560, mae: 1.287832, mean_q: 1.718666, mean_eps: 0.100000\n",
            " 1612130/1750000: episode: 3273, duration: 24.419s, episode steps: 822, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002827, mae: 1.288379, mean_q: 1.719695, mean_eps: 0.100000\n",
            " 1613138/1750000: episode: 3274, duration: 29.966s, episode steps: 1008, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002795, mae: 1.287903, mean_q: 1.718574, mean_eps: 0.100000\n",
            " 1613835/1750000: episode: 3275, duration: 20.724s, episode steps: 697, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002179, mae: 1.290792, mean_q: 1.723761, mean_eps: 0.100000\n",
            " 1614623/1750000: episode: 3276, duration: 23.204s, episode steps: 788, steps per second:  34, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.002756, mae: 1.290072, mean_q: 1.721124, mean_eps: 0.100000\n",
            " 1615736/1750000: episode: 3277, duration: 32.844s, episode steps: 1113, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002327, mae: 1.281105, mean_q: 1.709436, mean_eps: 0.100000\n",
            " 1616548/1750000: episode: 3278, duration: 24.356s, episode steps: 812, steps per second:  33, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002902, mae: 1.288957, mean_q: 1.719609, mean_eps: 0.100000\n",
            " 1617472/1750000: episode: 3279, duration: 27.352s, episode steps: 924, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.003546, mae: 1.282214, mean_q: 1.711294, mean_eps: 0.100000\n",
            " 1618672/1750000: episode: 3280, duration: 35.523s, episode steps: 1200, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002024, mae: 1.287175, mean_q: 1.718628, mean_eps: 0.100000\n",
            " 1619468/1750000: episode: 3281, duration: 23.665s, episode steps: 796, steps per second:  34, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: 0.002554, mae: 1.288780, mean_q: 1.719597, mean_eps: 0.100000\n",
            " 1620197/1750000: episode: 3282, duration: 21.421s, episode steps: 729, steps per second:  34, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.789 [0.000, 3.000],  loss: 0.002522, mae: 1.289032, mean_q: 1.720097, mean_eps: 0.100000\n",
            " 1621649/1750000: episode: 3283, duration: 43.140s, episode steps: 1452, steps per second:  34, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.002217, mae: 1.301120, mean_q: 1.735886, mean_eps: 0.100000\n",
            " 1622517/1750000: episode: 3284, duration: 25.611s, episode steps: 868, steps per second:  34, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002464, mae: 1.303633, mean_q: 1.739806, mean_eps: 0.100000\n",
            " 1624066/1750000: episode: 3285, duration: 45.492s, episode steps: 1549, steps per second:  34, episode reward: 27.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002364, mae: 1.304421, mean_q: 1.740767, mean_eps: 0.100000\n",
            " 1625720/1750000: episode: 3286, duration: 49.024s, episode steps: 1654, steps per second:  34, episode reward: 30.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.002887, mae: 1.302374, mean_q: 1.737222, mean_eps: 0.100000\n",
            " 1627059/1750000: episode: 3287, duration: 39.749s, episode steps: 1339, steps per second:  34, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.816 [0.000, 3.000],  loss: 0.002612, mae: 1.300285, mean_q: 1.735962, mean_eps: 0.100000\n",
            " 1627959/1750000: episode: 3288, duration: 26.699s, episode steps: 900, steps per second:  34, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002837, mae: 1.308032, mean_q: 1.745466, mean_eps: 0.100000\n",
            " 1629380/1750000: episode: 3289, duration: 41.721s, episode steps: 1421, steps per second:  34, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002608, mae: 1.303012, mean_q: 1.738138, mean_eps: 0.100000\n",
            " 1631044/1750000: episode: 3290, duration: 49.505s, episode steps: 1664, steps per second:  34, episode reward: 30.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.003159, mae: 1.308772, mean_q: 1.746380, mean_eps: 0.100000\n",
            " 1632010/1750000: episode: 3291, duration: 28.621s, episode steps: 966, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.002606, mae: 1.308644, mean_q: 1.745975, mean_eps: 0.100000\n",
            " 1632971/1750000: episode: 3292, duration: 28.329s, episode steps: 961, steps per second:  34, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002752, mae: 1.309753, mean_q: 1.747295, mean_eps: 0.100000\n",
            " 1634420/1750000: episode: 3293, duration: 42.631s, episode steps: 1449, steps per second:  34, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.337 [0.000, 3.000],  loss: 0.002436, mae: 1.312054, mean_q: 1.750887, mean_eps: 0.100000\n",
            " 1635495/1750000: episode: 3294, duration: 31.857s, episode steps: 1075, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002902, mae: 1.315224, mean_q: 1.755473, mean_eps: 0.100000\n",
            " 1636982/1750000: episode: 3295, duration: 43.841s, episode steps: 1487, steps per second:  34, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002743, mae: 1.309841, mean_q: 1.748449, mean_eps: 0.100000\n",
            " 1637811/1750000: episode: 3296, duration: 24.433s, episode steps: 829, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002173, mae: 1.312145, mean_q: 1.751965, mean_eps: 0.100000\n",
            " 1638570/1750000: episode: 3297, duration: 22.684s, episode steps: 759, steps per second:  33, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.002888, mae: 1.309555, mean_q: 1.747828, mean_eps: 0.100000\n",
            " 1639542/1750000: episode: 3298, duration: 28.557s, episode steps: 972, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.003030, mae: 1.308263, mean_q: 1.745932, mean_eps: 0.100000\n",
            " 1640440/1750000: episode: 3299, duration: 26.667s, episode steps: 898, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002312, mae: 1.315574, mean_q: 1.755508, mean_eps: 0.100000\n",
            " 1641507/1750000: episode: 3300, duration: 31.705s, episode steps: 1067, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002524, mae: 1.316931, mean_q: 1.756783, mean_eps: 0.100000\n",
            " 1642418/1750000: episode: 3301, duration: 27.067s, episode steps: 911, steps per second:  34, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002156, mae: 1.320539, mean_q: 1.761947, mean_eps: 0.100000\n",
            " 1643355/1750000: episode: 3302, duration: 27.524s, episode steps: 937, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002885, mae: 1.315980, mean_q: 1.756270, mean_eps: 0.100000\n",
            " 1644790/1750000: episode: 3303, duration: 42.346s, episode steps: 1435, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.815 [0.000, 3.000],  loss: 0.002283, mae: 1.315272, mean_q: 1.756254, mean_eps: 0.100000\n",
            " 1646056/1750000: episode: 3304, duration: 37.413s, episode steps: 1266, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.002524, mae: 1.310947, mean_q: 1.749182, mean_eps: 0.100000\n",
            " 1647011/1750000: episode: 3305, duration: 28.115s, episode steps: 955, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.330 [0.000, 3.000],  loss: 0.002458, mae: 1.317334, mean_q: 1.757838, mean_eps: 0.100000\n",
            " 1648004/1750000: episode: 3306, duration: 29.374s, episode steps: 993, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002075, mae: 1.314953, mean_q: 1.755431, mean_eps: 0.100000\n",
            " 1648813/1750000: episode: 3307, duration: 23.869s, episode steps: 809, steps per second:  34, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.002285, mae: 1.316097, mean_q: 1.756944, mean_eps: 0.100000\n",
            " 1649671/1750000: episode: 3308, duration: 25.379s, episode steps: 858, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002786, mae: 1.318816, mean_q: 1.760488, mean_eps: 0.100000\n",
            " 1650682/1750000: episode: 3309, duration: 30.075s, episode steps: 1011, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002476, mae: 1.317175, mean_q: 1.757724, mean_eps: 0.100000\n",
            " 1651767/1750000: episode: 3310, duration: 31.831s, episode steps: 1085, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.002362, mae: 1.322401, mean_q: 1.764711, mean_eps: 0.100000\n",
            " 1652786/1750000: episode: 3311, duration: 30.233s, episode steps: 1019, steps per second:  34, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002868, mae: 1.315164, mean_q: 1.754980, mean_eps: 0.100000\n",
            " 1653952/1750000: episode: 3312, duration: 34.489s, episode steps: 1166, steps per second:  34, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002500, mae: 1.316932, mean_q: 1.757949, mean_eps: 0.100000\n",
            " 1654877/1750000: episode: 3313, duration: 27.506s, episode steps: 925, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.002041, mae: 1.320747, mean_q: 1.762397, mean_eps: 0.100000\n",
            " 1655843/1750000: episode: 3314, duration: 28.510s, episode steps: 966, steps per second:  34, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.002738, mae: 1.323613, mean_q: 1.766895, mean_eps: 0.100000\n",
            " 1656441/1750000: episode: 3315, duration: 17.752s, episode steps: 598, steps per second:  34, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002094, mae: 1.328339, mean_q: 1.772820, mean_eps: 0.100000\n",
            " 1657430/1750000: episode: 3316, duration: 29.331s, episode steps: 989, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.003735, mae: 1.317421, mean_q: 1.757797, mean_eps: 0.100000\n",
            " 1658291/1750000: episode: 3317, duration: 25.192s, episode steps: 861, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002734, mae: 1.322952, mean_q: 1.765745, mean_eps: 0.100000\n",
            " 1659156/1750000: episode: 3318, duration: 25.658s, episode steps: 865, steps per second:  34, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.932 [0.000, 3.000],  loss: 0.002918, mae: 1.317876, mean_q: 1.758802, mean_eps: 0.100000\n",
            " 1660363/1750000: episode: 3319, duration: 35.769s, episode steps: 1207, steps per second:  34, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002532, mae: 1.322857, mean_q: 1.764917, mean_eps: 0.100000\n",
            " 1661516/1750000: episode: 3320, duration: 34.234s, episode steps: 1153, steps per second:  34, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002432, mae: 1.325980, mean_q: 1.769479, mean_eps: 0.100000\n",
            " 1662487/1750000: episode: 3321, duration: 28.799s, episode steps: 971, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002195, mae: 1.320556, mean_q: 1.763783, mean_eps: 0.100000\n",
            " 1663367/1750000: episode: 3322, duration: 25.956s, episode steps: 880, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002991, mae: 1.316544, mean_q: 1.757048, mean_eps: 0.100000\n",
            " 1664435/1750000: episode: 3323, duration: 31.448s, episode steps: 1068, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002943, mae: 1.318274, mean_q: 1.759315, mean_eps: 0.100000\n",
            " 1665300/1750000: episode: 3324, duration: 25.392s, episode steps: 865, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.002459, mae: 1.326434, mean_q: 1.769668, mean_eps: 0.100000\n",
            " 1666090/1750000: episode: 3325, duration: 23.410s, episode steps: 790, steps per second:  34, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 0.986 [0.000, 3.000],  loss: 0.002994, mae: 1.319399, mean_q: 1.760009, mean_eps: 0.100000\n",
            " 1667063/1750000: episode: 3326, duration: 28.626s, episode steps: 973, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.295 [0.000, 3.000],  loss: 0.003252, mae: 1.318935, mean_q: 1.759090, mean_eps: 0.100000\n",
            " 1668482/1750000: episode: 3327, duration: 42.021s, episode steps: 1419, steps per second:  34, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002294, mae: 1.321899, mean_q: 1.763946, mean_eps: 0.100000\n",
            " 1669519/1750000: episode: 3328, duration: 30.730s, episode steps: 1037, steps per second:  34, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.002598, mae: 1.319821, mean_q: 1.760856, mean_eps: 0.100000\n",
            " 1670376/1750000: episode: 3329, duration: 25.300s, episode steps: 857, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.002565, mae: 1.322099, mean_q: 1.763985, mean_eps: 0.100000\n",
            " 1671318/1750000: episode: 3330, duration: 28.121s, episode steps: 942, steps per second:  33, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002437, mae: 1.328118, mean_q: 1.773126, mean_eps: 0.100000\n",
            " 1672226/1750000: episode: 3331, duration: 26.786s, episode steps: 908, steps per second:  34, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.334 [0.000, 3.000],  loss: 0.002554, mae: 1.329362, mean_q: 1.773874, mean_eps: 0.100000\n",
            " 1673068/1750000: episode: 3332, duration: 24.950s, episode steps: 842, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002556, mae: 1.321401, mean_q: 1.762567, mean_eps: 0.100000\n",
            " 1673922/1750000: episode: 3333, duration: 25.243s, episode steps: 854, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: 0.002510, mae: 1.324774, mean_q: 1.766921, mean_eps: 0.100000\n",
            " 1675049/1750000: episode: 3334, duration: 33.431s, episode steps: 1127, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.002255, mae: 1.329026, mean_q: 1.774186, mean_eps: 0.100000\n",
            " 1676335/1750000: episode: 3335, duration: 37.967s, episode steps: 1286, steps per second:  34, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.002324, mae: 1.323472, mean_q: 1.766810, mean_eps: 0.100000\n",
            " 1677823/1750000: episode: 3336, duration: 43.944s, episode steps: 1488, steps per second:  34, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.261 [0.000, 3.000],  loss: 0.002425, mae: 1.323773, mean_q: 1.766491, mean_eps: 0.100000\n",
            " 1678622/1750000: episode: 3337, duration: 23.692s, episode steps: 799, steps per second:  34, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002215, mae: 1.326805, mean_q: 1.770237, mean_eps: 0.100000\n",
            " 1679439/1750000: episode: 3338, duration: 24.104s, episode steps: 817, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.003039, mae: 1.323283, mean_q: 1.765642, mean_eps: 0.100000\n",
            " 1680299/1750000: episode: 3339, duration: 25.525s, episode steps: 860, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002419, mae: 1.329690, mean_q: 1.774797, mean_eps: 0.100000\n",
            " 1680895/1750000: episode: 3340, duration: 17.553s, episode steps: 596, steps per second:  34, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.002659, mae: 1.335510, mean_q: 1.781827, mean_eps: 0.100000\n",
            " 1681921/1750000: episode: 3341, duration: 30.205s, episode steps: 1026, steps per second:  34, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.002164, mae: 1.333231, mean_q: 1.779416, mean_eps: 0.100000\n",
            " 1682525/1750000: episode: 3342, duration: 17.886s, episode steps: 604, steps per second:  34, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002894, mae: 1.339453, mean_q: 1.787966, mean_eps: 0.100000\n",
            " 1683447/1750000: episode: 3343, duration: 27.199s, episode steps: 922, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.002379, mae: 1.332448, mean_q: 1.777777, mean_eps: 0.100000\n",
            " 1684533/1750000: episode: 3344, duration: 32.148s, episode steps: 1086, steps per second:  34, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.002491, mae: 1.336271, mean_q: 1.783802, mean_eps: 0.100000\n",
            " 1686430/1750000: episode: 3345, duration: 56.107s, episode steps: 1897, steps per second:  34, episode reward: 34.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.002229, mae: 1.333430, mean_q: 1.779641, mean_eps: 0.100000\n",
            " 1687538/1750000: episode: 3346, duration: 32.769s, episode steps: 1108, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.002865, mae: 1.332449, mean_q: 1.777541, mean_eps: 0.100000\n",
            " 1688482/1750000: episode: 3347, duration: 28.224s, episode steps: 944, steps per second:  33, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002839, mae: 1.330362, mean_q: 1.774282, mean_eps: 0.100000\n",
            " 1689162/1750000: episode: 3348, duration: 20.343s, episode steps: 680, steps per second:  33, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.002126, mae: 1.332477, mean_q: 1.778130, mean_eps: 0.100000\n",
            " 1690069/1750000: episode: 3349, duration: 26.965s, episode steps: 907, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.002801, mae: 1.334473, mean_q: 1.780356, mean_eps: 0.100000\n",
            " 1691060/1750000: episode: 3350, duration: 29.434s, episode steps: 991, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002940, mae: 1.343279, mean_q: 1.791612, mean_eps: 0.100000\n",
            " 1691992/1750000: episode: 3351, duration: 27.570s, episode steps: 932, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.182 [0.000, 3.000],  loss: 0.002534, mae: 1.337401, mean_q: 1.784143, mean_eps: 0.100000\n",
            " 1692626/1750000: episode: 3352, duration: 18.685s, episode steps: 634, steps per second:  34, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.989 [0.000, 3.000],  loss: 0.003005, mae: 1.339484, mean_q: 1.787859, mean_eps: 0.100000\n",
            " 1693421/1750000: episode: 3353, duration: 23.628s, episode steps: 795, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002190, mae: 1.344474, mean_q: 1.793774, mean_eps: 0.100000\n",
            " 1694212/1750000: episode: 3354, duration: 23.388s, episode steps: 791, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.805 [0.000, 3.000],  loss: 0.002828, mae: 1.338274, mean_q: 1.785577, mean_eps: 0.100000\n",
            " 1695806/1750000: episode: 3355, duration: 47.161s, episode steps: 1594, steps per second:  34, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002397, mae: 1.341835, mean_q: 1.791063, mean_eps: 0.100000\n",
            " 1696717/1750000: episode: 3356, duration: 26.986s, episode steps: 911, steps per second:  34, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002232, mae: 1.338217, mean_q: 1.785188, mean_eps: 0.100000\n",
            " 1697804/1750000: episode: 3357, duration: 32.203s, episode steps: 1087, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002966, mae: 1.342595, mean_q: 1.791286, mean_eps: 0.100000\n",
            " 1698820/1750000: episode: 3358, duration: 30.142s, episode steps: 1016, steps per second:  34, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002868, mae: 1.342505, mean_q: 1.790621, mean_eps: 0.100000\n",
            " 1699716/1750000: episode: 3359, duration: 26.830s, episode steps: 896, steps per second:  33, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.002220, mae: 1.342984, mean_q: 1.792107, mean_eps: 0.100000\n",
            " 1701044/1750000: episode: 3360, duration: 39.433s, episode steps: 1328, steps per second:  34, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.329 [0.000, 3.000],  loss: 0.003287, mae: 1.343058, mean_q: 1.791182, mean_eps: 0.100000\n",
            " 1701835/1750000: episode: 3361, duration: 23.447s, episode steps: 791, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001919, mae: 1.343846, mean_q: 1.793605, mean_eps: 0.100000\n",
            " 1702927/1750000: episode: 3362, duration: 32.207s, episode steps: 1092, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.003327, mae: 1.340771, mean_q: 1.788811, mean_eps: 0.100000\n",
            " 1703667/1750000: episode: 3363, duration: 21.919s, episode steps: 740, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.002512, mae: 1.346353, mean_q: 1.797192, mean_eps: 0.100000\n",
            " 1704526/1750000: episode: 3364, duration: 25.599s, episode steps: 859, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002379, mae: 1.343953, mean_q: 1.792880, mean_eps: 0.100000\n",
            " 1705308/1750000: episode: 3365, duration: 23.059s, episode steps: 782, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002668, mae: 1.345966, mean_q: 1.796146, mean_eps: 0.100000\n",
            " 1706554/1750000: episode: 3366, duration: 36.993s, episode steps: 1246, steps per second:  34, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.003140, mae: 1.345005, mean_q: 1.794463, mean_eps: 0.100000\n",
            " 1707605/1750000: episode: 3367, duration: 31.221s, episode steps: 1051, steps per second:  34, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002651, mae: 1.345477, mean_q: 1.794906, mean_eps: 0.100000\n",
            " 1708641/1750000: episode: 3368, duration: 30.528s, episode steps: 1036, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.003016, mae: 1.342874, mean_q: 1.792632, mean_eps: 0.100000\n",
            " 1709618/1750000: episode: 3369, duration: 28.797s, episode steps: 977, steps per second:  34, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.002203, mae: 1.341019, mean_q: 1.789565, mean_eps: 0.100000\n",
            " 1711079/1750000: episode: 3370, duration: 43.240s, episode steps: 1461, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002774, mae: 1.348778, mean_q: 1.799628, mean_eps: 0.100000\n",
            " 1712005/1750000: episode: 3371, duration: 27.316s, episode steps: 926, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002615, mae: 1.345200, mean_q: 1.795665, mean_eps: 0.100000\n",
            " 1713272/1750000: episode: 3372, duration: 37.384s, episode steps: 1267, steps per second:  34, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.002921, mae: 1.346067, mean_q: 1.795977, mean_eps: 0.100000\n",
            " 1714236/1750000: episode: 3373, duration: 28.473s, episode steps: 964, steps per second:  34, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.003828, mae: 1.349056, mean_q: 1.799369, mean_eps: 0.100000\n",
            " 1715009/1750000: episode: 3374, duration: 22.971s, episode steps: 773, steps per second:  34, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002830, mae: 1.352745, mean_q: 1.805352, mean_eps: 0.100000\n",
            " 1716075/1750000: episode: 3375, duration: 31.649s, episode steps: 1066, steps per second:  34, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.787 [0.000, 3.000],  loss: 0.002570, mae: 1.351539, mean_q: 1.804187, mean_eps: 0.100000\n",
            " 1717030/1750000: episode: 3376, duration: 28.225s, episode steps: 955, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.002879, mae: 1.348001, mean_q: 1.798948, mean_eps: 0.100000\n",
            " 1717869/1750000: episode: 3377, duration: 24.864s, episode steps: 839, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.002901, mae: 1.352309, mean_q: 1.803517, mean_eps: 0.100000\n",
            " 1718809/1750000: episode: 3378, duration: 27.900s, episode steps: 940, steps per second:  34, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.002674, mae: 1.350711, mean_q: 1.802782, mean_eps: 0.100000\n",
            " 1719445/1750000: episode: 3379, duration: 18.811s, episode steps: 636, steps per second:  34, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002227, mae: 1.340329, mean_q: 1.788929, mean_eps: 0.100000\n",
            " 1720842/1750000: episode: 3380, duration: 41.819s, episode steps: 1397, steps per second:  33, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002860, mae: 1.355895, mean_q: 1.808777, mean_eps: 0.100000\n",
            " 1721619/1750000: episode: 3381, duration: 22.861s, episode steps: 777, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002441, mae: 1.356780, mean_q: 1.810164, mean_eps: 0.100000\n",
            " 1722392/1750000: episode: 3382, duration: 22.868s, episode steps: 773, steps per second:  34, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002675, mae: 1.361072, mean_q: 1.815598, mean_eps: 0.100000\n",
            " 1722934/1750000: episode: 3383, duration: 15.902s, episode steps: 542, steps per second:  34, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.310 [0.000, 3.000],  loss: 0.002431, mae: 1.362200, mean_q: 1.817814, mean_eps: 0.100000\n",
            " 1723881/1750000: episode: 3384, duration: 28.283s, episode steps: 947, steps per second:  33, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002796, mae: 1.361571, mean_q: 1.816611, mean_eps: 0.100000\n",
            " 1725113/1750000: episode: 3385, duration: 36.517s, episode steps: 1232, steps per second:  34, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002588, mae: 1.356595, mean_q: 1.810585, mean_eps: 0.100000\n",
            " 1726182/1750000: episode: 3386, duration: 31.619s, episode steps: 1069, steps per second:  34, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002284, mae: 1.358338, mean_q: 1.812417, mean_eps: 0.100000\n",
            " 1727050/1750000: episode: 3387, duration: 25.763s, episode steps: 868, steps per second:  34, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.002980, mae: 1.365380, mean_q: 1.821999, mean_eps: 0.100000\n",
            " 1728163/1750000: episode: 3388, duration: 33.136s, episode steps: 1113, steps per second:  34, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.003355, mae: 1.354360, mean_q: 1.807278, mean_eps: 0.100000\n",
            " 1729266/1750000: episode: 3389, duration: 32.816s, episode steps: 1103, steps per second:  34, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002682, mae: 1.357289, mean_q: 1.810823, mean_eps: 0.100000\n",
            " 1730531/1750000: episode: 3390, duration: 37.204s, episode steps: 1265, steps per second:  34, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.002914, mae: 1.358266, mean_q: 1.812345, mean_eps: 0.100000\n",
            " 1731183/1750000: episode: 3391, duration: 19.310s, episode steps: 652, steps per second:  34, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.857 [0.000, 3.000],  loss: 0.002880, mae: 1.363056, mean_q: 1.819200, mean_eps: 0.100000\n",
            " 1731970/1750000: episode: 3392, duration: 23.469s, episode steps: 787, steps per second:  34, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.001830, mae: 1.361151, mean_q: 1.817214, mean_eps: 0.100000\n",
            " 1732981/1750000: episode: 3393, duration: 29.941s, episode steps: 1011, steps per second:  34, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.002758, mae: 1.359201, mean_q: 1.813089, mean_eps: 0.100000\n",
            " 1734435/1750000: episode: 3394, duration: 43.322s, episode steps: 1454, steps per second:  34, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002617, mae: 1.359450, mean_q: 1.813259, mean_eps: 0.100000\n",
            " 1735470/1750000: episode: 3395, duration: 30.610s, episode steps: 1035, steps per second:  34, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002566, mae: 1.361571, mean_q: 1.816707, mean_eps: 0.100000\n",
            " 1736269/1750000: episode: 3396, duration: 23.544s, episode steps: 799, steps per second:  34, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.002635, mae: 1.365780, mean_q: 1.821401, mean_eps: 0.100000\n",
            " 1737110/1750000: episode: 3397, duration: 24.903s, episode steps: 841, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002269, mae: 1.363466, mean_q: 1.819404, mean_eps: 0.100000\n",
            " 1737798/1750000: episode: 3398, duration: 20.462s, episode steps: 688, steps per second:  34, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002983, mae: 1.357434, mean_q: 1.811248, mean_eps: 0.100000\n",
            " 1738640/1750000: episode: 3399, duration: 24.789s, episode steps: 842, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002906, mae: 1.363087, mean_q: 1.818566, mean_eps: 0.100000\n",
            " 1739495/1750000: episode: 3400, duration: 25.354s, episode steps: 855, steps per second:  34, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.003391, mae: 1.359768, mean_q: 1.813758, mean_eps: 0.100000\n",
            " 1740264/1750000: episode: 3401, duration: 23.147s, episode steps: 769, steps per second:  33, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.002758, mae: 1.360850, mean_q: 1.816460, mean_eps: 0.100000\n",
            " 1741354/1750000: episode: 3402, duration: 32.233s, episode steps: 1090, steps per second:  34, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.003142, mae: 1.363832, mean_q: 1.820380, mean_eps: 0.100000\n",
            " 1742498/1750000: episode: 3403, duration: 33.939s, episode steps: 1144, steps per second:  34, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002413, mae: 1.366853, mean_q: 1.823321, mean_eps: 0.100000\n",
            " 1743563/1750000: episode: 3404, duration: 31.375s, episode steps: 1065, steps per second:  34, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002378, mae: 1.369698, mean_q: 1.827359, mean_eps: 0.100000\n",
            " 1744201/1750000: episode: 3405, duration: 18.846s, episode steps: 638, steps per second:  34, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.871 [0.000, 3.000],  loss: 0.002296, mae: 1.366791, mean_q: 1.824736, mean_eps: 0.100000\n",
            " 1745012/1750000: episode: 3406, duration: 23.927s, episode steps: 811, steps per second:  34, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002994, mae: 1.366263, mean_q: 1.823004, mean_eps: 0.100000\n",
            " 1746141/1750000: episode: 3407, duration: 33.711s, episode steps: 1129, steps per second:  33, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.003086, mae: 1.366580, mean_q: 1.823913, mean_eps: 0.100000\n",
            " 1747440/1750000: episode: 3408, duration: 38.480s, episode steps: 1299, steps per second:  34, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002412, mae: 1.368858, mean_q: 1.826891, mean_eps: 0.100000\n",
            " 1748675/1750000: episode: 3409, duration: 36.503s, episode steps: 1235, steps per second:  34, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.002130, mae: 1.367070, mean_q: 1.823739, mean_eps: 0.100000\n",
            " 1749630/1750000: episode: 3410, duration: 28.203s, episode steps: 955, steps per second:  34, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002866, mae: 1.367795, mean_q: 1.824530, mean_eps: 0.100000\n",
            "done, took 44113.236 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}